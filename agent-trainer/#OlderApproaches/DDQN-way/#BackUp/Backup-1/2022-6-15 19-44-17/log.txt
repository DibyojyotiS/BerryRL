copied Agent.py to .temp\2022-6-15 19-44-17/pyfiles-backup
copied ensemble.py to .temp\2022-6-15 19-44-17/pyfiles-backup
copied eval.py to .temp\2022-6-15 19-44-17/pyfiles-backup
copied train.py to .temp\2022-6-15 19-44-17/pyfiles-backup
copied utils.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/copyfiles
copied __init__.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/copyfiles

copied random_env.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/env_generation
copied __init__.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/env_generation

copied exploration.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/exploration_subroutines
copied random_exploration.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/exploration_subroutines
copied __init__.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/exploration_subroutines

copied make_net.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/nn_utils
copied __init__.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/nn_utils

copied utils.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/printing
copied __init__.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/printing

copied env_picture.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/visualization
copied __init__.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils/visualization

copied __init__.py to .temp\2022-6-15 19-44-17/pyfiles-backup/utils


random_train_env
getBabyEnv :
	 logDir : .temp\2022-6-15 19-44-17
	 initial_juice : 0.5
	 end_on_boundary_hit : False
	 penalize_boundary_hit : False
	 allow_no_action : False
	 no_action_threshold : 0.7
	 add_exploration : True
	 field_size : (20000, 20000)
	 initial_pos_around_berry : True
	 nberries : 80
	 num_patches : 10
	 patch_size : (2600, 2600)
	 patch_with_agent_at_center : True
	 sampling_type : 0
	 seperation : 2400
	 show : False
	 spawn_radius : 100


Agent :
	 self : <Agent.Agent object at 0x000001BA91F64B48>
	 berryField : <BerryFieldEnv instance>
	 mode : train
	 angle : 45
	 persistence : 0.8
	 worth_offset : 0.0
	 noise : 0.01
	 nstep_transition : [1]
	 reward_patch_discovery : True
	 positive_emphasis : 0
	 add_exploration : True
	 time_memory_delta : 0.01
	 time_memory_exp : 1
	 debug : False
	 debugDir : .temp
	 kwargs : {}


with living cost, rewards scaled by 2/(berryField.REWARD_RATE*MAXSIZE)
rewards are clipped between 0 and 2
Exploration subroutine as an action
initial p_action: 1
The state-transitions being appended 
            every action will be as [[state, action, sum-reward, nextState, done]] where:
            state is the one the model has taken action on,
            sum-reward is the sum of the rewards in the skip-trajectory,
            nextState is the new state after the action was repeated at most skip-steps times,
            done is wether the terminal state was reached.
Rewarding the agent for discovering new patches
Exploration subroutine added
agent now aware of total-juice
total-params:  2034
with living cost, rewards scaled by 2/(berryField.REWARD_RATE*MAXSIZE)
rewards are clipped between 0 and 2
Exploration subroutine as an action
initial p_action: 4
net(
  (feedforward): ModuleList(
    (0): Linear(in_features=39, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.1)
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): LeakyReLU(negative_slope=0.1)
  )
  (final_stage): ModuleList(
    (0): Linear(in_features=16, out_features=8, bias=True)
    (1): LeakyReLU(negative_slope=0.1)
  )
  (valueL): Linear(in_features=8, out_features=1, bias=True)
  (actadvs): Linear(in_features=8, out_features=9, bias=True)
)
PrioritizedBuffer of type replace-min
optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
), num_gradient_steps= 25
optimizing the online-model after every 100 actions
batch size=512, gamma=0.9, alpha=0.95
polyak_tau=0.1, update_freq=5

=== episode:0 Env-steps-taken:49056
action_counts: {0: 1199, 1: 1287, 2: 2046, 3: 1166, 4: 2156, 5: 1199, 6: 7007, 7: 2112, 8: 2069}
picked:  4
episode: 0/2000 -> reward: 6.479166666666667, steps:20241, time-taken: 0.71min, time-elasped: 0.71min
-> berries picked: 4 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 5 | amount-filled: 3.07%
	| action-stats:  [2, 6, 8] [1, 3, 1]
	| approx positives in sample 512: 11
	| approx action-dist in sample 512: [2, 6, 8] [3, 7, 1]
	Time taken saving stuff: 0.06s

=== episode:0 Env-steps-taken:48672
action_counts: {0: 0, 1: 968, 2: 451, 3: 22319, 4: 0, 5: 0, 6: 0, 7: 24934, 8: 0}
picked:  3

==================================================
eval-episode: 0 -> reward: 3.484375, steps: 48672.0, wall-time: 33.98s
-> berries picked: 3 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:1 Env-steps-taken:49824
action_counts: {0: 2519, 1: 3223, 2: 13244, 3: 2145, 4: 2387, 5: 2299, 6: 10229, 7: 2387, 8: 5038}
picked:  7
episode: 1/2000 -> reward: 10.463541666666666, steps:43471, time-taken: 1.28min, time-elasped: 2.56min
-> berries picked: 7 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 13 | amount-filled: 9.65%
	| action-stats:  [2, 6, 8] [4, 6, 3]
	| approx positives in sample 512: 19
	| approx action-dist in sample 512: [2, 6] [10, 9]
	Time taken saving stuff: 0.02s

=== episode:2 Env-steps-taken:56448
action_counts: {0: 1613, 1: 1705, 2: 7678, 3: 1606, 4: 4807, 5: 4334, 6: 4873, 7: 2068, 8: 3443}
picked:  28
episode: 2/2000 -> reward: 45.85416666666666, steps:32127, time-taken: 1.11min, time-elasped: 3.67min
-> berries picked: 28 of 800 | patches-visited: [0, 8, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 43 | amount-filled: 14.52%
	| action-stats:  [2, 4, 5, 6, 8] [16, 7, 4, 9, 7]
	| approx positives in sample 512: 45
	| approx action-dist in sample 512: [2, 4, 5, 6, 8] [19, 7, 4, 10, 5]
	Time taken saving stuff: 0.02s

=== episode:3 Env-steps-taken:52704
action_counts: {0: 1650, 1: 3113, 2: 5032, 3: 2134, 4: 2233, 5: 2563, 6: 6215, 7: 2090, 8: 8525}
picked:  15
episode: 3/2000 -> reward: 27.421875000000004, steps:33555, time-taken: 1.09min, time-elasped: 4.76min
-> berries picked: 15 of 800 | patches-visited: [0, 1, 6, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 61 | amount-filled: 19.61%
	| action-stats:  [1, 2, 4, 5, 6, 8] [1, 19, 8, 8, 10, 15]
	| approx positives in sample 512: 39
	| approx action-dist in sample 512: [2, 4, 5, 6, 8] [16, 6, 4, 7, 6]
	Time taken saving stuff: 0.09s

=== episode:4 Env-steps-taken:54624
action_counts: {0: 1232, 1: 1837, 2: 3003, 3: 1386, 4: 1672, 5: 2596, 6: 2519, 7: 3993, 8: 3840}
picked:  21
episode: 4/2000 -> reward: 35.39062500000001, steps:22078, time-taken: 0.90min, time-elasped: 5.67min
-> berries picked: 21 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 82 | amount-filled: 22.95%
	| action-stats:  [0, 1, 2, 4, 5, 6, 7, 8] [1, 1, 27, 9, 13, 11, 2, 18]
	| approx positives in sample 512: 52
	| approx action-dist in sample 512: [2, 4, 5, 6, 7, 8] [27, 7, 3, 2, 2, 11]
	Time taken saving stuff: 0.10s

=== episode:5 Env-steps-taken:51456
action_counts: {0: 1375, 1: 2838, 2: 3124, 3: 1551, 4: 2134, 5: 3872, 6: 4583, 7: 3960, 8: 3036}
picked:  11
episode: 5/2000 -> reward: 18.942708333333332, steps:26473, time-taken: 0.98min, time-elasped: 6.65min
-> berries picked: 11 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 94 | amount-filled: 26.97%
	| action-stats:  [0, 1, 2, 4, 5, 6, 7, 8] [1, 1, 33, 9, 15, 12, 2, 21]
	| approx positives in sample 512: 49
	| approx action-dist in sample 512: [1, 2, 4, 5, 6, 7, 8] [1, 20, 3, 7, 4, 4, 10]
	Time taken saving stuff: 0.00s

=== episode:6 Env-steps-taken:56256
action_counts: {0: 3586, 1: 4327, 2: 4576, 3: 3674, 4: 3861, 5: 5126, 6: 5489, 7: 11099, 8: 8162}
picked:  27
episode: 6/2000 -> reward: 43.859374999999986, steps:49900, time-taken: 1.63min, time-elasped: 8.28min
-> berries picked: 27 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 122 | amount-filled: 34.53%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 3, 38, 2, 11, 17, 15, 10, 22]
	| approx positives in sample 512: 29
	| approx action-dist in sample 512: [0, 1, 2, 4, 5, 6, 7, 8] [2, 1, 6, 2, 5, 2, 4, 7]
	Time taken saving stuff: 0.12s

=== episode:7 Env-steps-taken:53664
action_counts: {0: 3212, 1: 5808, 2: 10450, 3: 2981, 4: 3740, 5: 4697, 6: 9405, 7: 8437, 8: 4841}
picked:  21
episode: 7/2000 -> reward: 29.390625000000007, steps:53571, time-taken: 1.71min, time-elasped: 10.00min
-> berries picked: 21 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 142 | amount-filled: 42.65%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 7, 43, 3, 13, 19, 19, 10, 24]
	| approx positives in sample 512: 35
	| approx action-dist in sample 512: [1, 2, 4, 5, 6, 7, 8] [2, 11, 7, 6, 2, 3, 4]
	Time taken saving stuff: 0.09s

=== episode:8 Env-steps-taken:55104
action_counts: {0: 2068, 1: 7282, 2: 4521, 3: 2068, 4: 5546, 5: 2387, 6: 3938, 7: 3498, 8: 3278}
picked:  24
episode: 8/2000 -> reward: 37.875, steps:34586, time-taken: 1.20min, time-elasped: 11.21min
-> berries picked: 24 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 167 | amount-filled: 47.89%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 11, 44, 3, 20, 21, 25, 13, 26]
	| approx positives in sample 512: 39
	| approx action-dist in sample 512: [1, 2, 3, 4, 5, 6, 7, 8] [1, 13, 2, 3, 3, 7, 1, 9]
	Time taken saving stuff: 0.11s

=== episode:9 Env-steps-taken:55200
action_counts: {0: 2332, 1: 4774, 2: 3575, 3: 5104, 4: 6193, 5: 6567, 6: 4246, 7: 4455, 8: 3642}
picked:  26
episode: 9/2000 -> reward: 39.36458333333333, steps:40888, time-taken: 1.43min, time-elasped: 12.64min
-> berries picked: 26 of 800 | patches-visited: [0, 3, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 195 | amount-filled: 54.09%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 17, 47, 6, 25, 23, 30, 14, 29]
	| approx positives in sample 512: 40
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 5, 9, 1, 1, 3, 6, 7, 6]
	Time taken saving stuff: 0.00s

=== episode:10 Env-steps-taken:60192
action_counts: {0: 2871, 1: 5070, 2: 4389, 3: 6732, 4: 7403, 5: 3421, 6: 6314, 7: 6292, 8: 2937}
picked:  43
episode: 10/2000 -> reward: 65.27604166666663, steps:45429, time-taken: 1.64min, time-elasped: 14.28min
-> berries picked: 43 of 800 | patches-visited: [0, 6, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 240 | amount-filled: 60.97%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 23, 53, 11, 36, 25, 36, 17, 33]
	| approx positives in sample 512: 42
	| approx action-dist in sample 512: [1, 2, 3, 4, 5, 6, 7, 8] [4, 5, 4, 6, 3, 5, 7, 8]
	Time taken saving stuff: 0.14s

=== episode:1 Env-steps-taken:53376
action_counts: {0: 0, 1: 957, 2: 220, 3: 15277, 4: 3245, 5: 374, 6: 1584, 7: 14740, 8: 968}
picked:  19

==================================================
eval-episode: 10 -> reward: 28.90104166666668, steps: 37365.0, wall-time: 26.17s
-> berries picked: 19 of 800 | patches-visited: [0, 1] | juice left:-0.00
==================================================


=== episode:11 Env-steps-taken:62304
action_counts: {0: 3718, 1: 3453, 2: 4906, 3: 7579, 4: 7029, 5: 4169, 6: 4895, 7: 8723, 8: 5269}
picked:  46
episode: 11/2000 -> reward: 76.26041666666667, steps:49741, time-taken: 1.69min, time-elasped: 16.41min
-> berries picked: 46 of 800 | patches-visited: [0, 1, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 288 | amount-filled: 68.50%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 25, 61, 17, 46, 29, 46, 20, 37]
	| approx positives in sample 512: 43
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 2, 8, 2, 8, 7, 4, 4, 7]
	Time taken saving stuff: 0.10s

=== episode:12 Env-steps-taken:57504
action_counts: {0: 4158, 1: 7073, 2: 5027, 3: 10780, 4: 4631, 5: 3553, 6: 8162, 7: 6413, 8: 3180}
picked:  36
episode: 12/2000 -> reward: 49.31249999999997, steps:52977, time-taken: 1.74min, time-elasped: 18.15min
-> berries picked: 36 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 324 | amount-filled: 76.53%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 27, 73, 21, 53, 33, 52, 21, 37]
	| approx positives in sample 512: 51
	| approx action-dist in sample 512: [1, 2, 3, 4, 5, 6, 7, 8] [6, 12, 6, 6, 9, 4, 2, 6]
	Time taken saving stuff: 0.10s

=== episode:13 Env-steps-taken:60864
action_counts: {0: 2233, 1: 5467, 2: 3080, 3: 3883, 4: 5918, 5: 3080, 6: 6072, 7: 5456, 8: 4827}
picked:  48
episode: 13/2000 -> reward: 67.74999999999997, steps:40016, time-taken: 1.33min, time-elasped: 19.48min
-> berries picked: 48 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 373 | amount-filled: 82.60%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 35, 78, 26, 64, 36, 60, 24, 42]
	| approx positives in sample 512: 53
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 3, 11, 7, 7, 4, 9, 5, 6]
	Time taken saving stuff: 0.10s

=== episode:14 Env-steps-taken:57504
action_counts: {0: 2948, 1: 5973, 2: 5082, 3: 7810, 4: 6050, 5: 2002, 6: 4455, 7: 5489, 8: 4830}
picked:  29
episode: 14/2000 -> reward: 47.859374999999986, steps:44639, time-taken: 1.48min, time-elasped: 20.96min
-> berries picked: 29 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 401 | amount-filled: 89.36%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 39, 83, 29, 68, 36, 68, 25, 44]
	| approx positives in sample 512: 46
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 2, 6, 5, 12, 2, 8, 6, 4]
	Time taken saving stuff: 0.00s

=== episode:15 Env-steps-taken:61152
action_counts: {0: 2288, 1: 6138, 2: 2794, 3: 4983, 4: 4631, 5: 2849, 6: 5797, 7: 5368, 8: 3037}
picked:  41
episode: 15/2000 -> reward: 69.28645833333331, steps:37885, time-taken: 1.51min, time-elasped: 22.48min
-> berries picked: 41 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 443 | amount-filled: 95.10%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 48, 84, 32, 80, 40, 76, 28, 45]
	| approx positives in sample 512: 58
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 7, 9, 3, 10, 3, 10, 8, 7]
	Time taken saving stuff: 0.01s

=== episode:16 Env-steps-taken:56736
action_counts: {0: 2354, 1: 3267, 2: 2299, 3: 4620, 4: 7601, 5: 2134, 6: 5148, 7: 4345, 8: 4379}
picked:  32
episode: 16/2000 -> reward: 46.333333333333314, steps:36147, time-taken: 1.64min, time-elasped: 24.12min
-> berries picked: 32 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 476 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 52, 85, 35, 90, 43, 79, 30, 48]
	| approx positives in sample 512: 43
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 3, 5, 7, 7, 4, 8, 2, 6]
	Time taken saving stuff: 0.11s

=== episode:17 Env-steps-taken:53760
action_counts: {0: 715, 1: 2068, 2: 1584, 3: 2057, 4: 1639, 5: 1199, 6: 1089, 7: 1584, 8: 2256}
picked:  17
episode: 17/2000 -> reward: 30.911458333333343, steps:14191, time-taken: 0.72min, time-elasped: 24.85min
-> berries picked: 17 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 494 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 57, 86, 38, 94, 44, 82, 30, 49]
	| approx positives in sample 512: 39
	| approx action-dist in sample 512: [1, 2, 3, 4, 5, 6, 7, 8] [8, 7, 1, 7, 1, 6, 3, 6]
	Time taken saving stuff: 0.10s

=== episode:18 Env-steps-taken:54144
action_counts: {0: 1155, 1: 3971, 2: 1881, 3: 2310, 4: 2112, 5: 1133, 6: 1892, 7: 1804, 8: 1376}
picked:  18
episode: 18/2000 -> reward: 32.90625000000001, steps:17634, time-taken: 0.87min, time-elasped: 25.72min
-> berries picked: 18 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 512 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 61, 86, 43, 96, 45, 84, 32, 51]
	| approx positives in sample 512: 41
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 4, 5, 4, 6, 2, 7, 3, 7]
	Time taken saving stuff: 0.11s

=== episode:19 Env-steps-taken:61440
action_counts: {0: 3630, 1: 8735, 2: 4631, 3: 7480, 4: 7095, 5: 3454, 6: 6314, 7: 6776, 8: 4180}
picked:  46
episode: 19/2000 -> reward: 70.76041666666664, steps:52295, time-taken: 1.79min, time-elasped: 27.52min
-> berries picked: 46 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 558 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 64, 92, 48, 105, 49, 91, 41, 54]
	| approx positives in sample 512: 35
	| approx action-dist in sample 512: [1, 2, 3, 4, 5, 6, 7, 8] [3, 3, 4, 8, 4, 7, 4, 2]
	Time taken saving stuff: 0.10s

=== episode:20 Env-steps-taken:65760
action_counts: {0: 3399, 1: 5951, 2: 3663, 3: 5665, 4: 4961, 5: 3641, 6: 6446, 7: 5643, 8: 5982}
picked:  57
episode: 20/2000 -> reward: 93.20312500000004, steps:45351, time-taken: 1.78min, time-elasped: 29.30min
-> berries picked: 57 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 614 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 67, 96, 53, 119, 56, 100, 48, 59]
	| approx positives in sample 512: 32
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 3, 5, 5, 5, 3, 2, 3, 5]
	Time taken saving stuff: 0.14s

=== episode:2 Env-steps-taken:60768
action_counts: {0: 275, 1: 14212, 2: 1452, 3: 3773, 4: 11858, 5: 583, 6: 6147, 7: 11000, 8: 1584}
picked:  43

==================================================
eval-episode: 20 -> reward: 67.27604166666663, steps: 50884.0, wall-time: 36.10s
-> berries picked: 43 of 800 | patches-visited: [1, 7] | juice left:-0.00
==================================================


=== episode:21 Env-steps-taken:56352
action_counts: {0: 4125, 1: 4273, 2: 2882, 3: 8118, 4: 6347, 5: 2816, 6: 4114, 7: 8833, 8: 2992}
picked:  27
episode: 21/2000 -> reward: 44.35937499999999, steps:44500, time-taken: 1.54min, time-elasped: 31.45min
-> berries picked: 27 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 638 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [17, 68, 98, 54, 127, 55, 103, 55, 61]
	| approx positives in sample 512: 45
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 4, 8, 8, 3, 3, 5, 9, 4]
	Time taken saving stuff: 0.00s

=== episode:22 Env-steps-taken:64512
action_counts: {0: 4323, 1: 8327, 2: 4554, 3: 7073, 4: 5698, 5: 3377, 6: 4774, 7: 5236, 8: 6454}
picked:  58
episode: 22/2000 -> reward: 86.69791666666671, steps:49816, time-taken: 1.89min, time-elasped: 33.34min
-> berries picked: 58 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 692 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 78, 102, 67, 135, 59, 109, 60, 64]
	| approx positives in sample 512: 35
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 4, 2, 8, 5, 3, 2, 2, 6]
	Time taken saving stuff: 0.11s

=== episode:23 Env-steps-taken:55872
action_counts: {0: 1034, 1: 2486, 2: 1749, 3: 1760, 4: 1969, 5: 1276, 6: 1716, 7: 1364, 8: 2498}
picked:  27
episode: 23/2000 -> reward: 41.85937499999999, steps:15852, time-taken: 0.90min, time-elasped: 34.25min
-> berries picked: 27 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 719 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 79, 107, 70, 138, 63, 112, 63, 69]
	| approx positives in sample 512: 45
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 8, 7, 3, 6, 4, 6, 5, 5]
	Time taken saving stuff: 0.00s

=== episode:24 Env-steps-taken:68064
action_counts: {0: 2574, 1: 4620, 2: 3630, 3: 4928, 4: 6479, 5: 2695, 6: 4477, 7: 5533, 8: 4268}
picked:  68
episode: 24/2000 -> reward: 106.15104166666676, steps:39204, time-taken: 1.48min, time-elasped: 35.73min
-> berries picked: 68 of 800 | patches-visited: [0, 5, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 786 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [20, 87, 111, 75, 154, 69, 125, 71, 74]
	| approx positives in sample 512: 44
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 6, 6, 3, 9, 2, 6, 5, 6]
	Time taken saving stuff: 0.09s

=== episode:25 Env-steps-taken:56448
action_counts: {0: 3179, 1: 5830, 2: 4122, 3: 12628, 4: 4796, 5: 2673, 6: 3212, 7: 3432, 8: 12056}
picked:  29
episode: 25/2000 -> reward: 44.84895833333333, steps:51928, time-taken: 1.79min, time-elasped: 37.52min
-> berries picked: 29 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 808 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [20, 90, 113, 80, 159, 69, 129, 72, 76]
	| approx positives in sample 512: 55
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 10, 6, 9, 9, 1, 7, 7, 5]
	Time taken saving stuff: 0.00s

=== episode:26 Env-steps-taken:61632
action_counts: {0: 3025, 1: 5533, 2: 3641, 3: 4697, 4: 4004, 5: 2156, 6: 2882, 7: 3355, 8: 4096}
picked:  47
episode: 26/2000 -> reward: 71.75520833333333, steps:33389, time-taken: 1.29min, time-elasped: 38.82min
-> berries picked: 47 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 852 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [24, 102, 117, 87, 165, 73, 131, 73, 80]
	| approx positives in sample 512: 62
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 10, 4, 9, 10, 2, 11, 6, 9]
	Time taken saving stuff: 0.10s

=== episode:27 Env-steps-taken:69504
action_counts: {0: 4323, 1: 8514, 2: 3707, 3: 5038, 4: 6512, 5: 4136, 6: 6347, 7: 6336, 8: 5500}
picked:  67
episode: 27/2000 -> reward: 113.65104166666679, steps:50413, time-taken: 1.86min, time-elasped: 40.68min
-> berries picked: 67 of 800 | patches-visited: [0, 7, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 912 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [27, 111, 123, 94, 177, 82, 134, 81, 83]
	| approx positives in sample 512: 49
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 4, 3, 9, 9, 5, 8, 4, 5]
	Time taken saving stuff: 0.00s

=== episode:28 Env-steps-taken:58560
action_counts: {0: 3724, 1: 9130, 2: 2926, 3: 5236, 4: 3322, 5: 2442, 6: 2255, 7: 3102, 8: 5599}
picked:  33
episode: 28/2000 -> reward: 56.82812499999998, steps:37736, time-taken: 1.51min, time-elasped: 42.19min
-> berries picked: 33 of 800 | patches-visited: [0, 3, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 939 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [27, 114, 129, 101, 181, 82, 134, 85, 86]
	| approx positives in sample 512: 55
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 6, 7, 4, 14, 6, 6, 6, 4]
	Time taken saving stuff: 0.10s

=== episode:29 Env-steps-taken:62400
action_counts: {0: 3245, 1: 5313, 2: 2717, 3: 4642, 4: 4301, 5: 2629, 6: 4224, 7: 6842, 8: 3656}
picked:  42
episode: 29/2000 -> reward: 75.78125, steps:37569, time-taken: 1.43min, time-elasped: 43.63min
-> berries picked: 42 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 973 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [27, 114, 133, 108, 185, 86, 139, 92, 89]
	| approx positives in sample 512: 59
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 5, 3, 7, 18, 3, 9, 6, 5]
	Time taken saving stuff: 0.10s

=== episode:30 Env-steps-taken:56064
action_counts: {0: 2695, 1: 3542, 2: 3223, 3: 3575, 4: 3146, 5: 1529, 6: 2057, 7: 2244, 8: 2927}
picked:  35
episode: 30/2000 -> reward: 41.81770833333332, steps:24938, time-taken: 0.90min, time-elasped: 44.53min
-> berries picked: 35 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1007 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [28, 122, 138, 112, 198, 87, 140, 92, 90]
	| approx positives in sample 512: 58
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 6, 4, 10, 16, 6, 2, 7, 6]
	Time taken saving stuff: 0.05s

=== episode:3 Env-steps-taken:52224
action_counts: {0: 440, 1: 24768, 2: 242, 3: 33, 4: 803, 5: 198, 6: 671, 7: 1210, 8: 23859}
picked:  14

==================================================
eval-episode: 30 -> reward: 21.927083333333336, steps: 52224.0, wall-time: 33.64s
-> berries picked: 14 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:31 Env-steps-taken:61248
action_counts: {0: 7414, 1: 10692, 2: 4543, 3: 7689, 4: 5951, 5: 3289, 6: 4939, 7: 7106, 8: 9625}
picked:  51
episode: 31/2000 -> reward: 68.73437499999997, steps:61248, time-taken: 1.96min, time-elasped: 47.05min
-> berries picked: 51 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1046 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [32, 129, 141, 121, 207, 89, 141, 94, 92]
	| approx positives in sample 512: 60
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 7, 2, 9, 15, 2, 6, 7, 6]
	Time taken saving stuff: 0.00s

=== episode:32 Env-steps-taken:66912
action_counts: {0: 4792, 1: 5797, 2: 4026, 3: 4818, 4: 7887, 5: 3531, 6: 4939, 7: 6435, 8: 4620}
picked:  65
episode: 32/2000 -> reward: 101.16145833333339, steps:46845, time-taken: 1.69min, time-elasped: 48.75min
-> berries picked: 65 of 800 | patches-visited: [0, 2, 6, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1106 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [34, 135, 149, 131, 222, 91, 145, 103, 96]
	| approx positives in sample 512: 64
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 12, 5, 11, 8, 5, 5, 11, 6]
	Time taken saving stuff: 0.11s

=== episode:33 Env-steps-taken:61056
action_counts: {0: 3454, 1: 5137, 2: 3421, 3: 4917, 4: 4796, 5: 3080, 6: 3322, 7: 4323, 8: 3894}
picked:  47
episode: 33/2000 -> reward: 68.7552083333333, steps:36344, time-taken: 1.48min, time-elasped: 50.23min
-> berries picked: 47 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1150 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [36, 143, 152, 136, 232, 96, 152, 106, 97]
	| approx positives in sample 512: 64
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 3, 4, 15, 17, 8, 2, 5, 7]
	Time taken saving stuff: 0.00s

=== episode:34 Env-steps-taken:57312
action_counts: {0: 3773, 1: 3212, 2: 2464, 3: 3817, 4: 3861, 5: 2156, 6: 2376, 7: 4653, 8: 4377}
picked:  33
episode: 34/2000 -> reward: 49.32812499999998, steps:30689, time-taken: 1.23min, time-elasped: 51.46min
-> berries picked: 33 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1176 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [38, 146, 154, 141, 238, 99, 154, 108, 98]
	| approx positives in sample 512: 64
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 12, 11, 4, 9, 3, 11, 5, 8]
	Time taken saving stuff: 0.01s

=== episode:35 Env-steps-taken:60288
action_counts: {0: 6908, 1: 6017, 2: 4345, 3: 6006, 4: 7227, 5: 2794, 6: 3949, 7: 6358, 8: 3477}
picked:  42
episode: 35/2000 -> reward: 63.78124999999996, steps:47081, time-taken: 1.49min, time-elasped: 52.95min
-> berries picked: 42 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1211 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [39, 155, 156, 145, 244, 102, 158, 113, 99]
	| approx positives in sample 512: 73
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 7, 11, 11, 8, 7, 8, 12, 3]
	Time taken saving stuff: 0.00s

=== episode:36 Env-steps-taken:60576
action_counts: {0: 1914, 1: 2145, 2: 1980, 3: 3201, 4: 4213, 5: 2387, 6: 3872, 7: 3498, 8: 1816}
picked:  42
episode: 36/2000 -> reward: 67.28124999999997, steps:25026, time-taken: 1.00min, time-elasped: 53.95min
-> berries picked: 42 of 800 | patches-visited: [0, 3, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1251 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [40, 161, 158, 149, 251, 108, 166, 117, 101]
	| approx positives in sample 512: 56
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 4, 8, 3, 8, 5, 10, 8, 6]
	Time taken saving stuff: 0.10s

=== episode:37 Env-steps-taken:64320
action_counts: {0: 3653, 1: 2981, 2: 5687, 3: 4521, 4: 5236, 5: 3883, 6: 4433, 7: 5269, 8: 5104}
picked:  56
episode: 37/2000 -> reward: 85.70833333333336, steps:40767, time-taken: 1.36min, time-elasped: 55.32min
-> berries picked: 56 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1302 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [43, 163, 167, 155, 266, 112, 171, 122, 103]
	| approx positives in sample 512: 73
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 11, 7, 9, 17, 1, 6, 9, 9]
	Time taken saving stuff: 0.00s

=== episode:38 Env-steps-taken:61440
action_counts: {0: 3718, 1: 4675, 2: 3652, 3: 3663, 4: 7271, 5: 3355, 6: 5049, 7: 5257, 8: 3443}
picked:  47
episode: 38/2000 -> reward: 70.75520833333333, steps:40083, time-taken: 1.37min, time-elasped: 56.68min
-> berries picked: 47 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1338 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [43, 172, 171, 156, 273, 117, 173, 128, 105]
	| approx positives in sample 512: 75
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 5, 10, 10, 22, 5, 9, 6, 6]
	Time taken saving stuff: 0.12s

=== episode:39 Env-steps-taken:64608
action_counts: {0: 5874, 1: 4917, 2: 6127, 3: 3619, 4: 5401, 5: 3850, 6: 5016, 7: 7392, 8: 5380}
picked:  55
episode: 39/2000 -> reward: 87.2135416666667, steps:47576, time-taken: 1.59min, time-elasped: 58.27min
-> berries picked: 55 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1388 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [44, 178, 183, 160, 278, 122, 178, 137, 108]
	| approx positives in sample 512: 67
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 8, 5, 6, 15, 6, 13, 7, 4]
	Time taken saving stuff: 0.00s

=== episode:40 Env-steps-taken:58176
action_counts: {0: 3465, 1: 2189, 2: 2772, 3: 1980, 4: 2838, 5: 2640, 6: 2508, 7: 3432, 8: 2352}
picked:  34
episode: 40/2000 -> reward: 54.82291666666664, steps:24176, time-taken: 0.96min, time-elasped: 59.24min
-> berries picked: 34 of 800 | patches-visited: [0, 1, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1422 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [50, 182, 185, 163, 282, 127, 185, 138, 110]
	| approx positives in sample 512: 69
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 9, 11, 5, 21, 4, 6, 3, 7]
	Time taken saving stuff: 0.16s

=== episode:4 Env-steps-taken:73824
action_counts: {0: 3685, 1: 4532, 2: 2805, 3: 2937, 4: 15169, 5: 2893, 6: 3124, 7: 6028, 8: 13135}
picked:  92

==================================================
eval-episode: 40 -> reward: 135.02083333333348, steps: 54308.0, wall-time: 42.90s
-> berries picked: 92 of 800 | patches-visited: [0, 1] | juice left:-0.00
==================================================


=== episode:41 Env-steps-taken:59040
action_counts: {0: 5940, 1: 2904, 2: 3355, 3: 2882, 4: 3894, 5: 2871, 6: 3729, 7: 4840, 8: 3211}
picked:  39
episode: 41/2000 -> reward: 57.296874999999964, steps:33626, time-taken: 1.37min, time-elasped: 61.33min
-> berries picked: 39 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1451 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [54, 184, 188, 166, 287, 133, 187, 141, 111]
	| approx positives in sample 512: 73
	| approx action-dist in sample 512: [1, 2, 3, 4, 5, 6, 7, 8] [14, 16, 7, 12, 4, 4, 9, 7]
	Time taken saving stuff: 0.02s

=== episode:42 Env-steps-taken:59232
action_counts: {0: 2783, 1: 3630, 2: 2541, 3: 2222, 4: 3663, 5: 1991, 6: 2750, 7: 3696, 8: 2371}
picked:  40
episode: 42/2000 -> reward: 59.291666666666636, steps:25647, time-taken: 1.13min, time-elasped: 62.46min
-> berries picked: 40 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1485 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [55, 188, 194, 170, 293, 138, 190, 144, 113]
	| approx positives in sample 512: 78
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 5, 10, 17, 19, 4, 5, 8, 7]
	Time taken saving stuff: 0.00s

=== episode:43 Env-steps-taken:55872
action_counts: {0: 2486, 1: 2970, 2: 2585, 3: 2288, 4: 2255, 5: 1452, 6: 1418, 7: 2090, 8: 1617}
picked:  25
episode: 43/2000 -> reward: 41.86979166666666, steps:19161, time-taken: 0.83min, time-elasped: 63.29min
-> berries picked: 25 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1507 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [56, 193, 198, 174, 293, 139, 192, 148, 114]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 12, 9, 13, 17, 6, 9, 16, 11]
	Time taken saving stuff: 0.00s

=== episode:44 Env-steps-taken:59520
action_counts: {0: 5775, 1: 5830, 2: 3625, 3: 3927, 4: 5280, 5: 4565, 6: 5280, 7: 6479, 8: 6116}
picked:  39
episode: 44/2000 -> reward: 60.796874999999964, steps:46877, time-taken: 1.66min, time-elasped: 64.95min
-> berries picked: 39 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1538 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [59, 197, 200, 174, 298, 149, 195, 150, 116]
	| approx positives in sample 512: 67
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 10, 10, 8, 10, 5, 10, 4, 8]
	Time taken saving stuff: 0.11s

=== episode:45 Env-steps-taken:58464
action_counts: {0: 2772, 1: 2772, 2: 2728, 3: 2090, 4: 2805, 5: 2596, 6: 4114, 7: 3839, 8: 2033}
picked:  37
episode: 45/2000 -> reward: 55.30729166666664, steps:25749, time-taken: 1.05min, time-elasped: 66.00min
-> berries picked: 37 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1567 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [60, 202, 203, 174, 304, 154, 196, 152, 122]
	| approx positives in sample 512: 85
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 10, 14, 7, 19, 7, 8, 9, 8]
	Time taken saving stuff: 0.00s

=== episode:46 Env-steps-taken:67200
action_counts: {0: 5819, 1: 6259, 2: 5115, 3: 5060, 4: 6611, 5: 5973, 6: 5896, 7: 7095, 8: 5171}
picked:  65
episode: 46/2000 -> reward: 100.66145833333341, steps:52999, time-taken: 1.98min, time-elasped: 67.99min
-> berries picked: 65 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1621 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [63, 208, 206, 176, 314, 165, 203, 159, 127]
	| approx positives in sample 512: 97
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 9, 10, 9, 18, 10, 14, 12, 11]
	Time taken saving stuff: 0.11s

=== episode:47 Env-steps-taken:63456
action_counts: {0: 7601, 1: 5632, 2: 4546, 3: 3564, 4: 5544, 5: 3905, 6: 3872, 7: 5544, 8: 5907}
picked:  57
episode: 47/2000 -> reward: 81.20312500000001, steps:46115, time-taken: 1.55min, time-elasped: 69.54min
-> berries picked: 57 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1675 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [68, 216, 211, 182, 324, 172, 205, 165, 132]
	| approx positives in sample 512: 89
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 20, 12, 9, 16, 6, 6, 9, 8]
	Time taken saving stuff: 0.00s

=== episode:48 Env-steps-taken:64608
action_counts: {0: 4741, 1: 3762, 2: 3388, 3: 4367, 4: 5632, 5: 4257, 6: 4356, 7: 5533, 8: 3939}
picked:  61
episode: 48/2000 -> reward: 87.18229166666671, steps:39975, time-taken: 1.41min, time-elasped: 70.94min
-> berries picked: 61 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1730 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [69, 223, 216, 189, 328, 181, 215, 174, 135]
	| approx positives in sample 512: 86
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 8, 6, 15, 14, 10, 12, 10, 8]
	Time taken saving stuff: 0.11s

=== episode:49 Env-steps-taken:70656
action_counts: {0: 7227, 1: 7073, 2: 6875, 3: 5764, 4: 8855, 5: 6116, 6: 6897, 7: 7753, 8: 6039}
picked:  83
episode: 49/2000 -> reward: 119.56770833333348, steps:62599, time-taken: 2.11min, time-elasped: 73.06min
-> berries picked: 83 of 800 | patches-visited: [0, 1, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1801 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [71, 232, 229, 198, 336, 189, 226, 181, 139]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 11, 18, 11, 17, 12, 11, 5, 5]
	Time taken saving stuff: 0.09s

=== episode:50 Env-steps-taken:58944
action_counts: {0: 3723, 1: 3916, 2: 2948, 3: 3564, 4: 3608, 5: 2299, 6: 2970, 7: 4202, 8: 3014}
picked:  36
episode: 50/2000 -> reward: 58.81249999999998, steps:30244, time-taken: 1.34min, time-elasped: 74.40min
-> berries picked: 36 of 800 | patches-visited: [0, 1, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1830 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [74, 237, 233, 204, 340, 190, 228, 180, 144]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 13, 7, 12, 22, 6, 9, 12, 7]
	Time taken saving stuff: 0.15s

=== episode:5 Env-steps-taken:72672
action_counts: {0: 9581, 1: 4752, 2: 3641, 3: 1045, 4: 6897, 5: 1881, 6: 3146, 7: 3564, 8: 17356}
picked:  88

==================================================
eval-episode: 50 -> reward: 130.0416666666668, steps: 51863.0, wall-time: 34.84s
-> berries picked: 88 of 800 | patches-visited: [1, 5, 7] | juice left:-0.00
==================================================


=== episode:51 Env-steps-taken:57600
action_counts: {0: 2915, 1: 2783, 2: 3190, 3: 2189, 4: 2750, 5: 2827, 6: 3553, 7: 3025, 8: 3379}
picked:  34
episode: 51/2000 -> reward: 50.82291666666664, steps:26611, time-taken: 1.03min, time-elasped: 76.01min
-> berries picked: 34 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1858 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [73, 240, 243, 206, 344, 189, 233, 183, 147]
	| approx positives in sample 512: 83
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 8, 8, 10, 13, 3, 14, 15, 9]
	Time taken saving stuff: 0.00s

=== episode:52 Env-steps-taken:60384
action_counts: {0: 2915, 1: 3443, 2: 2937, 3: 2123, 4: 2651, 5: 1793, 6: 3245, 7: 2541, 8: 4068}
picked:  42
episode: 52/2000 -> reward: 66.28124999999996, steps:25716, time-taken: 1.05min, time-elasped: 77.06min
-> berries picked: 42 of 800 | patches-visited: [0, 5, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1896 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [75, 247, 248, 210, 348, 193, 238, 184, 153]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 19, 16, 13, 17, 7, 18, 6, 16]
	Time taken saving stuff: 0.02s

=== episode:53 Env-steps-taken:62112
action_counts: {0: 3883, 1: 4477, 2: 4323, 3: 4279, 4: 4554, 5: 2321, 6: 3685, 7: 3212, 8: 4318}
picked:  47
episode: 53/2000 -> reward: 74.25520833333331, steps:35052, time-taken: 1.27min, time-elasped: 78.34min
-> berries picked: 47 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1940 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [76, 255, 249, 215, 361, 199, 244, 186, 155]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 14, 16, 11, 15, 5, 13, 16, 6]
	Time taken saving stuff: 0.00s

=== episode:54 Env-steps-taken:56448
action_counts: {0: 3146, 1: 3344, 2: 3003, 3: 3036, 4: 2233, 5: 2387, 6: 2530, 7: 2585, 8: 2920}
picked:  28
episode: 54/2000 -> reward: 43.85416666666665, steps:25184, time-taken: 0.92min, time-elasped: 79.26min
-> berries picked: 28 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1964 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [76, 257, 257, 219, 364, 202, 245, 189, 155]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 14, 11, 16, 21, 14, 6, 11, 14]
	Time taken saving stuff: 0.10s

=== episode:55 Env-steps-taken:68928
action_counts: {0: 5687, 1: 5225, 2: 6314, 3: 4719, 4: 5148, 5: 6028, 6: 5830, 7: 5973, 8: 5605}
picked:  79
episode: 55/2000 -> reward: 109.58854166666677, steps:50529, time-taken: 1.61min, time-elasped: 80.88min
-> berries picked: 79 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2031 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [86, 263, 265, 226, 370, 214, 257, 193, 157]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 18, 14, 15, 15, 8, 9, 11, 12]
	Time taken saving stuff: 0.00s

=== episode:56 Env-steps-taken:57216
action_counts: {0: 4763, 1: 2772, 2: 3113, 3: 2453, 4: 3861, 5: 4158, 6: 3960, 7: 2893, 8: 3026}
picked:  37
episode: 56/2000 -> reward: 47.80729166666664, steps:30999, time-taken: 1.06min, time-elasped: 81.94min
-> berries picked: 37 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2063 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [88, 267, 269, 229, 373, 222, 264, 194, 157]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 10, 12, 15, 21, 6, 6, 13, 13]
	Time taken saving stuff: 0.00s

=== episode:57 Env-steps-taken:58176
action_counts: {0: 3025, 1: 4499, 2: 3443, 3: 2959, 4: 2871, 5: 3476, 6: 2750, 7: 2475, 8: 2608}
picked:  35
episode: 57/2000 -> reward: 52.817708333333314, steps:28106, time-taken: 1.05min, time-elasped: 82.99min
-> berries picked: 35 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2093 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [90, 270, 276, 234, 375, 226, 269, 195, 158]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 13, 21, 11, 22, 8, 14, 14, 6]
	Time taken saving stuff: 0.11s

=== episode:58 Env-steps-taken:65184
action_counts: {0: 5654, 1: 8580, 2: 6050, 3: 7007, 4: 5489, 5: 4829, 6: 5038, 7: 5467, 8: 5556}
picked:  60
episode: 58/2000 -> reward: 89.18750000000001, steps:53670, time-taken: 1.56min, time-elasped: 84.55min
-> berries picked: 60 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2134 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [91, 280, 283, 238, 379, 230, 270, 202, 161]
	| approx positives in sample 512: 124
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 15, 13, 19, 15, 9, 15, 15, 12]
	Time taken saving stuff: 0.00s

=== episode:59 Env-steps-taken:62208
action_counts: {0: 5599, 1: 4026, 2: 4015, 3: 4015, 4: 4411, 5: 5929, 6: 4631, 7: 4642, 8: 4926}
picked:  53
episode: 59/2000 -> reward: 73.72395833333334, steps:42194, time-taken: 1.43min, time-elasped: 85.98min
-> berries picked: 53 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2182 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [92, 283, 290, 240, 391, 240, 277, 205, 164]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 17, 14, 9, 20, 12, 10, 15, 12]
	Time taken saving stuff: 0.10s

=== episode:60 Env-steps-taken:62592
action_counts: {0: 3300, 1: 4246, 2: 3487, 3: 2376, 4: 3630, 5: 4081, 6: 4598, 7: 3344, 8: 2993}
picked:  49
episode: 60/2000 -> reward: 77.74479166666667, steps:32055, time-taken: 1.16min, time-elasped: 87.15min
-> berries picked: 49 of 800 | patches-visited: [0, 3, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2225 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [95, 286, 296, 244, 398, 245, 284, 208, 169]
	| approx positives in sample 512: 120
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 12, 14, 18, 26, 9, 10, 15, 7]
	Time taken saving stuff: 0.05s

=== episode:6 Env-steps-taken:59520
action_counts: {0: 22932, 1: 1309, 2: 2420, 3: 1100, 4: 2761, 5: 1771, 6: 1056, 7: 891, 8: 22209}
picked:  40

==================================================
eval-episode: 60 -> reward: 58.80208333333329, steps: 56449.0, wall-time: 34.70s
-> berries picked: 40 of 800 | patches-visited: [0, 1] | juice left:-0.00
==================================================


=== episode:61 Env-steps-taken:54624
action_counts: {0: 9856, 1: 4972, 2: 4235, 3: 3938, 4: 3696, 5: 3619, 6: 4037, 7: 7755, 8: 8922}
picked:  29
episode: 61/2000 -> reward: 34.34895833333334, steps:51030, time-taken: 1.52min, time-elasped: 89.25min
-> berries picked: 29 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2238 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [99, 289, 300, 246, 393, 243, 286, 213, 169]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 10, 11, 8, 19, 5, 13, 15, 9]
	Time taken saving stuff: 0.12s

=== episode:62 Env-steps-taken:57504
action_counts: {0: 3201, 1: 3355, 2: 4389, 3: 2893, 4: 3102, 5: 3146, 6: 3553, 7: 2321, 8: 2079}
picked:  34
episode: 62/2000 -> reward: 49.32291666666666, steps:28039, time-taken: 0.99min, time-elasped: 90.25min
-> berries picked: 34 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2264 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [102, 293, 302, 250, 399, 244, 289, 216, 169]
	| approx positives in sample 512: 128
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 14, 13, 15, 16, 8, 16, 22, 18]
	Time taken saving stuff: 0.11s

=== episode:63 Env-steps-taken:67200
action_counts: {0: 6864, 1: 3894, 2: 6391, 3: 5291, 4: 5038, 5: 5478, 6: 4994, 7: 3817, 8: 2692}
picked:  66
episode: 63/2000 -> reward: 101.65625000000007, steps:44459, time-taken: 1.48min, time-elasped: 91.74min
-> berries picked: 66 of 800 | patches-visited: [0, 5, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2326 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [102, 298, 312, 264, 408, 250, 299, 220, 173]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 20, 19, 13, 15, 10, 15, 15, 7]
	Time taken saving stuff: 0.08s

=== episode:64 Env-steps-taken:61920
action_counts: {0: 5265, 1: 7711, 2: 7128, 3: 5467, 4: 5621, 5: 4873, 6: 5038, 7: 4268, 8: 4136}
picked:  49
episode: 64/2000 -> reward: 73.24479166666664, steps:49507, time-taken: 1.55min, time-elasped: 93.29min
-> berries picked: 49 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2357 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [105, 301, 321, 265, 411, 248, 308, 223, 175]
	| approx positives in sample 512: 120
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 16, 18, 15, 22, 3, 12, 16, 14]
	Time taken saving stuff: 0.02s

=== episode:65 Env-steps-taken:64992
action_counts: {0: 6512, 1: 6336, 2: 7667, 3: 4840, 4: 4807, 5: 7645, 6: 6413, 7: 4950, 8: 3928}
picked:  68
episode: 65/2000 -> reward: 89.14583333333336, steps:53098, time-taken: 1.68min, time-elasped: 94.97min
-> berries picked: 68 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2420 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [107, 309, 329, 278, 421, 256, 317, 225, 178]
	| approx positives in sample 512: 120
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 17, 12, 19, 22, 10, 13, 12, 10]
	Time taken saving stuff: 0.11s

=== episode:66 Env-steps-taken:66144
action_counts: {0: 5995, 1: 5005, 2: 6182, 3: 4279, 4: 4675, 5: 3828, 6: 4510, 7: 3575, 8: 2707}
picked:  59
episode: 66/2000 -> reward: 95.19270833333339, steps:40756, time-taken: 1.38min, time-elasped: 96.35min
-> berries picked: 59 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2470 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [109, 314, 340, 284, 428, 261, 324, 226, 184]
	| approx positives in sample 512: 131
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 24, 12, 17, 30, 8, 15, 11, 11]
	Time taken saving stuff: 0.02s

=== episode:67 Env-steps-taken:67200
action_counts: {0: 8052, 1: 6336, 2: 5665, 3: 5137, 4: 6424, 5: 5153, 6: 6798, 7: 5951, 8: 5577}
picked:  63
episode: 67/2000 -> reward: 100.67187500000007, steps:55093, time-taken: 1.71min, time-elasped: 98.06min
-> berries picked: 63 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2506 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [112, 323, 352, 291, 429, 260, 329, 225, 185]
	| approx positives in sample 512: 139
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [17, 23, 12, 25, 19, 2, 17, 11, 13]
	Time taken saving stuff: 0.10s

=== episode:68 Env-steps-taken:70176
action_counts: {0: 6644, 1: 6704, 2: 6116, 3: 5412, 4: 6215, 5: 7260, 6: 7249, 7: 5489, 8: 3531}
picked:  79
episode: 68/2000 -> reward: 116.08854166666679, steps:54620, time-taken: 1.73min, time-elasped: 99.80min
-> berries picked: 79 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2567 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [114, 331, 353, 299, 437, 270, 344, 229, 190]
	| approx positives in sample 512: 131
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 14, 19, 10, 28, 11, 13, 10, 15]
	Time taken saving stuff: 0.10s

=== episode:69 Env-steps-taken:77280
action_counts: {0: 7667, 1: 7997, 2: 8074, 3: 7590, 4: 6030, 5: 6820, 6: 7656, 7: 5005, 8: 4488}
picked:  94
episode: 69/2000 -> reward: 154.01041666666663, steps:61327, time-taken: 1.94min, time-elasped: 101.74min
-> berries picked: 94 of 800 | patches-visited: [0, 2, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2633 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [117, 341, 367, 308, 439, 280, 354, 236, 191]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 20, 19, 13, 26, 4, 6, 10, 12]
	Time taken saving stuff: 0.01s

=== episode:70 Env-steps-taken:74304
action_counts: {0: 8173, 1: 7634, 2: 5907, 3: 5258, 4: 7051, 5: 8547, 6: 8393, 7: 6457, 8: 6341}
picked:  89
episode: 70/2000 -> reward: 137.53645833333343, steps:63761, time-taken: 1.98min, time-elasped: 103.72min
-> berries picked: 89 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2693 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [120, 345, 376, 310, 449, 283, 370, 247, 193]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 13, 11, 18, 19, 11, 14, 17, 7]
	Time taken saving stuff: 0.05s

=== episode:7 Env-steps-taken:65568
action_counts: {0: 2640, 1: 2123, 2: 3696, 3: 902, 4: 2035, 5: 2860, 6: 2981, 7: 2112, 8: 5006}
picked:  66

==================================================
eval-episode: 70 -> reward: 92.15625000000009, steps: 24355.0, wall-time: 26.15s
-> berries picked: 66 of 800 | patches-visited: [1, 3] | juice left:-0.00
==================================================


=== episode:71 Env-steps-taken:55584
action_counts: {0: 2068, 1: 1375, 2: 2464, 3: 1661, 4: 2046, 5: 4301, 6: 2783, 7: 2211, 8: 2787}
picked:  30
episode: 71/2000 -> reward: 39.34374999999999, steps:21696, time-taken: 0.84min, time-elasped: 105.00min
-> berries picked: 30 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2719 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [123, 348, 376, 314, 451, 283, 378, 252, 194]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 14, 14, 10, 20, 10, 14, 13, 13]
	Time taken saving stuff: 0.04s

=== episode:72 Env-steps-taken:50784
action_counts: {0: 429, 1: 462, 2: 1045, 3: 605, 4: 737, 5: 539, 6: 495, 7: 913, 8: 792}
picked:  8
episode: 72/2000 -> reward: 14.458333333333332, steps:6017, time-taken: 0.45min, time-elasped: 105.45min
-> berries picked: 8 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2726 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [123, 348, 378, 315, 451, 283, 379, 255, 194]
	| approx positives in sample 512: 136
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 23, 18, 12, 24, 8, 12, 21, 13]
	Time taken saving stuff: 0.02s

=== episode:73 Env-steps-taken:71136
action_counts: {0: 5159, 1: 4565, 2: 6413, 3: 4367, 4: 5291, 5: 8822, 6: 7415, 7: 7205, 8: 3938}
picked:  83
episode: 73/2000 -> reward: 122.06770833333347, steps:53175, time-taken: 1.73min, time-elasped: 107.18min
-> berries picked: 83 of 800 | patches-visited: [0, 3, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2791 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [125, 353, 379, 325, 460, 290, 393, 266, 200]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 20, 13, 14, 23, 6, 14, 12, 11]
	Time taken saving stuff: 0.12s

=== episode:74 Env-steps-taken:69504
action_counts: {0: 8481, 1: 6908, 2: 6479, 3: 4961, 4: 6358, 5: 8217, 6: 8833, 7: 6941, 8: 6292}
picked:  79
episode: 74/2000 -> reward: 112.58854166666677, steps:63470, time-taken: 1.93min, time-elasped: 109.12min
-> berries picked: 79 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2843 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [128, 363, 383, 330, 464, 289, 416, 269, 201]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 15, 12, 16, 7, 11, 14, 10]
	Time taken saving stuff: 0.02s

=== episode:75 Env-steps-taken:57120
action_counts: {0: 10274, 1: 4631, 2: 4917, 3: 5357, 4: 6204, 5: 5786, 6: 6820, 7: 7609, 8: 5522}
picked:  34
episode: 75/2000 -> reward: 47.32291666666665, steps:57120, time-taken: 1.74min, time-elasped: 110.86min
-> berries picked: 34 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2860 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [129, 367, 386, 331, 464, 293, 419, 270, 201]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 14, 9, 13, 22, 10, 12, 17, 8]
	Time taken saving stuff: 0.11s

=== episode:76 Env-steps-taken:68544
action_counts: {0: 7557, 1: 5016, 2: 7909, 3: 4653, 4: 5324, 5: 5544, 6: 4609, 7: 4532, 8: 4984}
picked:  80
episode: 76/2000 -> reward: 107.58333333333344, steps:50128, time-taken: 1.58min, time-elasped: 112.44min
-> berries picked: 80 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2905 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [132, 375, 393, 335, 464, 300, 429, 274, 203]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 12, 10, 11, 17, 6, 12, 6, 14]
	Time taken saving stuff: 0.10s

=== episode:77 Env-steps-taken:69120
action_counts: {0: 6897, 1: 5335, 2: 6578, 3: 4785, 4: 5731, 5: 5709, 6: 3938, 7: 5907, 8: 5490}
picked:  82
episode: 77/2000 -> reward: 110.57291666666683, steps:50370, time-taken: 1.58min, time-elasped: 114.02min
-> berries picked: 82 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2956 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [136, 376, 405, 338, 474, 306, 434, 276, 211]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 12, 13, 12, 31, 13, 12, 10, 8]
	Time taken saving stuff: 0.11s

=== episode:78 Env-steps-taken:68736
action_counts: {0: 10296, 1: 8327, 2: 8624, 3: 4059, 4: 6468, 5: 7755, 6: 6765, 7: 5247, 8: 5909}
picked:  82
episode: 78/2000 -> reward: 108.57291666666684, steps:63450, time-taken: 1.97min, time-elasped: 115.99min
-> berries picked: 82 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3001 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [140, 383, 409, 344, 474, 313, 444, 279, 215]
	| approx positives in sample 512: 127
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 9, 13, 15, 23, 8, 20, 13, 17]
	Time taken saving stuff: 0.13s

=== episode:79 Env-steps-taken:62688
action_counts: {0: 4158, 1: 4070, 2: 4389, 3: 2728, 4: 3949, 5: 4994, 6: 4177, 7: 2717, 8: 2376}
picked:  52
episode: 79/2000 -> reward: 77.22916666666667, steps:33558, time-taken: 1.22min, time-elasped: 117.21min
-> berries picked: 52 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3037 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [144, 391, 416, 344, 478, 318, 448, 279, 219]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 14, 14, 14, 14, 2, 22, 14, 8]
	Time taken saving stuff: 0.11s

=== episode:80 Env-steps-taken:65088
action_counts: {0: 10296, 1: 5973, 2: 5511, 3: 4862, 4: 5467, 5: 7282, 6: 6831, 7: 6523, 8: 6073}
picked:  69
episode: 80/2000 -> reward: 88.64062500000006, steps:58818, time-taken: 1.70min, time-elasped: 118.92min
-> berries picked: 69 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3071 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [147, 396, 417, 348, 484, 319, 455, 286, 219]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 12, 9, 11, 14, 16, 15, 20, 8]
	Time taken saving stuff: 0.14s

=== episode:8 Env-steps-taken:60864
action_counts: {0: 605, 1: 3586, 2: 3619, 3: 4059, 4: 1100, 5: 792, 6: 1056, 7: 36710, 8: 264}
picked:  45

==================================================
eval-episode: 80 -> reward: 67.76562499999999, steps: 51791.0, wall-time: 24.28s
-> berries picked: 45 of 800 | patches-visited: [1, 7] | juice left:-0.00
==================================================


=== episode:81 Env-steps-taken:61920
action_counts: {0: 2893, 1: 3619, 2: 3179, 3: 3003, 4: 3003, 5: 2354, 6: 3190, 7: 3916, 8: 1915}
picked:  46
episode: 81/2000 -> reward: 73.26041666666666, steps:27072, time-taken: 1.03min, time-elasped: 120.35min
-> berries picked: 46 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3091 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [149, 403, 421, 345, 481, 317, 463, 291, 221]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 17, 10, 15, 7, 13, 22, 13]
	Time taken saving stuff: 0.00s

=== episode:82 Env-steps-taken:66816
action_counts: {0: 7579, 1: 4840, 2: 6292, 3: 3333, 4: 4664, 5: 4455, 6: 4928, 7: 5082, 8: 4522}
picked:  67
episode: 82/2000 -> reward: 98.65104166666671, steps:45695, time-taken: 1.45min, time-elasped: 121.80min
-> berries picked: 67 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3118 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [159, 407, 420, 346, 487, 317, 462, 296, 224]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 18, 8, 14, 10, 9, 11, 15, 14]
	Time taken saving stuff: 0.11s

=== episode:83 Env-steps-taken:59520
action_counts: {0: 2024, 1: 2750, 2: 4202, 3: 2636, 4: 2849, 5: 3751, 6: 3311, 7: 2684, 8: 2684}
picked:  42
episode: 83/2000 -> reward: 61.78124999999996, steps:26891, time-taken: 1.01min, time-elasped: 122.81min
-> berries picked: 42 of 800 | patches-visited: [0, 1, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3142 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [159, 407, 424, 347, 487, 322, 466, 303, 227]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 23, 10, 10, 15, 14, 13, 11, 15]
	Time taken saving stuff: 0.00s

=== episode:84 Env-steps-taken:73824
action_counts: {0: 9757, 1: 6954, 2: 8778, 3: 6710, 4: 7073, 5: 7799, 6: 7216, 7: 7447, 8: 6050}
picked:  103
episode: 84/2000 -> reward: 134.9635416666668, steps:67784, time-taken: 1.97min, time-elasped: 124.79min
-> berries picked: 103 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3179 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [170, 411, 427, 354, 488, 325, 472, 306, 226]
	| approx positives in sample 512: 123
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 10, 18, 9, 20, 9, 13, 17, 14]
	Time taken saving stuff: 0.01s

=== episode:85 Env-steps-taken:71712
action_counts: {0: 7447, 1: 5731, 2: 7359, 3: 5368, 4: 5665, 5: 4917, 6: 5918, 7: 5236, 8: 3861}
picked:  83
episode: 85/2000 -> reward: 125.0677083333335, steps:51502, time-taken: 1.68min, time-elasped: 126.47min
-> berries picked: 83 of 800 | patches-visited: [0, 1, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3214 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [175, 414, 432, 357, 488, 333, 472, 316, 227]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 18, 9, 20, 13, 8, 13, 19, 13]
	Time taken saving stuff: 0.00s

=== episode:86 Env-steps-taken:58080
action_counts: {0: 4158, 1: 2838, 2: 3707, 3: 2145, 4: 4785, 5: 4279, 6: 2970, 7: 2325, 8: 4499}
picked:  36
episode: 86/2000 -> reward: 53.31249999999997, steps:31706, time-taken: 1.10min, time-elasped: 127.57min
-> berries picked: 36 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3215 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [180, 409, 428, 355, 495, 335, 472, 315, 226]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 13, 15, 9, 16, 7, 8, 13, 19]
	Time taken saving stuff: 0.10s

=== episode:87 Env-steps-taken:68256
action_counts: {0: 8206, 1: 7590, 2: 8367, 3: 6677, 4: 6171, 5: 5511, 6: 7821, 7: 6556, 8: 5841}
picked:  86
episode: 87/2000 -> reward: 106.05208333333346, steps:62740, time-taken: 1.92min, time-elasped: 129.49min
-> berries picked: 86 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3233 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [189, 416, 433, 357, 500, 328, 468, 315, 227]
	| approx positives in sample 512: 131
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 25, 19, 6, 19, 11, 12, 14, 15]
	Time taken saving stuff: 0.00s

=== episode:88 Env-steps-taken:67200
action_counts: {0: 5995, 1: 3542, 2: 5166, 3: 3498, 4: 3949, 5: 3245, 6: 3828, 7: 4059, 8: 3322}
picked:  66
episode: 88/2000 -> reward: 100.65625000000009, steps:36604, time-taken: 1.28min, time-elasped: 130.78min
-> berries picked: 66 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3263 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [195, 421, 438, 362, 502, 327, 477, 313, 228]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 21, 21, 10, 18, 5, 13, 17, 11]
	Time taken saving stuff: 0.11s

=== episode:89 Env-steps-taken:58848
action_counts: {0: 3729, 1: 4092, 2: 4246, 3: 2299, 4: 3454, 5: 3322, 6: 5126, 7: 2709, 8: 3993}
picked:  42
episode: 89/2000 -> reward: 58.281249999999964, steps:32970, time-taken: 1.11min, time-elasped: 131.90min
-> berries picked: 42 of 800 | patches-visited: [0, 5, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3277 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [205, 422, 437, 360, 502, 327, 477, 317, 230]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 18, 10, 9, 22, 4, 9, 7, 13]
	Time taken saving stuff: 0.00s

=== episode:90 Env-steps-taken:59904
action_counts: {0: 4059, 1: 2299, 2: 3212, 3: 2310, 4: 2090, 5: 2032, 6: 3300, 7: 2849, 8: 1903}
picked:  45
episode: 90/2000 -> reward: 63.76562499999996, steps:24054, time-taken: 0.93min, time-elasped: 132.82min
-> berries picked: 45 of 800 | patches-visited: [0, 1, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3304 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [210, 425, 444, 362, 503, 325, 477, 325, 233]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 16, 11, 10, 25, 5, 13, 10, 12]
	Time taken saving stuff: 0.16s

=== episode:9 Env-steps-taken:62304
action_counts: {0: 33320, 1: 1683, 2: 3663, 3: 242, 4: 946, 5: 968, 6: 1122, 7: 3069, 8: 5071}
picked:  50

==================================================
eval-episode: 90 -> reward: 75.23958333333334, steps: 50084.0, wall-time: 29.70s
-> berries picked: 50 of 800 | patches-visited: [1, 7] | juice left:-0.00
==================================================


=== episode:91 Env-steps-taken:60000
action_counts: {0: 1782, 1: 2519, 2: 3465, 3: 3036, 4: 2354, 5: 2211, 6: 3817, 7: 2684, 8: 3037}
picked:  40
episode: 91/2000 -> reward: 63.291666666666636, steps:24905, time-taken: 0.95min, time-elasped: 134.28min
-> berries picked: 40 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3317 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [210, 427, 444, 366, 505, 320, 482, 329, 234]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 24, 17, 12, 12, 5, 7, 14, 8]
	Time taken saving stuff: 0.12s

=== episode:92 Env-steps-taken:67488
action_counts: {0: 5148, 1: 5588, 2: 5863, 3: 5071, 4: 3960, 5: 4279, 6: 5995, 7: 4587, 8: 5600}
picked:  69
episode: 92/2000 -> reward: 103.1406250000001, steps:46091, time-taken: 1.52min, time-elasped: 135.80min
-> berries picked: 69 of 800 | patches-visited: [0, 5, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3343 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [213, 437, 452, 366, 506, 324, 481, 326, 238]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 18, 9, 17, 16, 8, 12, 11, 6]
	Time taken saving stuff: 0.01s

=== episode:93 Env-steps-taken:64320
action_counts: {0: 5577, 1: 4708, 2: 5643, 3: 4169, 4: 5126, 5: 5918, 6: 10054, 7: 5885, 8: 7503}
picked:  64
episode: 93/2000 -> reward: 85.66666666666669, steps:54583, time-taken: 1.66min, time-elasped: 137.46min
-> berries picked: 64 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3342 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [213, 437, 450, 365, 503, 323, 482, 331, 238]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 13, 23, 14, 14, 8, 16, 7, 13]
	Time taken saving stuff: 0.09s

=== episode:94 Env-steps-taken:63840
action_counts: {0: 2398, 1: 4092, 2: 4169, 3: 4037, 4: 4675, 5: 3443, 6: 4741, 7: 3465, 8: 3013}
picked:  54
episode: 94/2000 -> reward: 83.21875000000003, steps:34033, time-taken: 1.20min, time-elasped: 138.67min
-> berries picked: 54 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3367 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [214, 435, 451, 368, 504, 333, 488, 335, 239]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 19, 17, 16, 9, 3, 7, 7, 14]
	Time taken saving stuff: 0.10s

=== episode:95 Env-steps-taken:57600
action_counts: {0: 1892, 1: 2684, 2: 3333, 3: 2145, 4: 2189, 5: 1661, 6: 1903, 7: 2356, 8: 2090}
picked:  34
episode: 95/2000 -> reward: 51.82291666666664, steps:20253, time-taken: 0.80min, time-elasped: 139.47min
-> berries picked: 34 of 800 | patches-visited: [0, 1, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3381 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [219, 442, 454, 368, 501, 332, 485, 338, 242]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 15, 20, 13, 12, 5, 15, 12, 17]
	Time taken saving stuff: 0.12s

=== episode:96 Env-steps-taken:62880
action_counts: {0: 7172, 1: 4895, 2: 5544, 3: 4554, 4: 4719, 5: 6325, 6: 11154, 7: 8932, 8: 8979}
picked:  58
episode: 96/2000 -> reward: 77.19791666666667, steps:62274, time-taken: 1.85min, time-elasped: 141.32min
-> berries picked: 58 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3361 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [222, 438, 454, 360, 485, 326, 495, 338, 243]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [19, 17, 6, 18, 5, 11, 12, 12, 14]
	Time taken saving stuff: 0.10s

=== episode:97 Env-steps-taken:57792
action_counts: {0: 3850, 1: 3190, 2: 3802, 3: 3608, 4: 2761, 5: 3861, 6: 5115, 7: 9240, 8: 5610}
picked:  37
episode: 97/2000 -> reward: 51.80729166666665, steps:41037, time-taken: 1.33min, time-elasped: 142.66min
-> berries picked: 37 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3361 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [226, 435, 450, 363, 480, 321, 497, 345, 244]
	| approx positives in sample 512: 127
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 16, 16, 8, 15, 9, 20, 15, 16]
	Time taken saving stuff: 0.09s

=== episode:98 Env-steps-taken:65280
action_counts: {0: 5159, 1: 4488, 2: 4807, 3: 4499, 4: 4444, 5: 4554, 6: 6831, 7: 4983, 8: 4247}
picked:  63
episode: 98/2000 -> reward: 90.67187500000006, steps:44012, time-taken: 1.49min, time-elasped: 144.15min
-> berries picked: 63 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3376 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [232, 436, 450, 361, 478, 327, 501, 346, 245]
	| approx positives in sample 512: 124
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 19, 17, 14, 11, 11, 10, 14, 15]
	Time taken saving stuff: 0.02s

=== episode:99 Env-steps-taken:72960
action_counts: {0: 8470, 1: 5758, 2: 6897, 3: 5566, 4: 5566, 5: 6556, 6: 9328, 7: 7359, 8: 6050}
picked:  87
episode: 99/2000 -> reward: 131.54687500000014, steps:61550, time-taken: 1.90min, time-elasped: 146.05min
-> berries picked: 87 of 800 | patches-visited: [0, 2, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3385 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [236, 443, 452, 357, 466, 331, 498, 351, 251]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 12, 19, 12, 17, 8, 10, 12, 9]
	Time taken saving stuff: 0.12s

=== episode:100 Env-steps-taken:65664
action_counts: {0: 6930, 1: 7271, 2: 6077, 3: 4323, 4: 4389, 5: 5984, 6: 8767, 7: 7612, 8: 6193}
picked:  70
episode: 100/2000 -> reward: 92.63541666666673, steps:57546, time-taken: 1.82min, time-elasped: 147.88min
-> berries picked: 70 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3396 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [239, 442, 460, 351, 459, 334, 504, 351, 256]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 13, 13, 8, 15, 9, 12, 14, 14]
	Time taken saving stuff: 0.05s

=== episode:10 Env-steps-taken:82272
action_counts: {0: 8037, 1: 7755, 2: 4037, 3: 5247, 4: 5401, 5: 3465, 6: 11044, 7: 2618, 8: 12265}
picked:  128

==================================================
eval-episode: 100 -> reward: 180.83333333333314, steps: 59869.0, wall-time: 40.75s
-> berries picked: 128 of 800 | patches-visited: [0, 1, 2, 3] | juice left:-0.00
==================================================


=== episode:101 Env-steps-taken:63360
action_counts: {0: 2970, 1: 4114, 2: 3685, 3: 2992, 4: 2959, 5: 2728, 6: 4774, 7: 3817, 8: 3026}
picked:  55
episode: 101/2000 -> reward: 81.71354166666669, steps:31065, time-taken: 1.12min, time-elasped: 149.68min
-> berries picked: 55 of 800 | patches-visited: [0, 2, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3417 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [241, 449, 463, 352, 452, 336, 510, 356, 258]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 14, 22, 9, 8, 14, 13, 12, 16]
	Time taken saving stuff: 0.01s

=== episode:102 Env-steps-taken:63840
action_counts: {0: 6050, 1: 3938, 2: 5096, 3: 4378, 4: 5093, 5: 5269, 6: 7062, 7: 4488, 8: 5225}
picked:  63
episode: 102/2000 -> reward: 84.17187500000001, steps:46599, time-taken: 1.50min, time-elasped: 151.19min
-> berries picked: 63 of 800 | patches-visited: [0, 2, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3424 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [243, 443, 461, 355, 449, 341, 512, 358, 262]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 11, 22, 9, 9, 7, 13, 11, 14]
	Time taken saving stuff: 0.00s

=== episode:103 Env-steps-taken:61056
action_counts: {0: 2794, 1: 1793, 2: 2695, 3: 2959, 4: 2728, 5: 3938, 6: 4565, 7: 3465, 8: 2314}
picked:  48
episode: 103/2000 -> reward: 68.74999999999997, steps:27251, time-taken: 1.00min, time-elasped: 152.19min
-> berries picked: 48 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3446 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [247, 444, 465, 353, 457, 345, 513, 357, 265]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 22, 14, 13, 12, 7, 16, 6, 16]
	Time taken saving stuff: 0.01s

=== episode:104 Env-steps-taken:59328
action_counts: {0: 3498, 1: 4422, 2: 3091, 3: 2721, 4: 2431, 5: 2585, 6: 5357, 7: 3630, 8: 3344}
picked:  38
episode: 104/2000 -> reward: 60.8020833333333, steps:31079, time-taken: 1.08min, time-elasped: 153.28min
-> berries picked: 38 of 800 | patches-visited: [0, 4, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3445 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [250, 451, 462, 354, 452, 336, 516, 357, 267]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 17, 14, 11, 7, 14, 13, 7, 21]
	Time taken saving stuff: 0.02s

=== episode:105 Env-steps-taken:67872
action_counts: {0: 4928, 1: 5148, 2: 5302, 3: 3564, 4: 4323, 5: 4345, 6: 5753, 7: 4631, 8: 5476}
picked:  71
episode: 105/2000 -> reward: 105.13020833333341, steps:43470, time-taken: 1.40min, time-elasped: 154.67min
-> berries picked: 71 of 800 | patches-visited: [0, 5, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3469 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [252, 451, 470, 352, 456, 338, 519, 363, 268]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 16, 22, 7, 9, 12, 12, 13, 17]
	Time taken saving stuff: 0.01s

=== episode:106 Env-steps-taken:59616
action_counts: {0: 3564, 1: 2585, 2: 4312, 3: 2541, 4: 3454, 5: 2768, 6: 5489, 7: 5258, 8: 4928}
picked:  47
episode: 106/2000 -> reward: 62.25520833333328, steps:34899, time-taken: 1.16min, time-elasped: 155.84min
-> berries picked: 47 of 800 | patches-visited: [0, 1, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3472 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [256, 453, 465, 355, 452, 331, 522, 366, 272]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [17, 15, 10, 18, 7, 8, 14, 12, 14]
	Time taken saving stuff: 0.01s

=== episode:107 Env-steps-taken:66432
action_counts: {0: 4609, 1: 4477, 2: 5027, 3: 3707, 4: 4577, 5: 5148, 6: 8888, 7: 5808, 8: 5291}
picked:  67
episode: 107/2000 -> reward: 97.65104166666674, steps:47532, time-taken: 1.56min, time-elasped: 157.40min
-> berries picked: 67 of 800 | patches-visited: [0, 3, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3478 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [259, 447, 464, 352, 442, 338, 534, 369, 273]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 17, 17, 7, 10, 6, 16, 8, 18]
	Time taken saving stuff: 0.10s

=== episode:108 Env-steps-taken:68256
action_counts: {0: 5731, 1: 5038, 2: 4939, 3: 4257, 4: 4774, 5: 4917, 6: 7381, 7: 6864, 8: 4951}
picked:  72
episode: 108/2000 -> reward: 108.1250000000001, steps:48852, time-taken: 1.62min, time-elasped: 159.03min
-> berries picked: 72 of 800 | patches-visited: [0, 1, 2, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3498 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [268, 448, 470, 350, 436, 338, 535, 375, 278]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 18, 11, 9, 9, 7, 17, 9, 21]
	Time taken saving stuff: 0.02s

=== episode:109 Env-steps-taken:57120
action_counts: {0: 2156, 1: 1144, 2: 792, 3: 1430, 4: 1375, 5: 2266, 6: 3487, 7: 1683, 8: 1060}
picked:  31
episode: 109/2000 -> reward: 48.33854166666665, steps:15393, time-taken: 0.72min, time-elasped: 159.75min
-> berries picked: 31 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3512 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [273, 448, 466, 349, 437, 338, 541, 381, 279]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 16, 10, 8, 16, 4, 14, 19, 13]
	Time taken saving stuff: 0.10s

=== episode:110 Env-steps-taken:69504
action_counts: {0: 5027, 1: 5665, 2: 5170, 3: 4477, 4: 5148, 5: 5559, 6: 8591, 7: 8470, 8: 6523}
picked:  77
episode: 110/2000 -> reward: 112.59895833333344, steps:54630, time-taken: 1.73min, time-elasped: 161.48min
-> berries picked: 77 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3532 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [277, 446, 458, 350, 436, 338, 557, 390, 280]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 23, 16, 11, 16, 8, 13, 9, 13]
	Time taken saving stuff: 0.09s

=== episode:11 Env-steps-taken:63936
action_counts: {0: 825, 1: 5599, 2: 704, 3: 2486, 4: 1111, 5: 682, 6: 6171, 7: 7370, 8: 1475}
picked:  62

==================================================
eval-episode: 110 -> reward: 83.67708333333334, steps: 26423.0, wall-time: 25.47s
-> berries picked: 62 of 800 | patches-visited: [1, 4] | juice left:-0.00
==================================================


=== episode:111 Env-steps-taken:59040
action_counts: {0: 3047, 1: 2783, 2: 3443, 3: 3058, 4: 3674, 5: 3729, 6: 6457, 7: 7436, 8: 5655}
picked:  43
episode: 111/2000 -> reward: 58.27604166666663, steps:39282, time-taken: 1.33min, time-elasped: 163.24min
-> berries picked: 43 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3539 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [279, 443, 456, 351, 433, 338, 569, 389, 281]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 16, 12, 6, 15, 10, 10, 9, 17]
	Time taken saving stuff: 0.01s

=== episode:112 Env-steps-taken:60960
action_counts: {0: 4158, 1: 3828, 2: 3828, 3: 2838, 4: 4191, 5: 3344, 6: 5390, 7: 4894, 8: 4477}
picked:  49
episode: 112/2000 -> reward: 68.24479166666664, steps:36948, time-taken: 1.24min, time-elasped: 164.49min
-> berries picked: 49 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3561 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [279, 447, 453, 354, 438, 344, 574, 391, 281]
	| approx positives in sample 512: 130
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 20, 12, 10, 16, 11, 16, 10, 23]
	Time taken saving stuff: 0.00s

=== episode:113 Env-steps-taken:68256
action_counts: {0: 8690, 1: 6974, 2: 6732, 3: 5060, 4: 5016, 5: 7084, 6: 9966, 7: 8019, 8: 8163}
picked:  73
episode: 113/2000 -> reward: 105.11979166666679, steps:65704, time-taken: 2.01min, time-elasped: 166.50min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3571 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [285, 446, 449, 350, 439, 345, 574, 402, 281]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 16, 12, 7, 8, 10, 13, 15, 19]
	Time taken saving stuff: 0.10s

=== episode:114 Env-steps-taken:61344
action_counts: {0: 4246, 1: 4136, 2: 5082, 3: 3861, 4: 4070, 5: 4708, 6: 7139, 7: 5060, 8: 5149}
picked:  47
episode: 114/2000 -> reward: 70.25520833333331, steps:43451, time-taken: 1.38min, time-elasped: 167.88min
-> berries picked: 47 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3580 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [287, 447, 444, 349, 438, 349, 581, 400, 285]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 21, 13, 10, 16, 5, 14, 10, 13]
	Time taken saving stuff: 0.00s

=== episode:115 Env-steps-taken:60000
action_counts: {0: 2673, 1: 3212, 2: 2233, 3: 2299, 4: 1826, 5: 2618, 6: 3351, 7: 4114, 8: 1551}
picked:  40
episode: 115/2000 -> reward: 64.29166666666663, steps:23877, time-taken: 0.91min, time-elasped: 168.79min
-> berries picked: 40 of 800 | patches-visited: [0, 3, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3602 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [289, 452, 448, 352, 438, 351, 581, 405, 286]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 15, 15, 11, 14, 1, 10, 11, 13]
	Time taken saving stuff: 0.10s

=== episode:116 Env-steps-taken:59904
action_counts: {0: 2266, 1: 1694, 2: 2145, 3: 1881, 4: 3718, 5: 3267, 6: 4631, 7: 2860, 8: 2463}
picked:  40
episode: 116/2000 -> reward: 62.79166666666663, steps:24925, time-taken: 0.91min, time-elasped: 169.71min
-> berries picked: 40 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3607 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [290, 448, 443, 352, 441, 356, 581, 408, 288]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 11, 10, 11, 10, 7, 14, 16, 17]
	Time taken saving stuff: 0.00s

=== episode:117 Env-steps-taken:62592
action_counts: {0: 5423, 1: 5192, 2: 4246, 3: 3630, 4: 5203, 5: 5995, 6: 5863, 7: 10219, 8: 7788}
picked:  55
episode: 117/2000 -> reward: 76.71354166666667, steps:53559, time-taken: 1.65min, time-elasped: 171.36min
-> berries picked: 55 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3583 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [297, 439, 426, 351, 437, 359, 582, 404, 288]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 10, 14, 2, 9, 8, 17, 11, 18]
	Time taken saving stuff: 0.02s

=== episode:118 Env-steps-taken:61536
action_counts: {0: 5896, 1: 5554, 2: 5643, 3: 3861, 4: 4708, 5: 5445, 6: 8206, 7: 8888, 8: 9295}
picked:  56
episode: 118/2000 -> reward: 71.2083333333333, steps:57496, time-taken: 1.87min, time-elasped: 173.23min
-> berries picked: 56 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3568 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [305, 436, 420, 350, 432, 360, 575, 401, 289]
	| approx positives in sample 512: 87
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 10, 12, 7, 9, 9, 8, 11, 13]
	Time taken saving stuff: 0.00s

=== episode:119 Env-steps-taken:59808
action_counts: {0: 5555, 1: 8272, 2: 4521, 3: 4400, 4: 5137, 5: 4730, 6: 6457, 7: 7062, 8: 7364}
picked:  50
episode: 119/2000 -> reward: 61.239583333333286, steps:53498, time-taken: 1.65min, time-elasped: 174.89min
-> berries picked: 50 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3540 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [305, 429, 418, 346, 434, 355, 561, 405, 287]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 14, 11, 12, 11, 9, 20, 12, 12]
	Time taken saving stuff: 0.07s

=== episode:120 Env-steps-taken:57792
action_counts: {0: 3234, 1: 2310, 2: 2024, 3: 2156, 4: 2497, 5: 2959, 6: 4642, 7: 2894, 8: 2255}
picked:  34
episode: 120/2000 -> reward: 52.822916666666636, steps:24971, time-taken: 0.92min, time-elasped: 175.81min
-> berries picked: 34 of 800 | patches-visited: [0, 7, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3547 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [310, 429, 414, 349, 436, 354, 560, 406, 289]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 22, 14, 9, 11, 8, 16, 12, 13]
	Time taken saving stuff: 0.26s

=== episode:12 Env-steps-taken:66528
action_counts: {0: 2018, 1: 2398, 2: 7799, 3: 2024, 4: 1485, 5: 682, 6: 8338, 7: 5995, 8: 2607}
picked:  68

==================================================
eval-episode: 120 -> reward: 97.1458333333334, steps: 33346.0, wall-time: 30.80s
-> berries picked: 68 of 800 | patches-visited: [1, 3] | juice left:-0.00
==================================================


=== episode:121 Env-steps-taken:66144
action_counts: {0: 6413, 1: 5896, 2: 4268, 3: 4774, 4: 3443, 5: 4345, 6: 7821, 7: 6292, 8: 7051}
picked:  66
episode: 121/2000 -> reward: 94.15625000000006, steps:50303, time-taken: 1.57min, time-elasped: 177.91min
-> berries picked: 66 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3547 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [318, 429, 404, 345, 433, 356, 562, 412, 288]
	| approx positives in sample 512: 97
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 8, 9, 14, 7, 1, 16, 18, 14]
	Time taken saving stuff: 0.00s

=== episode:122 Env-steps-taken:59808
action_counts: {0: 2750, 1: 2827, 2: 2299, 3: 2453, 4: 1650, 5: 2244, 6: 4994, 7: 3806, 8: 2556}
picked:  40
episode: 122/2000 -> reward: 63.291666666666636, steps:25579, time-taken: 0.97min, time-elasped: 178.88min
-> berries picked: 40 of 800 | patches-visited: [0, 6, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3550 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [321, 432, 395, 344, 430, 354, 571, 412, 291]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 13, 20, 8, 10, 7, 13, 10, 12]
	Time taken saving stuff: 0.12s

=== episode:123 Env-steps-taken:58272
action_counts: {0: 4092, 1: 3036, 2: 2299, 3: 2618, 4: 2629, 5: 2431, 6: 4323, 7: 3597, 8: 3644}
picked:  38
episode: 123/2000 -> reward: 54.3020833333333, steps:28669, time-taken: 1.04min, time-elasped: 179.92min
-> berries picked: 38 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3552 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [328, 429, 392, 344, 430, 359, 568, 410, 292]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 11, 17, 9, 7, 11, 16, 13, 21]
	Time taken saving stuff: 0.02s

=== episode:124 Env-steps-taken:63360
action_counts: {0: 3696, 1: 3696, 2: 3273, 3: 2442, 4: 2376, 5: 3641, 6: 5907, 7: 5027, 8: 3245}
picked:  55
episode: 124/2000 -> reward: 81.71354166666669, steps:33303, time-taken: 1.22min, time-elasped: 181.14min
-> berries picked: 55 of 800 | patches-visited: [0, 2, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3569 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [330, 430, 392, 342, 430, 362, 573, 412, 298]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 18, 14, 8, 17, 18, 14, 17, 10]
	Time taken saving stuff: 0.00s

=== episode:125 Env-steps-taken:60288
action_counts: {0: 3718, 1: 4532, 2: 3102, 3: 2871, 4: 2288, 5: 2838, 6: 4279, 7: 5479, 8: 4532}
picked:  43
episode: 125/2000 -> reward: 64.77604166666663, steps:33639, time-taken: 1.13min, time-elasped: 182.27min
-> berries picked: 43 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3570 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [328, 430, 391, 339, 430, 366, 574, 410, 302]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 16, 10, 10, 16, 5, 12, 11, 13]
	Time taken saving stuff: 0.10s

=== episode:126 Env-steps-taken:54816
action_counts: {0: 1221, 1: 1419, 2: 2277, 3: 1309, 4: 1793, 5: 2607, 6: 2497, 7: 1683, 8: 1695}
picked:  26
episode: 126/2000 -> reward: 37.36458333333333, steps:16501, time-taken: 0.75min, time-elasped: 183.02min
-> berries picked: 26 of 800 | patches-visited: [0, 1, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3583 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [328, 426, 391, 341, 433, 371, 577, 412, 304]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 12, 11, 14, 6, 4, 10, 11, 23]
	Time taken saving stuff: 0.01s

=== episode:127 Env-steps-taken:57504
action_counts: {0: 1969, 1: 3091, 2: 1749, 3: 1551, 4: 1782, 5: 3410, 6: 2805, 7: 2893, 8: 2153}
picked:  36
episode: 127/2000 -> reward: 50.31249999999998, steps:21403, time-taken: 0.85min, time-elasped: 183.87min
-> berries picked: 36 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3585 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [329, 427, 391, 333, 437, 369, 580, 414, 305]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 13, 15, 7, 14, 5, 12, 12, 20]
	Time taken saving stuff: 0.02s

=== episode:128 Env-steps-taken:58848
action_counts: {0: 2959, 1: 2101, 2: 2310, 3: 1496, 4: 1749, 5: 2343, 6: 3047, 7: 2684, 8: 2641}
picked:  36
episode: 128/2000 -> reward: 58.312499999999964, steps:21330, time-taken: 0.87min, time-elasped: 184.74min
-> berries picked: 36 of 800 | patches-visited: [0, 5, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3591 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [330, 427, 387, 337, 437, 373, 581, 413, 306]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 9, 8, 8, 20, 12, 12, 20, 13]
	Time taken saving stuff: 0.10s

=== episode:129 Env-steps-taken:50496
action_counts: {0: 517, 1: 1100, 2: 836, 3: 550, 4: 319, 5: 473, 6: 825, 7: 385, 8: 519}
picked:  9
episode: 129/2000 -> reward: 12.953124999999996, steps:5524, time-taken: 0.45min, time-elasped: 185.20min
-> berries picked: 9 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3593 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [329, 426, 387, 335, 440, 373, 583, 414, 306]
	| approx positives in sample 512: 125
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 16, 11, 13, 13, 13, 14, 14, 18]
	Time taken saving stuff: 0.02s

=== episode:130 Env-steps-taken:59328
action_counts: {0: 4708, 1: 3300, 2: 3287, 3: 2244, 4: 2134, 5: 3410, 6: 4598, 7: 6028, 8: 4686}
picked:  38
episode: 130/2000 -> reward: 60.80208333333329, steps:34395, time-taken: 1.18min, time-elasped: 186.38min
-> berries picked: 38 of 800 | patches-visited: [0, 3, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3584 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [334, 423, 385, 325, 440, 372, 575, 419, 311]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 15, 8, 10, 9, 12, 15, 12, 28]
	Time taken saving stuff: 0.05s

=== episode:13 Env-steps-taken:56160
action_counts: {0: 330, 1: 2596, 2: 13156, 3: 154, 4: 484, 5: 11440, 6: 15466, 7: 1001, 8: 11533}
picked:  33

==================================================
eval-episode: 130 -> reward: 42.32812499999999, steps: 56160.0, wall-time: 31.67s
-> berries picked: 33 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:131 Env-steps-taken:62976
action_counts: {0: 3861, 1: 5808, 2: 5995, 3: 2618, 4: 2717, 5: 4565, 6: 7403, 7: 5765, 8: 5170}
picked:  56
episode: 131/2000 -> reward: 78.70833333333334, steps:43902, time-taken: 1.39min, time-elasped: 188.30min
-> berries picked: 56 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3584 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [343, 422, 379, 326, 435, 370, 575, 418, 316]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 12, 16, 9, 14, 5, 10, 19, 16]
	Time taken saving stuff: 0.10s

=== episode:132 Env-steps-taken:56448
action_counts: {0: 4609, 1: 4477, 2: 3828, 3: 3201, 4: 3905, 5: 4796, 6: 6589, 7: 7766, 8: 8623}
picked:  33
episode: 132/2000 -> reward: 43.828124999999986, steps:47794, time-taken: 1.47min, time-elasped: 189.78min
-> berries picked: 33 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3547 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [341, 414, 369, 315, 438, 370, 571, 413, 316]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 11, 8, 6, 13, 5, 10, 14, 13]
	Time taken saving stuff: 0.00s

=== episode:133 Env-steps-taken:58848
action_counts: {0: 5767, 1: 6589, 2: 4576, 3: 3421, 4: 4004, 5: 5511, 6: 9042, 7: 7359, 8: 7018}
picked:  41
episode: 133/2000 -> reward: 57.2864583333333, steps:53287, time-taken: 1.62min, time-elasped: 191.39min
-> berries picked: 41 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3520 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [341, 405, 359, 309, 437, 377, 565, 410, 317]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 10, 13, 9, 12, 11, 14, 9, 16]
	Time taken saving stuff: 0.01s

=== episode:134 Env-steps-taken:58848
action_counts: {0: 3212, 1: 3542, 2: 2447, 3: 2530, 4: 2717, 5: 3366, 6: 4785, 7: 3025, 8: 2332}
picked:  41
episode: 134/2000 -> reward: 57.28645833333329, steps:27956, time-taken: 0.96min, time-elasped: 192.36min
-> berries picked: 41 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3529 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [344, 403, 354, 307, 441, 380, 569, 413, 318]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 16, 17, 9, 13, 7, 16, 16, 13]
	Time taken saving stuff: 0.00s

=== episode:135 Env-steps-taken:62688
action_counts: {0: 3476, 1: 4400, 2: 3399, 3: 2134, 4: 2992, 5: 3322, 6: 4235, 7: 3531, 8: 4952}
picked:  52
episode: 135/2000 -> reward: 77.22916666666667, steps:32441, time-taken: 1.11min, time-elasped: 193.47min
-> berries picked: 52 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3534 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [346, 405, 350, 306, 443, 386, 563, 415, 320]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 10, 14, 10, 13, 4, 18, 12, 18]
	Time taken saving stuff: 0.07s

=== episode:136 Env-steps-taken:59424
action_counts: {0: 1595, 1: 3432, 2: 1980, 3: 1221, 4: 2090, 5: 3531, 6: 4268, 7: 2970, 8: 1915}
picked:  35
episode: 136/2000 -> reward: 61.3177083333333, steps:23002, time-taken: 0.90min, time-elasped: 194.38min
-> berries picked: 35 of 800 | patches-visited: [0, 1, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3545 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [347, 404, 350, 305, 446, 391, 562, 419, 321]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 13, 4, 14, 10, 13, 9, 15, 15]
	Time taken saving stuff: 0.00s

=== episode:137 Env-steps-taken:59808
action_counts: {0: 5269, 1: 5907, 2: 3872, 3: 3938, 4: 3278, 5: 3982, 6: 7007, 7: 4488, 8: 5666}
picked:  44
episode: 137/2000 -> reward: 62.27083333333329, steps:43407, time-taken: 1.38min, time-elasped: 195.76min
-> berries picked: 44 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3523 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [345, 399, 348, 301, 446, 382, 560, 420, 322]
	| approx positives in sample 512: 99
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 10, 7, 8, 9, 10, 11, 13, 24]
	Time taken saving stuff: 0.09s

=== episode:138 Env-steps-taken:61056
action_counts: {0: 7051, 1: 6864, 2: 6314, 3: 3894, 4: 4675, 5: 5603, 6: 7975, 7: 9515, 8: 9031}
picked:  50
episode: 138/2000 -> reward: 67.73958333333331, steps:60922, time-taken: 1.81min, time-elasped: 197.57min
-> berries picked: 50 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3499 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [339, 394, 344, 296, 444, 382, 559, 421, 320]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 17, 10, 8, 16, 8, 16, 10, 15]
	Time taken saving stuff: 0.00s

=== episode:139 Env-steps-taken:59328
action_counts: {0: 2948, 1: 3157, 2: 1980, 3: 2123, 4: 3432, 5: 3388, 6: 4664, 7: 3696, 8: 2790}
picked:  40
episode: 139/2000 -> reward: 58.79166666666663, steps:28178, time-taken: 0.99min, time-elasped: 198.56min
-> berries picked: 40 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3499 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [339, 390, 341, 296, 447, 384, 561, 422, 319]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 11, 13, 12, 7, 11, 15, 16, 19]
	Time taken saving stuff: 0.12s

=== episode:140 Env-steps-taken:54144
action_counts: {0: 1705, 1: 1837, 2: 2409, 3: 1804, 4: 1991, 5: 2387, 6: 1870, 7: 3256, 8: 4115}
picked:  20
episode: 140/2000 -> reward: 33.895833333333336, steps:21374, time-taken: 0.90min, time-elasped: 199.46min
-> berries picked: 20 of 800 | patches-visited: [0, 4, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3497 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [338, 393, 339, 294, 448, 386, 559, 420, 320]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 11, 14, 5, 10, 9, 11, 12, 20]
	Time taken saving stuff: 0.15s

=== episode:14 Env-steps-taken:66240
action_counts: {0: 3344, 1: 1650, 2: 4653, 3: 880, 4: 1705, 5: 5247, 6: 3575, 7: 4686, 8: 7174}
picked:  67

==================================================
eval-episode: 140 -> reward: 95.65104166666671, steps: 32914.0, wall-time: 27.78s
-> berries picked: 67 of 800 | patches-visited: [1, 7] | juice left:-0.00
==================================================


=== episode:141 Env-steps-taken:66432
action_counts: {0: 5126, 1: 4950, 2: 4182, 3: 4136, 4: 4048, 5: 4026, 6: 6556, 7: 6523, 8: 5247}
picked:  69
episode: 141/2000 -> reward: 97.64062500000007, steps:44794, time-taken: 1.55min, time-elasped: 201.48min
-> berries picked: 69 of 800 | patches-visited: [0, 1, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3504 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [340, 389, 336, 290, 453, 388, 558, 423, 327]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 10, 9, 12, 14, 4, 18, 16, 17]
	Time taken saving stuff: 0.10s

=== episode:142 Env-steps-taken:60288
action_counts: {0: 2552, 1: 2915, 2: 4224, 3: 2882, 4: 2431, 5: 2607, 6: 3993, 7: 3806, 8: 3070}
picked:  41
episode: 142/2000 -> reward: 64.78645833333329, steps:28480, time-taken: 1.05min, time-elasped: 202.54min
-> berries picked: 41 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3511 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [346, 390, 337, 290, 452, 387, 557, 422, 330]
	| approx positives in sample 512: 95
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 6, 6, 10, 7, 19, 12, 16]
	Time taken saving stuff: 0.10s

=== episode:143 Env-steps-taken:55872
action_counts: {0: 1595, 1: 1991, 2: 1694, 3: 1762, 4: 1914, 5: 2915, 6: 2783, 7: 2728, 8: 1925}
picked:  28
episode: 143/2000 -> reward: 41.85416666666666, steps:19307, time-taken: 0.79min, time-elasped: 203.33min
-> berries picked: 28 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3521 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [346, 390, 338, 287, 454, 391, 562, 423, 330]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 8, 7, 10, 14, 4, 16, 10, 25]
	Time taken saving stuff: 0.09s

=== episode:144 Env-steps-taken:58176
action_counts: {0: 3894, 1: 2475, 2: 2321, 3: 1375, 4: 1529, 5: 2244, 6: 1881, 7: 2104, 8: 2717}
picked:  35
episode: 144/2000 -> reward: 54.8177083333333, steps:20540, time-taken: 0.83min, time-elasped: 204.16min
-> berries picked: 35 of 800 | patches-visited: [0, 3, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3539 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [351, 392, 338, 287, 454, 392, 567, 426, 332]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 11, 8, 14, 9, 9, 17, 8, 24]
	Time taken saving stuff: 0.10s

=== episode:145 Env-steps-taken:60096
action_counts: {0: 2387, 1: 4279, 2: 3421, 3: 2750, 4: 3179, 5: 4938, 6: 5753, 7: 4554, 8: 5082}
picked:  42
episode: 145/2000 -> reward: 65.78124999999997, steps:36343, time-taken: 1.19min, time-elasped: 205.36min
-> berries picked: 42 of 800 | patches-visited: [0, 3, 4, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3542 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [347, 393, 340, 286, 451, 393, 566, 430, 336]
	| approx positives in sample 512: 120
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 14, 14, 12, 10, 4, 17, 8, 28]
	Time taken saving stuff: 0.02s

=== episode:146 Env-steps-taken:62208
action_counts: {0: 2123, 1: 3366, 2: 3201, 3: 2849, 4: 2761, 5: 4213, 6: 5159, 7: 4268, 8: 3370}
picked:  47
episode: 146/2000 -> reward: 75.75520833333333, steps:31310, time-taken: 1.11min, time-elasped: 206.48min
-> berries picked: 47 of 800 | patches-visited: [0, 1, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3544 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [348, 390, 336, 285, 451, 394, 567, 435, 338]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 15, 14, 5, 10, 11, 15, 10, 24]
	Time taken saving stuff: 0.10s

=== episode:147 Env-steps-taken:58944
action_counts: {0: 2750, 1: 3311, 2: 2838, 3: 1804, 4: 1958, 5: 2783, 6: 4830, 7: 3608, 8: 3080}
picked:  35
episode: 147/2000 -> reward: 57.8177083333333, steps:26962, time-taken: 1.04min, time-elasped: 207.52min
-> berries picked: 35 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3547 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [351, 386, 336, 290, 449, 397, 566, 433, 339]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 6, 11, 12, 17, 9, 13, 8, 19]
	Time taken saving stuff: 0.10s

=== episode:148 Env-steps-taken:63744
action_counts: {0: 3564, 1: 4273, 2: 3696, 3: 3256, 4: 4136, 5: 7524, 6: 6402, 7: 6820, 8: 7194}
picked:  54
episode: 148/2000 -> reward: 82.71875, steps:46865, time-taken: 1.48min, time-elasped: 209.01min
-> berries picked: 54 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3525 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [350, 379, 334, 277, 443, 395, 568, 436, 343]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 12, 18, 7, 6, 6, 10, 11, 19]
	Time taken saving stuff: 0.00s

=== episode:149 Env-steps-taken:61920
action_counts: {0: 3696, 1: 3586, 2: 3355, 3: 3058, 4: 2046, 5: 2662, 6: 5544, 7: 4664, 8: 5456}
picked:  50
episode: 149/2000 -> reward: 74.23958333333333, steps:34067, time-taken: 1.17min, time-elasped: 210.19min
-> berries picked: 50 of 800 | patches-visited: [0, 2, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3536 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [353, 385, 333, 286, 439, 390, 570, 433, 347]
	| approx positives in sample 512: 88
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 5, 4, 7, 6, 9, 14, 11, 22]
	Time taken saving stuff: 0.10s

=== episode:150 Env-steps-taken:51744
action_counts: {0: 858, 1: 649, 2: 924, 3: 781, 4: 1353, 5: 418, 6: 572, 7: 649, 8: 1596}
picked:  13
episode: 150/2000 -> reward: 19.432291666666668, steps:7800, time-taken: 0.50min, time-elasped: 210.69min
-> berries picked: 13 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3536 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [352, 384, 331, 289, 440, 391, 571, 431, 347]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 13, 10, 3, 14, 14, 17, 14, 22]
	Time taken saving stuff: 0.05s

=== episode:15 Env-steps-taken:58176
action_counts: {0: 8151, 1: 2101, 2: 121, 3: 715, 4: 715, 5: 10153, 6: 5346, 7: 1177, 8: 11441}
picked:  42

==================================================
eval-episode: 150 -> reward: 52.781249999999964, steps: 39920.0, wall-time: 28.85s
-> berries picked: 42 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:151 Env-steps-taken:62880
action_counts: {0: 5698, 1: 5533, 2: 4664, 3: 5148, 4: 5214, 5: 7590, 6: 8588, 7: 6600, 8: 9108}
picked:  67
episode: 151/2000 -> reward: 78.15104166666667, steps:58143, time-taken: 1.74min, time-elasped: 212.91min
-> berries picked: 67 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3549 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [354, 382, 327, 287, 448, 393, 579, 430, 349]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 9, 10, 5, 12, 8, 14, 10, 22]
	Time taken saving stuff: 0.11s

=== episode:152 Env-steps-taken:50208
action_counts: {0: 396, 1: 165, 2: 220, 3: 220, 4: 495, 5: 242, 6: 187, 7: 231, 8: 334}
picked:  7
episode: 152/2000 -> reward: 11.463541666666666, steps:2490, time-taken: 0.34min, time-elasped: 213.26min
-> berries picked: 7 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3553 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [354, 381, 327, 288, 447, 394, 580, 432, 350]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 12, 18, 7, 16, 12, 15, 11, 17]
	Time taken saving stuff: 0.00s

=== episode:153 Env-steps-taken:56544
action_counts: {0: 2717, 1: 2123, 2: 1485, 3: 1987, 4: 2739, 5: 2728, 6: 2387, 7: 3267, 8: 3971}
picked:  29
episode: 153/2000 -> reward: 46.34895833333332, steps:23404, time-taken: 0.89min, time-elasped: 214.15min
-> berries picked: 29 of 800 | patches-visited: [0, 3, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3563 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [358, 385, 325, 292, 451, 393, 576, 431, 352]
	| approx positives in sample 512: 97
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 9, 9, 8, 4, 11, 15, 15, 18]
	Time taken saving stuff: 0.10s

=== episode:154 Env-steps-taken:58080
action_counts: {0: 2849, 1: 3171, 2: 2585, 3: 1936, 4: 1848, 5: 2277, 6: 2794, 7: 3245, 8: 3146}
picked:  35
episode: 154/2000 -> reward: 54.31770833333331, steps:23851, time-taken: 0.96min, time-elasped: 215.11min
-> berries picked: 35 of 800 | patches-visited: [0, 2, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3582 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [362, 386, 326, 295, 451, 392, 579, 437, 354]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 7, 11, 9, 20, 15, 10, 20, 14]
	Time taken saving stuff: 0.10s

=== episode:155 Env-steps-taken:62496
action_counts: {0: 3366, 1: 2805, 2: 3168, 3: 2651, 4: 2761, 5: 2871, 6: 5621, 7: 3806, 8: 4420}
picked:  53
episode: 155/2000 -> reward: 76.22395833333333, steps:31469, time-taken: 1.09min, time-elasped: 216.21min
-> berries picked: 53 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3601 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [366, 384, 325, 297, 446, 394, 586, 446, 357]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [20, 16, 7, 9, 8, 7, 17, 13, 13]
	Time taken saving stuff: 0.11s

=== episode:156 Env-steps-taken:60576
action_counts: {0: 5170, 1: 4422, 2: 2992, 3: 2409, 4: 3146, 5: 4422, 6: 4675, 7: 6281, 8: 5177}
picked:  49
episode: 156/2000 -> reward: 66.24479166666663, steps:38694, time-taken: 1.32min, time-elasped: 217.53min
-> berries picked: 49 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3592 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [363, 376, 327, 297, 443, 393, 589, 444, 360]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 19, 9, 7, 9, 8, 15, 11, 20]
	Time taken saving stuff: 0.09s

=== episode:157 Env-steps-taken:60096
action_counts: {0: 8360, 1: 5874, 2: 4708, 3: 4037, 4: 4389, 5: 6248, 6: 8140, 7: 7205, 8: 8009}
picked:  50
episode: 157/2000 -> reward: 62.739583333333286, steps:56970, time-taken: 1.73min, time-elasped: 219.26min
-> berries picked: 50 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3583 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [362, 374, 322, 298, 438, 390, 593, 447, 359]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 15, 6, 10, 7, 7, 16, 10, 24]
	Time taken saving stuff: 0.01s

=== episode:158 Env-steps-taken:57120
action_counts: {0: 1936, 1: 1705, 2: 1947, 3: 1309, 4: 1837, 5: 2618, 6: 2365, 7: 2145, 8: 2234}
picked:  29
episode: 158/2000 -> reward: 48.34895833333332, steps:18096, time-taken: 0.81min, time-elasped: 220.07min
-> berries picked: 29 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3595 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [366, 375, 321, 299, 437, 393, 596, 448, 360]
	| approx positives in sample 512: 120
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 10, 13, 10, 18, 12, 18, 12, 20]
	Time taken saving stuff: 0.01s

=== episode:159 Env-steps-taken:59616
action_counts: {0: 3498, 1: 2750, 2: 2904, 3: 2211, 4: 2235, 5: 2651, 6: 3333, 7: 3487, 8: 3630}
picked:  39
episode: 159/2000 -> reward: 62.29687499999997, steps:26699, time-taken: 0.99min, time-elasped: 221.07min
-> berries picked: 39 of 800 | patches-visited: [0, 6, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3599 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [365, 376, 315, 296, 438, 399, 598, 450, 362]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 14, 7, 10, 13, 10, 16, 10, 24]
	Time taken saving stuff: 0.10s

=== episode:160 Env-steps-taken:65184
action_counts: {0: 4741, 1: 5555, 2: 4759, 3: 3795, 4: 4664, 5: 5225, 6: 6743, 7: 7260, 8: 8261}
picked:  62
episode: 160/2000 -> reward: 90.17708333333337, steps:51003, time-taken: 1.63min, time-elasped: 222.70min
-> berries picked: 62 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3597 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [363, 376, 316, 294, 438, 394, 596, 456, 364]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 13, 10, 8, 13, 5, 14, 14, 17]
	Time taken saving stuff: 0.15s

=== episode:16 Env-steps-taken:89088
action_counts: {0: 4818, 1: 7172, 2: 10351, 3: 2739, 4: 11770, 5: 2992, 6: 10208, 7: 7777, 8: 6772}
picked:  156

==================================================
eval-episode: 160 -> reward: 216.18749999999952, steps: 64599.0, wall-time: 40.35s
-> berries picked: 156 of 800 | patches-visited: [0, 1, 3, 9] | juice left:-0.00
==================================================


=== episode:161 Env-steps-taken:61728
action_counts: {0: 3740, 1: 3839, 2: 3124, 3: 3069, 4: 3839, 5: 5797, 6: 4862, 7: 4455, 8: 5028}
picked:  53
episode: 161/2000 -> reward: 72.22395833333333, steps:37753, time-taken: 1.33min, time-elasped: 224.71min
-> berries picked: 53 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3604 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [367, 377, 313, 299, 439, 386, 600, 456, 367]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 15, 13, 9, 9, 3, 13, 10, 32]
	Time taken saving stuff: 0.09s

=== episode:162 Env-steps-taken:49824
action_counts: {0: 286, 1: 374, 2: 220, 3: 231, 4: 330, 5: 440, 6: 495, 7: 352, 8: 606}
picked:  7
episode: 162/2000 -> reward: 9.463541666666666, steps:3334, time-taken: 0.34min, time-elasped: 225.05min
-> berries picked: 7 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3609 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [368, 377, 313, 298, 441, 388, 600, 457, 367]
	| approx positives in sample 512: 125
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 10, 9, 7, 11, 12, 21, 15, 27]
	Time taken saving stuff: 0.00s

=== episode:163 Env-steps-taken:66624
action_counts: {0: 5687, 1: 3575, 2: 3047, 3: 2937, 4: 3135, 5: 3850, 6: 5973, 7: 6094, 8: 4896}
picked:  63
episode: 163/2000 -> reward: 98.67187500000007, steps:39194, time-taken: 1.32min, time-elasped: 226.37min
-> berries picked: 63 of 800 | patches-visited: [0, 7, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3619 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [368, 377, 313, 297, 439, 385, 607, 459, 374]
	| approx positives in sample 512: 95
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 13, 8, 10, 7, 6, 20, 9, 16]
	Time taken saving stuff: 0.01s

=== episode:164 Env-steps-taken:62016
action_counts: {0: 4675, 1: 4510, 2: 4378, 3: 3289, 4: 4664, 5: 6457, 6: 8349, 7: 7549, 8: 5555}
picked:  52
episode: 164/2000 -> reward: 73.72916666666666, steps:49426, time-taken: 1.50min, time-elasped: 227.87min
-> berries picked: 52 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3623 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [364, 379, 311, 295, 441, 389, 609, 458, 377]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 10, 10, 4, 8, 12, 11, 16, 21]
	Time taken saving stuff: 0.00s

=== episode:165 Env-steps-taken:51264
action_counts: {0: 638, 1: 759, 2: 1067, 3: 616, 4: 583, 5: 374, 6: 836, 7: 1184, 8: 1188}
picked:  11
episode: 165/2000 -> reward: 17.942708333333332, steps:7245, time-taken: 0.46min, time-elasped: 228.33min
-> berries picked: 11 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3626 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [362, 380, 313, 295, 441, 389, 609, 459, 378]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 10, 7, 11, 7, 5, 13, 19, 26]
	Time taken saving stuff: 0.11s

=== episode:166 Env-steps-taken:61248
action_counts: {0: 2299, 1: 2233, 2: 1925, 3: 1430, 4: 1903, 5: 2530, 6: 2838, 7: 3003, 8: 3462}
picked:  43
episode: 166/2000 -> reward: 69.77604166666664, steps:21623, time-taken: 0.87min, time-elasped: 229.21min
-> berries picked: 43 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3636 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [361, 383, 314, 292, 444, 389, 608, 466, 379]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 10, 11, 8, 15, 13, 11, 12, 18]
	Time taken saving stuff: 0.00s

=== episode:167 Env-steps-taken:69792
action_counts: {0: 4323, 1: 4268, 2: 5544, 3: 3575, 4: 4972, 5: 4323, 6: 5907, 7: 4785, 8: 5526}
picked:  76
episode: 167/2000 -> reward: 115.10416666666679, steps:43223, time-taken: 1.48min, time-elasped: 230.69min
-> berries picked: 76 of 800 | patches-visited: [0, 4, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3639 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [363, 380, 314, 294, 440, 392, 612, 462, 382]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 13, 12, 6, 6, 11, 11, 11, 23]
	Time taken saving stuff: 0.03s

=== episode:168 Env-steps-taken:60960
action_counts: {0: 2937, 1: 3344, 2: 4653, 3: 2442, 4: 2431, 5: 2024, 6: 3542, 7: 4246, 8: 3125}
picked:  42
episode: 168/2000 -> reward: 69.28124999999997, steps:28744, time-taken: 1.27min, time-elasped: 231.96min
-> berries picked: 42 of 800 | patches-visited: [0, 4, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3645 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [369, 379, 314, 292, 440, 392, 616, 459, 384]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 10, 14, 8, 11, 10, 21, 15, 19]
	Time taken saving stuff: 0.00s

=== episode:169 Env-steps-taken:69984
action_counts: {0: 5368, 1: 6423, 2: 8140, 3: 4653, 4: 6358, 5: 5104, 6: 6556, 7: 7348, 8: 6688}
picked:  81
episode: 169/2000 -> reward: 115.07812500000011, steps:56638, time-taken: 1.75min, time-elasped: 233.71min
-> berries picked: 81 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3653 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [367, 383, 315, 290, 436, 396, 621, 460, 385]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 10, 16, 9, 12, 9, 10, 9, 20]
	Time taken saving stuff: 0.10s

=== episode:170 Env-steps-taken:69600
action_counts: {0: 7282, 1: 6413, 2: 6773, 3: 5390, 4: 5962, 5: 8360, 6: 7909, 7: 7106, 8: 8415}
picked:  78
episode: 170/2000 -> reward: 113.09375000000013, steps:63610, time-taken: 1.91min, time-elasped: 235.63min
-> berries picked: 78 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3653 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [366, 387, 313, 291, 436, 391, 625, 456, 388]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 11, 16, 11, 15, 7, 13, 10, 17]
	Time taken saving stuff: 0.05s

=== episode:17 Env-steps-taken:58464
action_counts: {0: 737, 1: 1331, 2: 858, 3: 352, 4: 1540, 5: 8556, 6: 1463, 7: 1133, 8: 10956}
picked:  37

==================================================
eval-episode: 170 -> reward: 56.30729166666664, steps: 26926.0, wall-time: 24.68s
-> berries picked: 37 of 800 | patches-visited: [0, 1, 3] | juice left:-0.00
==================================================


=== episode:171 Env-steps-taken:67392
action_counts: {0: 4840, 1: 4862, 2: 4109, 3: 3509, 4: 4389, 5: 4862, 6: 5687, 7: 7535, 8: 5412}
picked:  73
episode: 171/2000 -> reward: 102.61979166666674, steps:45205, time-taken: 1.50min, time-elasped: 237.54min
-> berries picked: 73 of 800 | patches-visited: [0, 3, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3681 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [377, 390, 315, 287, 441, 390, 626, 463, 392]
	| approx positives in sample 512: 123
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 12, 16, 5, 12, 15, 13, 13, 27]
	Time taken saving stuff: 0.02s

=== episode:172 Env-steps-taken:68448
action_counts: {0: 7029, 1: 7887, 2: 5495, 3: 4026, 4: 5830, 5: 7491, 6: 6193, 7: 7348, 8: 7161}
picked:  78
episode: 172/2000 -> reward: 107.09375000000013, steps:58460, time-taken: 1.87min, time-elasped: 239.42min
-> berries picked: 78 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3686 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [381, 391, 319, 278, 436, 392, 631, 467, 391]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 10, 10, 10, 15, 10, 12, 10, 19]
	Time taken saving stuff: 0.01s

=== episode:173 Env-steps-taken:65952
action_counts: {0: 5595, 1: 8932, 2: 4114, 3: 5016, 4: 6303, 5: 8063, 6: 6270, 7: 6435, 8: 6886}
picked:  78
episode: 173/2000 -> reward: 94.09375000000009, steps:57614, time-taken: 1.82min, time-elasped: 241.24min
-> berries picked: 78 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3691 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [373, 395, 317, 278, 440, 395, 631, 468, 394]
	| approx positives in sample 512: 127
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 14, 13, 9, 11, 11, 20, 20, 19]
	Time taken saving stuff: 0.10s

=== episode:174 Env-steps-taken:60384
action_counts: {0: 4301, 1: 3432, 2: 2684, 3: 2849, 4: 4554, 5: 5951, 6: 4664, 7: 4686, 8: 5258}
picked:  48
episode: 174/2000 -> reward: 66.24999999999997, steps:38379, time-taken: 1.30min, time-elasped: 242.54min
-> berries picked: 48 of 800 | patches-visited: [0, 1, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3686 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [375, 381, 314, 276, 441, 404, 627, 475, 393]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 9, 9, 12, 11, 16, 16, 8, 23]
	Time taken saving stuff: 0.10s

=== episode:175 Env-steps-taken:68640
action_counts: {0: 3828, 1: 5522, 2: 3707, 3: 3168, 4: 4554, 5: 4939, 6: 4323, 7: 4763, 8: 5214}
picked:  69
episode: 175/2000 -> reward: 108.1406250000001, steps:40018, time-taken: 1.36min, time-elasped: 243.91min
-> berries picked: 69 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3704 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [377, 385, 316, 274, 439, 412, 629, 478, 394]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 6, 9, 12, 13, 11, 8, 12, 24]
	Time taken saving stuff: 0.10s

=== episode:176 Env-steps-taken:58464
action_counts: {0: 2904, 1: 3190, 2: 2288, 3: 2002, 4: 2673, 5: 2904, 6: 5379, 7: 3553, 8: 3866}
picked:  39
episode: 176/2000 -> reward: 55.29687499999997, steps:28759, time-taken: 1.07min, time-elasped: 244.98min
-> berries picked: 39 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3702 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [372, 382, 311, 274, 442, 412, 634, 478, 397]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 10, 10, 9, 5, 10, 18, 9, 22]
	Time taken saving stuff: 0.01s

=== episode:177 Env-steps-taken:79104
action_counts: {0: 5654, 1: 7403, 2: 6517, 3: 4950, 4: 5555, 5: 7381, 6: 9724, 7: 8162, 8: 6149}
picked:  110
episode: 177/2000 -> reward: 163.42708333333331, steps:61495, time-taken: 1.92min, time-elasped: 246.90min
-> berries picked: 110 of 800 | patches-visited: [0, 3, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3736 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [369, 375, 309, 273, 452, 418, 650, 490, 400]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 12, 7, 9, 9, 10, 11, 19, 22]
	Time taken saving stuff: 0.10s

=== episode:178 Env-steps-taken:63456
action_counts: {0: 4422, 1: 4840, 2: 3168, 3: 2365, 4: 4389, 5: 4510, 6: 5258, 7: 3718, 8: 4593}
picked:  58
episode: 178/2000 -> reward: 82.19791666666669, steps:37263, time-taken: 1.28min, time-elasped: 248.18min
-> berries picked: 58 of 800 | patches-visited: [0, 1, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3753 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [372, 377, 307, 267, 462, 425, 654, 487, 402]
	| approx positives in sample 512: 89
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 7, 4, 4, 13, 13, 10, 10, 17]
	Time taken saving stuff: 0.02s

=== episode:179 Env-steps-taken:71424
action_counts: {0: 6215, 1: 4378, 2: 3762, 3: 3432, 4: 4213, 5: 6798, 6: 7018, 7: 6182, 8: 4422}
picked:  84
episode: 179/2000 -> reward: 123.56250000000016, steps:46420, time-taken: 1.62min, time-elasped: 249.81min
-> berries picked: 84 of 800 | patches-visited: [0, 1, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3780 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [379, 374, 308, 264, 460, 429, 668, 493, 405]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 2, 13, 7, 16, 13, 18, 12, 21]
	Time taken saving stuff: 0.10s

=== episode:180 Env-steps-taken:67488
action_counts: {0: 3894, 1: 4334, 2: 3179, 3: 3179, 4: 3872, 5: 3751, 6: 4994, 7: 4609, 8: 3939}
picked:  70
episode: 180/2000 -> reward: 103.13541666666674, steps:35751, time-taken: 1.24min, time-elasped: 251.06min
-> berries picked: 70 of 800 | patches-visited: [0, 3, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3799 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [381, 370, 312, 264, 456, 434, 674, 501, 407]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 10, 10, 10, 13, 12, 15, 14, 20]
	Time taken saving stuff: 0.15s

=== episode:18 Env-steps-taken:60672
action_counts: {0: 1562, 1: 17501, 2: 2530, 3: 275, 4: 770, 5: 17820, 6: 2321, 7: 3949, 8: 353}
picked:  46

==================================================
eval-episode: 180 -> reward: 66.76041666666663, steps: 47081.0, wall-time: 31.30s
-> berries picked: 46 of 800 | patches-visited: [1, 5] | juice left:-0.00
==================================================


=== episode:181 Env-steps-taken:61728
action_counts: {0: 3795, 1: 3784, 2: 4774, 3: 2288, 4: 2563, 5: 4015, 6: 5126, 7: 4840, 8: 5754}
picked:  55
episode: 181/2000 -> reward: 72.21354166666664, steps:36939, time-taken: 1.22min, time-elasped: 252.80min
-> berries picked: 55 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3782 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [379, 356, 311, 260, 450, 434, 681, 506, 405]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 13, 12, 7, 18, 12, 16, 7, 29]
	Time taken saving stuff: 0.00s

=== episode:182 Env-steps-taken:63072
action_counts: {0: 3531, 1: 4092, 2: 2486, 3: 2622, 4: 3498, 5: 4301, 6: 5214, 7: 4653, 8: 3619}
picked:  56
episode: 182/2000 -> reward: 80.20833333333334, steps:34016, time-taken: 1.20min, time-elasped: 254.00min
-> berries picked: 56 of 800 | patches-visited: [0, 5, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3783 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [376, 360, 308, 256, 447, 438, 686, 502, 410]
	| approx positives in sample 512: 92
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 10, 5, 7, 5, 12, 16, 8, 19]
	Time taken saving stuff: 0.01s

=== episode:183 Env-steps-taken:64320
action_counts: {0: 3718, 1: 4532, 2: 3047, 3: 2926, 4: 3399, 5: 4785, 6: 4950, 7: 4356, 8: 4530}
picked:  56
episode: 183/2000 -> reward: 86.70833333333337, steps:36243, time-taken: 1.24min, time-elasped: 255.24min
-> berries picked: 56 of 800 | patches-visited: [0, 3, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3773 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [378, 351, 308, 254, 443, 438, 685, 502, 414]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 8, 5, 10, 7, 10, 23, 16, 21]
	Time taken saving stuff: 0.00s

=== episode:184 Env-steps-taken:68832
action_counts: {0: 4994, 1: 5005, 2: 3575, 3: 4697, 4: 4312, 5: 6413, 6: 6765, 7: 5819, 8: 7855}
picked:  76
episode: 184/2000 -> reward: 111.10416666666676, steps:49435, time-taken: 1.64min, time-elasped: 256.88min
-> berries picked: 76 of 800 | patches-visited: [0, 1, 3, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3787 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [370, 360, 304, 264, 444, 444, 686, 500, 415]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 6, 13, 10, 9, 14, 19, 14, 16]
	Time taken saving stuff: 0.10s

=== episode:185 Env-steps-taken:62592
action_counts: {0: 2827, 1: 4466, 2: 4301, 3: 2299, 4: 2552, 5: 2420, 6: 3476, 7: 2981, 8: 4027}
picked:  53
episode: 185/2000 -> reward: 77.72395833333333, steps:29349, time-taken: 1.06min, time-elasped: 257.94min
-> berries picked: 53 of 800 | patches-visited: [0, 1, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3791 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [369, 367, 309, 265, 449, 444, 674, 498, 416]
	| approx positives in sample 512: 87
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 7, 6, 4, 9, 8, 17, 15, 10]
	Time taken saving stuff: 0.00s

=== episode:186 Env-steps-taken:61344
action_counts: {0: 2365, 1: 3278, 2: 3410, 3: 2002, 4: 2541, 5: 2519, 6: 4477, 7: 3907, 8: 3267}
picked:  47
episode: 186/2000 -> reward: 71.25520833333331, steps:27766, time-taken: 1.04min, time-elasped: 258.99min
-> berries picked: 47 of 800 | patches-visited: [0, 2, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3800 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [368, 365, 314, 265, 449, 445, 676, 502, 416]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 6, 15, 11, 17, 10, 15, 9, 11]
	Time taken saving stuff: 0.10s

=== episode:187 Env-steps-taken:68928
action_counts: {0: 3564, 1: 5064, 2: 4521, 3: 4169, 4: 4939, 5: 5676, 6: 7172, 7: 5137, 8: 5247}
picked:  77
episode: 187/2000 -> reward: 109.59895833333346, steps:45489, time-taken: 1.50min, time-elasped: 260.49min
-> berries picked: 77 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3803 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [368, 359, 317, 266, 443, 453, 682, 500, 415]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 9, 9, 4, 10, 14, 17, 17, 16]
	Time taken saving stuff: 0.00s

=== episode:188 Env-steps-taken:62592
action_counts: {0: 3014, 1: 3289, 2: 2519, 3: 3047, 4: 3289, 5: 3542, 6: 3333, 7: 3157, 8: 3774}
picked:  49
episode: 188/2000 -> reward: 76.74479166666666, steps:28964, time-taken: 1.09min, time-elasped: 261.58min
-> berries picked: 49 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3802 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [366, 359, 315, 266, 446, 453, 682, 500, 415]
	| approx positives in sample 512: 129
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 9, 11, 16, 11, 26, 11, 26]
	Time taken saving stuff: 0.11s

=== episode:189 Env-steps-taken:64128
action_counts: {0: 3806, 1: 3542, 2: 3322, 3: 2871, 4: 4092, 5: 4466, 6: 4026, 7: 3223, 8: 3014}
picked:  54
episode: 189/2000 -> reward: 85.71875000000001, steps:32362, time-taken: 1.16min, time-elasped: 262.74min
-> berries picked: 54 of 800 | patches-visited: [0, 3, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3820 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [363, 360, 314, 271, 451, 457, 689, 498, 417]
	| approx positives in sample 512: 130
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 11, 9, 9, 15, 17, 25, 14, 17]
	Time taken saving stuff: 0.00s

=== episode:190 Env-steps-taken:72576
action_counts: {0: 7084, 1: 6842, 2: 5324, 3: 5203, 4: 6193, 5: 7260, 6: 7293, 7: 6732, 8: 6424}
picked:  91
episode: 190/2000 -> reward: 128.52604166666686, steps:58355, time-taken: 1.88min, time-elasped: 264.62min
-> berries picked: 91 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3835 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [368, 361, 314, 276, 450, 464, 692, 495, 415]
	| approx positives in sample 512: 124
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 7, 5, 11, 11, 20, 22, 29]
	Time taken saving stuff: 0.14s

=== episode:19 Env-steps-taken:66048
action_counts: {0: 2695, 1: 3311, 2: 2552, 3: 1012, 4: 1331, 5: 1826, 6: 3190, 7: 3883, 8: 2002}
picked:  62

==================================================
eval-episode: 190 -> reward: 95.6770833333334, steps: 21802.0, wall-time: 27.15s
-> berries picked: 62 of 800 | patches-visited: [0, 1, 3] | juice left:-0.00
==================================================


=== episode:191 Env-steps-taken:68448
action_counts: {0: 5158, 1: 5610, 2: 4730, 3: 4235, 4: 5643, 5: 6435, 6: 6776, 7: 6314, 8: 7997}
picked:  73
episode: 191/2000 -> reward: 108.11979166666677, steps:52898, time-taken: 1.71min, time-elasped: 266.79min
-> berries picked: 73 of 800 | patches-visited: [0, 5, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3834 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [366, 354, 314, 282, 446, 468, 700, 488, 416]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 8, 8, 7, 13, 14, 17, 8, 22]
	Time taken saving stuff: 0.01s

=== episode:192 Env-steps-taken:68352
action_counts: {0: 4961, 1: 4840, 2: 4345, 3: 3663, 4: 4367, 5: 6325, 6: 6864, 7: 5621, 8: 5193}
picked:  72
episode: 192/2000 -> reward: 107.6250000000001, steps:46179, time-taken: 1.55min, time-elasped: 268.34min
-> berries picked: 72 of 800 | patches-visited: [0, 7, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3859 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [367, 353, 314, 281, 450, 474, 710, 493, 417]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 8, 4, 11, 9, 10, 19, 17, 17]
	Time taken saving stuff: 0.00s

=== episode:193 Env-steps-taken:68832
action_counts: {0: 6292, 1: 5368, 2: 4279, 3: 4565, 4: 4642, 5: 8701, 6: 6996, 7: 5808, 8: 5336}
picked:  76
episode: 193/2000 -> reward: 109.10416666666676, steps:51987, time-taken: 1.62min, time-elasped: 269.97min
-> berries picked: 76 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3868 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [376, 344, 315, 276, 455, 483, 713, 487, 419]
	| approx positives in sample 512: 124
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 9, 11, 9, 21, 10, 18, 14, 21]
	Time taken saving stuff: 0.11s

=== episode:194 Env-steps-taken:70080
action_counts: {0: 5973, 1: 4466, 2: 4708, 3: 4499, 4: 5060, 5: 7568, 6: 6006, 7: 6413, 8: 5489}
picked:  74
episode: 194/2000 -> reward: 115.61458333333346, steps:50182, time-taken: 1.62min, time-elasped: 271.59min
-> berries picked: 74 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3884 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [377, 340, 318, 275, 454, 491, 720, 488, 421]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 8, 11, 11, 10, 9, 20, 10, 13]
	Time taken saving stuff: 0.00s

=== episode:195 Env-steps-taken:61728
action_counts: {0: 3003, 1: 2882, 2: 2816, 3: 2404, 4: 3058, 5: 3432, 6: 2090, 7: 2541, 8: 1969}
picked:  46
episode: 195/2000 -> reward: 72.26041666666666, steps:24195, time-taken: 0.96min, time-elasped: 272.55min
-> berries picked: 46 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3892 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [371, 341, 320, 277, 456, 492, 724, 487, 424]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 11, 4, 7, 19, 19, 17, 11, 19]
	Time taken saving stuff: 0.10s

=== episode:196 Env-steps-taken:61632
action_counts: {0: 4686, 1: 2981, 2: 2816, 3: 2321, 4: 2420, 5: 3256, 6: 4125, 7: 2871, 8: 3713}
picked:  50
episode: 196/2000 -> reward: 72.73958333333333, steps:29189, time-taken: 1.04min, time-elasped: 273.60min
-> berries picked: 50 of 800 | patches-visited: [0, 2, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3913 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [376, 342, 321, 279, 461, 492, 729, 489, 424]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 7, 8, 11, 15, 11, 18, 4, 20]
	Time taken saving stuff: 0.00s

=== episode:197 Env-steps-taken:67392
action_counts: {0: 4752, 1: 4477, 2: 3630, 3: 3124, 4: 3729, 5: 4598, 6: 5071, 7: 4829, 8: 3382}
picked:  62
episode: 197/2000 -> reward: 101.67708333333341, steps:37592, time-taken: 1.30min, time-elasped: 274.90min
-> berries picked: 62 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3934 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [380, 342, 322, 277, 465, 493, 733, 495, 427]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 10, 11, 6, 22, 14, 22, 14, 15]
	Time taken saving stuff: 0.02s

=== episode:198 Env-steps-taken:59136
action_counts: {0: 3564, 1: 2838, 2: 3190, 3: 2981, 4: 2662, 5: 3641, 6: 3740, 7: 2123, 8: 4006}
picked:  42
episode: 198/2000 -> reward: 57.78124999999997, steps:28745, time-taken: 1.04min, time-elasped: 275.94min
-> berries picked: 42 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3952 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [380, 344, 326, 276, 466, 500, 740, 494, 426]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 8, 10, 9, 14, 12, 12, 15, 16]
	Time taken saving stuff: 0.00s

=== episode:199 Env-steps-taken:74976
action_counts: {0: 6116, 1: 6886, 2: 5467, 3: 5445, 4: 5775, 5: 6457, 6: 7315, 7: 6864, 8: 7800}
picked:  99
episode: 199/2000 -> reward: 141.98437500000003, steps:58125, time-taken: 1.81min, time-elasped: 277.76min
-> berries picked: 99 of 800 | patches-visited: [0, 3, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3986 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [384, 345, 325, 281, 472, 497, 750, 504, 428]
	| approx positives in sample 512: 99
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 6, 13, 4, 8, 18, 17, 11, 17]
	Time taken saving stuff: 0.10s

=== episode:200 Env-steps-taken:60096
action_counts: {0: 2783, 1: 3498, 2: 2453, 3: 2387, 4: 3146, 5: 4257, 6: 2475, 7: 2123, 8: 3504}
picked:  43
episode: 200/2000 -> reward: 63.77604166666663, steps:26626, time-taken: 1.00min, time-elasped: 278.76min
-> berries picked: 43 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4005 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [384, 346, 327, 285, 476, 498, 755, 504, 430]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 13, 9, 4, 15, 10, 18, 10, 29]
	Time taken saving stuff: 0.15s

=== episode:20 Env-steps-taken:54048
action_counts: {0: 8239, 1: 1122, 2: 3300, 3: 242, 4: 12122, 5: 4433, 6: 770, 7: 1518, 8: 7773}
picked:  22

==================================================
eval-episode: 200 -> reward: 31.385416666666675, steps: 39519.0, wall-time: 25.59s
-> berries picked: 22 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:201 Env-steps-taken:64128
action_counts: {0: 3520, 1: 2762, 2: 2519, 3: 2827, 4: 3025, 5: 4719, 6: 5632, 7: 4565, 8: 4301}
picked:  55
episode: 201/2000 -> reward: 86.7135416666667, steps:33870, time-taken: 1.23min, time-elasped: 280.42min
-> berries picked: 55 of 800 | patches-visited: [0, 5, 6, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4031 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [382, 347, 329, 284, 479, 501, 768, 508, 433]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 8, 9, 10, 15, 13, 15, 15, 11]
	Time taken saving stuff: 0.00s

=== episode:202 Env-steps-taken:68544
action_counts: {0: 5786, 1: 5863, 2: 3949, 3: 4851, 4: 5203, 5: 6290, 6: 8437, 7: 6171, 8: 6435}
picked:  77
episode: 202/2000 -> reward: 107.59895833333344, steps:52985, time-taken: 1.69min, time-elasped: 282.11min
-> berries picked: 77 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4043 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [379, 347, 328, 282, 481, 506, 778, 509, 433]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 13, 8, 7, 15, 7, 13, 16, 26]
	Time taken saving stuff: 0.02s

=== episode:203 Env-steps-taken:69504
action_counts: {0: 4807, 1: 4972, 2: 3828, 3: 3597, 4: 4389, 5: 4356, 6: 5665, 7: 5115, 8: 4323}
picked:  75
episode: 203/2000 -> reward: 113.60937500000013, steps:41052, time-taken: 1.40min, time-elasped: 283.51min
-> berries picked: 75 of 800 | patches-visited: [0, 3, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4064 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [380, 345, 330, 287, 485, 515, 774, 511, 437]
	| approx positives in sample 512: 136
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 12, 16, 9, 16, 12, 24, 20, 18]
	Time taken saving stuff: 0.00s

=== episode:204 Env-steps-taken:60672
action_counts: {0: 4598, 1: 2805, 2: 2211, 3: 2816, 4: 2636, 5: 4125, 6: 3080, 7: 3531, 8: 3927}
picked:  47
episode: 204/2000 -> reward: 66.7552083333333, steps:29729, time-taken: 1.06min, time-elasped: 284.57min
-> berries picked: 47 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4081 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [381, 344, 330, 291, 490, 521, 775, 510, 439]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 6, 12, 5, 17, 13, 25, 7, 18]
	Time taken saving stuff: 0.10s

=== episode:205 Env-steps-taken:59040
action_counts: {0: 2739, 1: 2211, 2: 1573, 3: 1925, 4: 2673, 5: 3168, 6: 2860, 7: 3608, 8: 3388}
picked:  41
episode: 205/2000 -> reward: 59.28645833333329, steps:24145, time-taken: 0.92min, time-elasped: 285.50min
-> berries picked: 41 of 800 | patches-visited: [0, 1, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4101 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [384, 343, 327, 296, 490, 524, 782, 514, 441]
	| approx positives in sample 512: 126
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [17, 9, 13, 9, 17, 15, 20, 9, 17]
	Time taken saving stuff: 0.00s

=== episode:206 Env-steps-taken:72672
action_counts: {0: 6402, 1: 6006, 2: 5291, 3: 5786, 4: 6336, 5: 9757, 6: 6149, 7: 4708, 8: 4590}
picked:  80
episode: 206/2000 -> reward: 130.08333333333346, steps:55025, time-taken: 1.83min, time-elasped: 287.33min
-> berries picked: 80 of 800 | patches-visited: [0, 4, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4121 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [383, 346, 331, 301, 499, 528, 783, 507, 443]
	| approx positives in sample 512: 99
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 8, 6, 9, 15, 8, 17, 14, 15]
	Time taken saving stuff: 0.02s

=== episode:207 Env-steps-taken:60576
action_counts: {0: 2937, 1: 2860, 2: 2596, 3: 2662, 4: 3333, 5: 5159, 6: 4829, 7: 3256, 8: 6502}
picked:  49
episode: 207/2000 -> reward: 65.24479166666663, steps:34134, time-taken: 1.13min, time-elasped: 288.45min
-> berries picked: 49 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4127 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [382, 345, 327, 301, 501, 533, 784, 512, 442]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 12, 9, 8, 13, 16, 16, 7, 22]
	Time taken saving stuff: 0.00s

=== episode:208 Env-steps-taken:72960
action_counts: {0: 4785, 1: 5179, 2: 3905, 3: 5434, 4: 6160, 5: 9174, 6: 6776, 7: 5368, 8: 5093}
picked:  84
episode: 208/2000 -> reward: 131.5625000000001, steps:51874, time-taken: 1.69min, time-elasped: 290.15min
-> berries picked: 84 of 800 | patches-visited: [0, 4, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4163 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [380, 343, 330, 304, 512, 543, 791, 516, 444]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 14, 12, 4, 11, 19, 19, 15, 14]
	Time taken saving stuff: 0.10s

=== episode:209 Env-steps-taken:61056
action_counts: {0: 3168, 1: 4268, 2: 2475, 3: 2508, 4: 2387, 5: 4774, 6: 4037, 7: 3465, 8: 3975}
picked:  47
episode: 209/2000 -> reward: 68.75520833333331, steps:31057, time-taken: 1.11min, time-elasped: 291.27min
-> berries picked: 47 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4181 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [383, 349, 327, 308, 517, 544, 791, 518, 444]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 11, 13, 10, 8, 9, 16, 14, 26]
	Time taken saving stuff: 0.00s

=== episode:210 Env-steps-taken:68736
action_counts: {0: 5874, 1: 4224, 2: 3806, 3: 4301, 4: 5214, 5: 5379, 6: 5588, 7: 5104, 8: 4553}
picked:  74
episode: 210/2000 -> reward: 108.61458333333343, steps:44043, time-taken: 1.47min, time-elasped: 292.74min
-> berries picked: 74 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4212 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [388, 351, 331, 312, 518, 550, 796, 522, 444]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 10, 8, 20, 15, 17, 14, 13, 17]
	Time taken saving stuff: 0.06s

=== episode:21 Env-steps-taken:56640
action_counts: {0: 649, 1: 21895, 2: 1144, 3: 726, 4: 2134, 5: 10428, 6: 1408, 7: 803, 8: 12056}
picked:  28

==================================================
eval-episode: 210 -> reward: 45.85416666666666, steps: 51243.0, wall-time: 29.99s
-> berries picked: 28 of 800 | patches-visited: [0, 1] | juice left:-0.00
==================================================


=== episode:211 Env-steps-taken:61824
action_counts: {0: 3898, 1: 3916, 2: 2266, 3: 3091, 4: 3278, 5: 5478, 6: 4037, 7: 3377, 8: 4004}
picked:  48
episode: 211/2000 -> reward: 73.74999999999999, steps:33345, time-taken: 1.19min, time-elasped: 294.43min
-> berries picked: 48 of 800 | patches-visited: [0, 5, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4226 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [394, 348, 325, 312, 524, 556, 797, 520, 450]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 5, 7, 8, 11, 10, 28, 14, 26]
	Time taken saving stuff: 0.01s

=== episode:212 Env-steps-taken:56928
action_counts: {0: 1793, 1: 2299, 2: 1672, 3: 1199, 4: 1441, 5: 3135, 6: 2497, 7: 2508, 8: 2213}
picked:  29
episode: 212/2000 -> reward: 48.34895833333332, steps:18757, time-taken: 0.81min, time-elasped: 295.24min
-> berries picked: 29 of 800 | patches-visited: [0, 6, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4238 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [394, 347, 327, 308, 525, 558, 804, 525, 450]
	| approx positives in sample 512: 136
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 11, 9, 10, 20, 21, 16, 20, 19]
	Time taken saving stuff: 0.09s

=== episode:213 Env-steps-taken:66240
action_counts: {0: 6721, 1: 4587, 2: 4554, 3: 3146, 4: 5137, 5: 6666, 6: 7289, 7: 5445, 8: 5841}
picked:  64
episode: 213/2000 -> reward: 95.66666666666673, steps:49386, time-taken: 1.60min, time-elasped: 296.85min
-> berries picked: 64 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4252 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [390, 346, 328, 308, 527, 560, 814, 531, 448]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 9, 8, 7, 15, 16, 17, 13, 21]
	Time taken saving stuff: 0.11s

=== episode:214 Env-steps-taken:74784
action_counts: {0: 7260, 1: 6574, 2: 5632, 3: 6050, 4: 5929, 5: 7337, 6: 5610, 7: 7975, 8: 7579}
picked:  95
episode: 214/2000 -> reward: 141.0052083333334, steps:59946, time-taken: 1.92min, time-elasped: 298.77min
-> berries picked: 95 of 800 | patches-visited: [0, 2, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4283 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [391, 341, 339, 309, 526, 566, 826, 535, 450]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 8, 13, 6, 17, 16, 19, 7, 21]
	Time taken saving stuff: 0.11s

=== episode:215 Env-steps-taken:68544
action_counts: {0: 6061, 1: 4664, 2: 2838, 3: 3212, 4: 4312, 5: 6149, 6: 8239, 7: 5654, 8: 5864}
picked:  76
episode: 215/2000 -> reward: 107.60416666666677, steps:46993, time-taken: 1.48min, time-elasped: 300.25min
-> berries picked: 76 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4298 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [394, 338, 337, 302, 521, 576, 834, 543, 453]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 7, 6, 5, 12, 11, 23, 11, 10]
	Time taken saving stuff: 0.02s

=== episode:216 Env-steps-taken:62880
action_counts: {0: 3971, 1: 3828, 2: 3388, 3: 3542, 4: 4525, 5: 3685, 6: 5896, 7: 4411, 8: 4246}
picked:  53
episode: 216/2000 -> reward: 78.22395833333334, steps:37492, time-taken: 1.29min, time-elasped: 301.54min
-> berries picked: 53 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4323 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [389, 337, 341, 310, 525, 579, 842, 545, 455]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 9, 7, 5, 9, 16, 14, 13, 28]
	Time taken saving stuff: 0.10s

=== episode:217 Env-steps-taken:55680
action_counts: {0: 957, 1: 1408, 2: 1166, 3: 1617, 4: 1650, 5: 1353, 6: 1562, 7: 957, 8: 2342}
picked:  25
episode: 217/2000 -> reward: 41.869791666666664, steps:13012, time-taken: 0.63min, time-elasped: 302.18min
-> berries picked: 25 of 800 | patches-visited: [0, 2, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4339 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [390, 338, 344, 311, 528, 580, 844, 546, 458]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 6, 6, 3, 11, 6, 31, 13, 18]
	Time taken saving stuff: 0.00s

=== episode:218 Env-steps-taken:56160
action_counts: {0: 1771, 1: 1089, 2: 1100, 3: 825, 4: 1496, 5: 1793, 6: 1848, 7: 2519, 8: 2367}
picked:  29
episode: 218/2000 -> reward: 43.34895833333332, steps:14808, time-taken: 0.71min, time-elasped: 302.89min
-> berries picked: 29 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4352 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [393, 340, 345, 312, 528, 584, 843, 549, 458]
	| approx positives in sample 512: 123
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 6, 10, 9, 15, 13, 19, 16, 22]
	Time taken saving stuff: 0.01s

=== episode:219 Env-steps-taken:68256
action_counts: {0: 4411, 1: 3168, 2: 2937, 3: 4334, 4: 5346, 5: 3960, 6: 4092, 7: 3487, 8: 4551}
picked:  72
episode: 219/2000 -> reward: 107.12500000000009, steps:36286, time-taken: 1.28min, time-elasped: 304.17min
-> berries picked: 72 of 800 | patches-visited: [0, 2, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4384 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [387, 339, 346, 315, 537, 589, 858, 556, 457]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 12, 9, 7, 12, 8, 20, 13, 15]
	Time taken saving stuff: 0.10s

=== episode:220 Env-steps-taken:62976
action_counts: {0: 3476, 1: 2893, 2: 2981, 3: 3620, 4: 4444, 5: 4466, 6: 5038, 7: 5082, 8: 4873}
picked:  58
episode: 220/2000 -> reward: 78.69791666666669, steps:36873, time-taken: 1.26min, time-elasped: 305.44min
-> berries picked: 58 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4397 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [386, 340, 347, 315, 537, 594, 860, 560, 458]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 13, 12, 5, 13, 18, 19, 10, 20]
	Time taken saving stuff: 0.15s

=== episode:22 Env-steps-taken:77760
action_counts: {0: 2398, 1: 1925, 2: 561, 3: 2893, 4: 1551, 5: 8712, 6: 5214, 7: 7370, 8: 11306}
picked:  107

==================================================
eval-episode: 220 -> reward: 157.44270833333326, steps: 41930.0, wall-time: 36.13s
-> berries picked: 107 of 800 | patches-visited: [0, 1, 2, 9] | juice left:-0.00
==================================================


=== episode:221 Env-steps-taken:63360
action_counts: {0: 5258, 1: 3993, 2: 3465, 3: 4235, 4: 6259, 5: 5940, 6: 5555, 7: 6127, 8: 6037}
picked:  57
episode: 221/2000 -> reward: 79.70312500000001, steps:46869, time-taken: 1.44min, time-elasped: 307.49min
-> berries picked: 57 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4411 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [389, 342, 344, 314, 536, 596, 870, 561, 459]
	| approx positives in sample 512: 123
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 12, 13, 14, 10, 15, 15, 12, 23]
	Time taken saving stuff: 0.10s

=== episode:222 Env-steps-taken:67968
action_counts: {0: 5236, 1: 5302, 2: 4323, 3: 4794, 4: 5522, 5: 5379, 6: 5698, 7: 5654, 8: 6809}
picked:  78
episode: 222/2000 -> reward: 105.59375000000011, steps:48717, time-taken: 1.65min, time-elasped: 309.14min
-> berries picked: 78 of 800 | patches-visited: [0, 4, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4421 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [389, 335, 342, 317, 538, 606, 869, 565, 460]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 15, 18, 10, 11, 2, 22, 9, 18]
	Time taken saving stuff: 0.00s

=== episode:223 Env-steps-taken:68448
action_counts: {0: 6061, 1: 4312, 2: 4026, 3: 5104, 4: 6765, 5: 6831, 6: 4521, 7: 5170, 8: 5378}
picked:  76
episode: 223/2000 -> reward: 108.10416666666677, steps:48168, time-taken: 1.54min, time-elasped: 310.69min
-> berries picked: 76 of 800 | patches-visited: [0, 6, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4447 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [384, 339, 347, 319, 545, 615, 875, 566, 457]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 7, 5, 5, 19, 15, 17, 16, 16]
	Time taken saving stuff: 0.10s

=== episode:224 Env-steps-taken:70368
action_counts: {0: 4169, 1: 3938, 2: 4477, 3: 5181, 4: 6204, 5: 4356, 6: 5093, 7: 5489, 8: 5589}
picked:  78
episode: 224/2000 -> reward: 117.09375000000014, steps:44496, time-taken: 1.45min, time-elasped: 312.15min
-> berries picked: 78 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4470 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [381, 336, 349, 330, 545, 616, 887, 569, 457]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 5, 6, 8, 22, 10, 29, 17, 11]
	Time taken saving stuff: 0.00s

=== episode:225 Env-steps-taken:64128
action_counts: {0: 4411, 1: 3058, 2: 2772, 3: 3091, 4: 3454, 5: 4609, 6: 6083, 7: 3388, 8: 3743}
picked:  61
episode: 225/2000 -> reward: 84.68229166666667, steps:34609, time-taken: 1.19min, time-elasped: 313.34min
-> berries picked: 61 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4484 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [387, 339, 348, 329, 542, 613, 893, 577, 456]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 8, 9, 7, 10, 9, 19, 15, 19]
	Time taken saving stuff: 0.02s

=== episode:226 Env-steps-taken:76032
action_counts: {0: 7557, 1: 6336, 2: 4873, 3: 6633, 4: 5687, 5: 5764, 6: 7992, 7: 7106, 8: 5434}
picked:  101
episode: 226/2000 -> reward: 146.47395833333343, steps:57382, time-taken: 1.85min, time-elasped: 315.19min
-> berries picked: 101 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4506 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [395, 344, 355, 326, 538, 615, 899, 576, 458]
	| approx positives in sample 512: 137
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 12, 12, 7, 11, 22, 25, 14, 19]
	Time taken saving stuff: 0.00s

=== episode:227 Env-steps-taken:56256
action_counts: {0: 1474, 1: 968, 2: 891, 3: 1342, 4: 1991, 5: 2101, 6: 2926, 7: 1936, 8: 1343}
picked:  29
episode: 227/2000 -> reward: 42.84895833333332, steps:14972, time-taken: 0.66min, time-elasped: 315.85min
-> berries picked: 29 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4511 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [395, 344, 353, 322, 538, 620, 903, 576, 460]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 8, 6, 8, 11, 20, 23, 14, 23]
	Time taken saving stuff: 0.09s

=== episode:228 Env-steps-taken:59040
action_counts: {0: 2387, 1: 2299, 2: 2112, 3: 1991, 4: 3465, 5: 2409, 6: 2277, 7: 2607, 8: 2089}
picked:  39
episode: 228/2000 -> reward: 58.29687499999996, steps:21636, time-taken: 0.87min, time-elasped: 316.73min
-> berries picked: 39 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4522 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [397, 343, 357, 321, 543, 620, 902, 579, 460]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 8, 4, 9, 16, 13, 21, 10, 15]
	Time taken saving stuff: 0.00s

=== episode:229 Env-steps-taken:55584
action_counts: {0: 1804, 1: 1782, 2: 1694, 3: 1595, 4: 1749, 5: 1529, 6: 1496, 7: 1056, 8: 2594}
picked:  25
episode: 229/2000 -> reward: 39.369791666666664, steps:15299, time-taken: 0.67min, time-elasped: 317.40min
-> berries picked: 25 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4525 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [399, 343, 355, 320, 545, 622, 903, 580, 458]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 7, 8, 6, 14, 14, 23, 12, 22]
	Time taken saving stuff: 0.10s

=== episode:230 Env-steps-taken:63744
action_counts: {0: 5236, 1: 4224, 2: 3223, 3: 4939, 4: 4884, 5: 4477, 6: 3894, 7: 4972, 8: 3950}
picked:  58
episode: 230/2000 -> reward: 82.69791666666671, steps:39799, time-taken: 1.34min, time-elasped: 318.75min
-> berries picked: 58 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4528 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [397, 345, 351, 320, 546, 623, 904, 583, 459]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 6, 4, 10, 14, 9, 13, 14, 23]
	Time taken saving stuff: 0.05s

=== episode:23 Env-steps-taken:69024
action_counts: {0: 14124, 1: 1133, 2: 528, 3: 2112, 4: 1716, 5: 3619, 6: 3597, 7: 3696, 8: 7426}
picked:  73

==================================================
eval-episode: 230 -> reward: 111.11979166666679, steps: 37951.0, wall-time: 31.03s
-> berries picked: 73 of 800 | patches-visited: [1, 5, 7] | juice left:-0.00
==================================================


=== episode:231 Env-steps-taken:63168
action_counts: {0: 4301, 1: 3872, 2: 2981, 3: 3993, 4: 3509, 5: 2706, 6: 5093, 7: 3014, 8: 3528}
picked:  54
episode: 231/2000 -> reward: 80.71875000000001, steps:32997, time-taken: 1.18min, time-elasped: 320.44min
-> berries picked: 54 of 800 | patches-visited: [0, 8, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4525 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [393, 346, 348, 319, 551, 620, 910, 581, 457]
	| approx positives in sample 512: 128
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 10, 12, 16, 18, 16, 20, 19, 13]
	Time taken saving stuff: 0.11s

=== episode:232 Env-steps-taken:66240
action_counts: {0: 5016, 1: 3982, 2: 3982, 3: 4367, 4: 6292, 5: 4851, 6: 5995, 7: 4097, 8: 6149}
picked:  64
episode: 232/2000 -> reward: 96.66666666666674, steps:44731, time-taken: 1.49min, time-elasped: 321.94min
-> berries picked: 64 of 800 | patches-visited: [0, 3, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4527 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [383, 347, 339, 318, 557, 628, 915, 580, 460]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 14, 12, 7, 8, 14, 20, 15, 17]
	Time taken saving stuff: 0.10s

=== episode:233 Env-steps-taken:53184
action_counts: {0: 1584, 1: 1177, 2: 935, 3: 583, 4: 517, 5: 473, 6: 638, 7: 880, 8: 661}
picked:  15
episode: 233/2000 -> reward: 26.921875000000004, steps:7448, time-taken: 0.49min, time-elasped: 322.43min
-> berries picked: 15 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4524 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [385, 349, 336, 318, 556, 625, 915, 580, 460]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 5, 10, 7, 18, 9, 21, 15, 25]
	Time taken saving stuff: 0.01s

=== episode:234 Env-steps-taken:70464
action_counts: {0: 5720, 1: 5588, 2: 4950, 3: 5368, 4: 7359, 5: 7502, 6: 7843, 7: 5148, 8: 5881}
picked:  83
episode: 234/2000 -> reward: 117.5677083333335, steps:55359, time-taken: 1.76min, time-elasped: 324.19min
-> berries picked: 83 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4525 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [380, 346, 327, 324, 561, 630, 917, 581, 459]
	| approx positives in sample 512: 120
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 6, 16, 11, 14, 17, 21, 14, 15]
	Time taken saving stuff: 0.13s

=== episode:235 Env-steps-taken:74592
action_counts: {0: 4059, 1: 3927, 2: 3509, 3: 6820, 4: 4642, 5: 5225, 6: 6127, 7: 4840, 8: 2730}
picked:  92
episode: 235/2000 -> reward: 141.0208333333334, steps:41879, time-taken: 1.46min, time-elasped: 325.65min
-> berries picked: 92 of 800 | patches-visited: [0, 1, 4, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4538 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [379, 341, 331, 328, 562, 635, 924, 577, 461]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 5, 10, 5, 12, 13, 20, 15, 16]
	Time taken saving stuff: 0.01s

=== episode:236 Env-steps-taken:70656
action_counts: {0: 7370, 1: 4785, 2: 5192, 3: 6281, 4: 6358, 5: 5665, 6: 6710, 7: 4620, 8: 6490}
picked:  88
episode: 236/2000 -> reward: 118.54166666666684, steps:53471, time-taken: 1.69min, time-elasped: 327.34min
-> berries picked: 88 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4558 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [374, 344, 330, 333, 569, 643, 930, 577, 458]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 6, 11, 8, 11, 14, 21, 15, 13]
	Time taken saving stuff: 0.12s

=== episode:237 Env-steps-taken:65760
action_counts: {0: 4037, 1: 3883, 2: 3608, 3: 5203, 4: 6270, 5: 4202, 6: 4356, 7: 3498, 8: 4015}
picked:  64
episode: 237/2000 -> reward: 93.16666666666673, steps:39072, time-taken: 1.27min, time-elasped: 328.62min
-> berries picked: 64 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4566 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [367, 340, 329, 333, 574, 647, 935, 579, 462]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 10, 5, 12, 18, 12, 20, 13, 14]
	Time taken saving stuff: 0.10s

=== episode:238 Env-steps-taken:62016
action_counts: {0: 2717, 1: 2618, 2: 2299, 3: 2832, 4: 2772, 5: 3850, 6: 4279, 7: 2519, 8: 5390}
picked:  50
episode: 238/2000 -> reward: 75.73958333333334, steps:29276, time-taken: 1.08min, time-elasped: 329.70min
-> berries picked: 50 of 800 | patches-visited: [0, 2, 4, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4571 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [366, 342, 330, 331, 576, 651, 940, 571, 464]
	| approx positives in sample 512: 123
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 8, 4, 8, 19, 19, 21, 18, 16]
	Time taken saving stuff: 0.10s

=== episode:239 Env-steps-taken:55584
action_counts: {0: 1298, 1: 1419, 2: 1826, 3: 1694, 4: 1650, 5: 1980, 6: 2079, 7: 1683, 8: 1821}
picked:  25
episode: 239/2000 -> reward: 39.369791666666664, steps:15450, time-taken: 0.72min, time-elasped: 330.42min
-> berries picked: 25 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4560 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [366, 341, 327, 327, 576, 653, 939, 568, 463]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 10, 2, 6, 18, 19, 16, 18, 18]
	Time taken saving stuff: 0.01s

=== episode:240 Env-steps-taken:59232
action_counts: {0: 3542, 1: 3927, 2: 2420, 3: 3795, 4: 4191, 5: 3168, 6: 3641, 7: 2783, 8: 3113}
picked:  43
episode: 240/2000 -> reward: 58.276041666666636, steps:30580, time-taken: 1.03min, time-elasped: 331.45min
-> berries picked: 43 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4563 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [365, 341, 323, 331, 578, 654, 942, 567, 462]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 7, 4, 6, 17, 17, 20, 14, 13]
	Time taken saving stuff: 0.06s

=== episode:24 Env-steps-taken:76992
action_counts: {0: 14350, 1: 3531, 2: 6512, 3: 7590, 4: 3883, 5: 10626, 6: 4620, 7: 2101, 8: 2552}
picked:  102

==================================================
eval-episode: 240 -> reward: 153.46874999999994, steps: 55765.0, wall-time: 34.41s
-> berries picked: 102 of 800 | patches-visited: [1, 4, 5, 7] | juice left:-0.00
==================================================


=== episode:241 Env-steps-taken:61824
action_counts: {0: 3069, 1: 2585, 2: 2519, 3: 2475, 4: 4719, 5: 4264, 6: 4499, 7: 2574, 8: 2398}
picked:  44
episode: 241/2000 -> reward: 72.77083333333331, steps:29102, time-taken: 1.06min, time-elasped: 333.08min
-> berries picked: 44 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4565 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [365, 339, 323, 329, 581, 658, 942, 566, 462]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 12, 9, 8, 13, 12, 24, 9, 20]
	Time taken saving stuff: 0.02s

=== episode:242 Env-steps-taken:57504
action_counts: {0: 2431, 1: 2299, 2: 2563, 3: 1771, 4: 3355, 5: 2629, 6: 2596, 7: 2310, 8: 3422}
picked:  35
episode: 242/2000 -> reward: 49.317708333333314, steps:23376, time-taken: 0.88min, time-elasped: 333.96min
-> berries picked: 35 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4561 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [363, 332, 325, 332, 581, 665, 944, 562, 457]
	| approx positives in sample 512: 126
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 9, 5, 7, 26, 17, 14, 13, 20]
	Time taken saving stuff: 0.10s

=== episode:243 Env-steps-taken:69312
action_counts: {0: 7018, 1: 4752, 2: 4235, 3: 5038, 4: 6600, 5: 5368, 6: 8602, 7: 4802, 8: 5192}
picked:  81
episode: 243/2000 -> reward: 111.57812500000013, steps:51607, time-taken: 1.68min, time-elasped: 335.64min
-> berries picked: 81 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4573 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [355, 328, 328, 328, 576, 671, 957, 571, 459]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 7, 9, 6, 17, 17, 16, 19, 14]
	Time taken saving stuff: 0.10s

=== episode:244 Env-steps-taken:55776
action_counts: {0: 4334, 1: 2035, 2: 1760, 3: 2079, 4: 2508, 5: 2420, 6: 2288, 7: 2079, 8: 3463}
picked:  31
episode: 244/2000 -> reward: 41.33854166666665, steps:22966, time-taken: 0.87min, time-elasped: 336.51min
-> berries picked: 31 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4574 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [355, 322, 332, 329, 576, 676, 954, 570, 460]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 7, 11, 12, 8, 16, 21, 12, 18]
	Time taken saving stuff: 0.08s

=== episode:245 Env-steps-taken:65184
action_counts: {0: 4609, 1: 3872, 2: 4389, 3: 4389, 4: 4356, 5: 4631, 6: 4015, 7: 3047, 8: 3158}
picked:  65
episode: 245/2000 -> reward: 90.16145833333337, steps:36466, time-taken: 1.24min, time-elasped: 337.75min
-> berries picked: 65 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4571 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [348, 326, 326, 332, 575, 683, 957, 566, 458]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 6, 6, 11, 13, 12, 21, 16, 15]
	Time taken saving stuff: 0.11s

=== episode:246 Env-steps-taken:62592
action_counts: {0: 4389, 1: 3454, 2: 3311, 3: 3300, 4: 2948, 5: 4037, 6: 3586, 7: 3971, 8: 2720}
picked:  48
episode: 246/2000 -> reward: 77.75000000000001, steps:31716, time-taken: 1.15min, time-elasped: 338.90min
-> berries picked: 48 of 800 | patches-visited: [0, 2, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4570 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [342, 328, 329, 332, 572, 684, 960, 562, 461]
	| approx positives in sample 512: 120
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 8, 10, 6, 17, 14, 18, 19, 19]
	Time taken saving stuff: 0.01s

=== episode:247 Env-steps-taken:64608
action_counts: {0: 5104, 1: 3311, 2: 3762, 3: 4114, 4: 3806, 5: 5544, 6: 4939, 7: 4697, 8: 2860}
picked:  55
episode: 247/2000 -> reward: 87.21354166666671, steps:38137, time-taken: 1.25min, time-elasped: 340.16min
-> berries picked: 55 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4570 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [342, 323, 326, 329, 570, 690, 962, 566, 462]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 3, 6, 11, 20, 14, 28, 15, 19]
	Time taken saving stuff: 0.00s

=== episode:248 Env-steps-taken:62784
action_counts: {0: 4620, 1: 4158, 2: 4840, 3: 4312, 4: 4543, 5: 5313, 6: 5676, 7: 3465, 8: 5616}
picked:  55
episode: 248/2000 -> reward: 76.71354166666667, steps:42543, time-taken: 1.37min, time-elasped: 341.53min
-> berries picked: 55 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4573 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [344, 318, 331, 331, 574, 690, 959, 563, 463]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 12, 2, 6, 12, 12, 22, 12, 14]
	Time taken saving stuff: 0.10s

=== episode:249 Env-steps-taken:55104
action_counts: {0: 1529, 1: 737, 2: 991, 3: 1232, 4: 1342, 5: 2387, 6: 1639, 7: 990, 8: 1518}
picked:  23
episode: 249/2000 -> reward: 37.88020833333333, steps:12365, time-taken: 0.62min, time-elasped: 342.15min
-> berries picked: 23 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4571 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [343, 316, 333, 327, 574, 690, 959, 565, 464]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 13, 8, 13, 16, 17, 20, 10, 13]
	Time taken saving stuff: 0.00s

=== episode:250 Env-steps-taken:65664
action_counts: {0: 2695, 1: 2111, 2: 2079, 3: 3080, 4: 2068, 5: 4389, 6: 5709, 7: 4708, 8: 2574}
picked:  58
episode: 250/2000 -> reward: 94.69791666666673, steps:29413, time-taken: 1.10min, time-elasped: 343.26min
-> berries picked: 58 of 800 | patches-visited: [0, 1, 5, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4591 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [347, 313, 332, 323, 574, 695, 974, 567, 466]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 9, 8, 9, 20, 12, 21, 14, 17]
	Time taken saving stuff: 0.16s

=== episode:25 Env-steps-taken:51072
action_counts: {0: 20341, 1: 253, 2: 286, 3: 440, 4: 20812, 5: 264, 6: 308, 7: 594, 8: 649}
picked:  12

==================================================
eval-episode: 250 -> reward: 15.937499999999995, steps: 43947.0, wall-time: 28.09s
-> berries picked: 12 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:251 Env-steps-taken:62592
action_counts: {0: 3806, 1: 3388, 2: 3916, 3: 4246, 4: 6303, 5: 3795, 6: 4301, 7: 3553, 8: 5061}
picked:  57
episode: 251/2000 -> reward: 75.70312500000001, steps:38369, time-taken: 1.25min, time-elasped: 344.98min
-> berries picked: 57 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4567 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [341, 313, 333, 324, 572, 689, 968, 564, 463]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 6, 8, 16, 13, 13, 18, 16, 17]
	Time taken saving stuff: 0.10s

=== episode:252 Env-steps-taken:66240
action_counts: {0: 3201, 1: 3729, 2: 3212, 3: 5269, 4: 3634, 5: 6743, 6: 4345, 7: 3872, 8: 3157}
picked:  63
episode: 252/2000 -> reward: 96.67187500000006, steps:37162, time-taken: 1.27min, time-elasped: 346.26min
-> berries picked: 63 of 800 | patches-visited: [0, 1, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4557 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [337, 311, 334, 330, 573, 682, 967, 559, 464]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 6, 9, 7, 17, 14, 18, 14, 23]
	Time taken saving stuff: 0.02s

=== episode:253 Env-steps-taken:51840
action_counts: {0: 1307, 1: 935, 2: 781, 3: 726, 4: 704, 5: 649, 6: 561, 7: 1408, 8: 946}
picked:  13
episode: 253/2000 -> reward: 19.932291666666668, steps:8017, time-taken: 0.50min, time-elasped: 346.76min
-> berries picked: 13 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4554 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [339, 313, 332, 330, 572, 681, 965, 558, 464]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 7, 9, 8, 13, 12, 22, 10, 16]
	Time taken saving stuff: 0.05s

=== episode:254 Env-steps-taken:79296
action_counts: {0: 8030, 1: 6116, 2: 5544, 3: 6534, 4: 7700, 5: 6941, 6: 8096, 7: 8239, 8: 6282}
picked:  108
episode: 254/2000 -> reward: 164.43749999999991, steps:63482, time-taken: 2.01min, time-elasped: 348.77min
-> berries picked: 108 of 800 | patches-visited: [0, 1, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4545 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [337, 311, 331, 336, 564, 679, 963, 560, 464]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 6, 10, 8, 16, 15, 15, 14, 8]
	Time taken saving stuff: 0.02s

=== episode:255 Env-steps-taken:59232
action_counts: {0: 4466, 1: 2420, 2: 2541, 3: 2915, 4: 4455, 5: 3223, 6: 2717, 7: 3069, 8: 3326}
picked:  39
episode: 255/2000 -> reward: 59.296874999999964, steps:29132, time-taken: 1.07min, time-elasped: 349.85min
-> berries picked: 39 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4544 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [340, 312, 329, 340, 564, 677, 957, 560, 465]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 9, 8, 5, 9, 18, 15, 17, 15]
	Time taken saving stuff: 0.10s

=== episode:256 Env-steps-taken:64704
action_counts: {0: 6996, 1: 4510, 2: 5401, 3: 5687, 4: 6809, 5: 7964, 6: 5764, 7: 4804, 8: 3916}
picked:  58
episode: 256/2000 -> reward: 87.69791666666671, steps:51851, time-taken: 1.66min, time-elasped: 351.51min
-> berries picked: 58 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4501 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [331, 310, 326, 338, 552, 672, 956, 557, 459]
	| approx positives in sample 512: 123
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 9, 8, 6, 17, 13, 28, 16, 19]
	Time taken saving stuff: 0.10s

=== episode:257 Env-steps-taken:61344
action_counts: {0: 3608, 1: 2365, 2: 3135, 3: 3454, 4: 3714, 5: 5302, 6: 4818, 7: 3509, 8: 5016}
picked:  51
episode: 257/2000 -> reward: 70.23437499999997, steps:34921, time-taken: 1.18min, time-elasped: 352.69min
-> berries picked: 51 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4499 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [331, 307, 326, 335, 553, 674, 962, 552, 459]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 10, 7, 10, 11, 17, 17, 10, 12]
	Time taken saving stuff: 0.03s

=== episode:258 Env-steps-taken:61824
action_counts: {0: 2981, 1: 2475, 2: 2123, 3: 2607, 4: 2552, 5: 3806, 6: 5588, 7: 4024, 8: 3993}
picked:  47
episode: 258/2000 -> reward: 74.75520833333333, steps:30149, time-taken: 1.15min, time-elasped: 353.84min
-> berries picked: 47 of 800 | patches-visited: [0, 1, 7, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4504 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [334, 310, 322, 334, 556, 673, 961, 552, 462]
	| approx positives in sample 512: 95
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 3, 8, 10, 10, 16, 11, 15, 16]
	Time taken saving stuff: 0.01s

=== episode:259 Env-steps-taken:65472
action_counts: {0: 5049, 1: 3014, 2: 4103, 3: 4136, 4: 5269, 5: 5302, 6: 4125, 7: 4422, 8: 5380}
picked:  68
episode: 259/2000 -> reward: 91.64583333333339, steps:40800, time-taken: 1.36min, time-elasped: 355.21min
-> berries picked: 68 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4508 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [327, 304, 328, 335, 561, 678, 962, 551, 462]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [20, 7, 11, 8, 16, 17, 17, 10, 15]
	Time taken saving stuff: 0.12s

=== episode:260 Env-steps-taken:64416
action_counts: {0: 2937, 1: 2519, 2: 2563, 3: 4143, 4: 3630, 5: 4455, 6: 3300, 7: 4730, 8: 3179}
picked:  55
episode: 260/2000 -> reward: 86.21354166666671, steps:31456, time-taken: 1.13min, time-elasped: 356.34min
-> berries picked: 55 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4527 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [330, 305, 330, 339, 562, 680, 966, 552, 463]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 8, 10, 6, 18, 18, 15, 10, 11]
	Time taken saving stuff: 0.15s

=== episode:26 Env-steps-taken:80160
action_counts: {0: 4356, 1: 1210, 2: 2156, 3: 3432, 4: 4301, 5: 2563, 6: 5687, 7: 5896, 8: 10339}
picked:  111

==================================================
eval-episode: 260 -> reward: 169.92187499999986, steps: 39940.0, wall-time: 34.68s
-> berries picked: 111 of 800 | patches-visited: [1, 2, 5, 9] | juice left:-0.00
==================================================


=== episode:261 Env-steps-taken:65856
action_counts: {0: 3034, 1: 3531, 2: 3179, 3: 4675, 4: 4202, 5: 5610, 6: 4708, 7: 4169, 8: 5720}
picked:  64
episode: 261/2000 -> reward: 93.66666666666673, steps:38828, time-taken: 1.32min, time-elasped: 358.25min
-> berries picked: 64 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4523 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [329, 306, 331, 342, 567, 682, 958, 547, 461]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 6, 13, 9, 8, 14, 23, 14, 14]
	Time taken saving stuff: 0.11s

=== episode:262 Env-steps-taken:64512
action_counts: {0: 6061, 1: 4714, 2: 4818, 3: 4906, 4: 4697, 5: 5786, 6: 4994, 7: 3003, 8: 5753}
picked:  61
episode: 262/2000 -> reward: 87.68229166666673, steps:44732, time-taken: 1.48min, time-elasped: 359.73min
-> berries picked: 61 of 800 | patches-visited: [0, 4, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4529 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [325, 311, 330, 351, 566, 684, 951, 547, 464]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 6, 12, 6, 12, 17, 22, 13, 16]
	Time taken saving stuff: 0.00s

=== episode:263 Env-steps-taken:75168
action_counts: {0: 3696, 1: 5192, 2: 4840, 3: 6985, 4: 5698, 5: 7084, 6: 7370, 7: 5764, 8: 4852}
picked:  89
episode: 263/2000 -> reward: 141.54687500000006, steps:51481, time-taken: 1.64min, time-elasped: 361.37min
-> berries picked: 89 of 800 | patches-visited: [0, 1, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4537 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [325, 310, 337, 354, 565, 685, 947, 549, 465]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 8, 8, 8, 11, 16, 21, 5, 11]
	Time taken saving stuff: 0.10s

=== episode:264 Env-steps-taken:54528
action_counts: {0: 1166, 1: 1011, 2: 1287, 3: 2156, 4: 3014, 5: 1826, 6: 1562, 7: 1991, 8: 2288}
picked:  25
episode: 264/2000 -> reward: 34.86979166666667, steps:16301, time-taken: 0.72min, time-elasped: 362.09min
-> berries picked: 25 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4545 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [326, 310, 339, 353, 572, 685, 947, 546, 467]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 3, 7, 12, 12, 17, 13, 20, 13]
	Time taken saving stuff: 0.00s

=== episode:265 Env-steps-taken:62880
action_counts: {0: 2530, 1: 2409, 2: 2255, 3: 3729, 4: 2893, 5: 5170, 6: 4323, 7: 3234, 8: 2628}
picked:  51
episode: 265/2000 -> reward: 80.234375, steps:29171, time-taken: 1.11min, time-elasped: 363.21min
-> berries picked: 51 of 800 | patches-visited: [0, 1, 6, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4557 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [330, 311, 338, 355, 569, 689, 948, 546, 471]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 8, 14, 6, 11, 9, 21, 18, 9]
	Time taken saving stuff: 0.10s

=== episode:266 Env-steps-taken:66336
action_counts: {0: 10714, 1: 5258, 2: 5404, 3: 7810, 4: 5302, 5: 9152, 6: 9108, 7: 6875, 8: 6644}
picked:  71
episode: 266/2000 -> reward: 95.13020833333339, steps:66267, time-taken: 2.02min, time-elasped: 365.23min
-> berries picked: 71 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4539 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [329, 312, 337, 353, 563, 686, 949, 540, 470]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 7, 6, 11, 13, 20, 15, 15, 13]
	Time taken saving stuff: 0.10s

=== episode:267 Env-steps-taken:60864
action_counts: {0: 1892, 1: 3531, 2: 2838, 3: 3520, 4: 3586, 5: 4125, 6: 4587, 7: 2849, 8: 2621}
picked:  47
episode: 267/2000 -> reward: 68.7552083333333, steps:29549, time-taken: 1.10min, time-elasped: 366.33min
-> berries picked: 47 of 800 | patches-visited: [0, 4, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4547 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [328, 314, 342, 350, 568, 688, 946, 538, 473]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 12, 7, 7, 8, 18, 16, 8, 17]
	Time taken saving stuff: 0.01s

=== episode:268 Env-steps-taken:62976
action_counts: {0: 5797, 1: 4884, 2: 5863, 3: 7051, 4: 7326, 5: 8822, 6: 8536, 7: 7359, 8: 7326}
picked:  62
episode: 268/2000 -> reward: 77.67708333333333, steps:62964, time-taken: 1.87min, time-elasped: 368.20min
-> berries picked: 62 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4532 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [323, 310, 341, 355, 565, 685, 948, 533, 472]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 11, 12, 9, 7, 14, 21, 16, 14]
	Time taken saving stuff: 0.11s

=== episode:269 Env-steps-taken:58560
action_counts: {0: 2046, 1: 1485, 2: 2530, 3: 2178, 4: 2893, 5: 2277, 6: 2552, 7: 1001, 8: 1468}
picked:  36
episode: 269/2000 -> reward: 55.81249999999997, steps:18430, time-taken: 0.77min, time-elasped: 368.97min
-> berries picked: 36 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4552 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [324, 312, 345, 363, 568, 684, 949, 534, 473]
	| approx positives in sample 512: 89
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 7, 8, 6, 10, 16, 9, 12, 15]
	Time taken saving stuff: 0.00s

=== episode:270 Env-steps-taken:60000
action_counts: {0: 3883, 1: 3944, 2: 4103, 3: 5588, 4: 5357, 5: 6138, 6: 4202, 7: 5302, 8: 5643}
picked:  44
episode: 270/2000 -> reward: 62.2708333333333, steps:44160, time-taken: 1.34min, time-elasped: 370.31min
-> berries picked: 44 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4539 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [327, 310, 342, 358, 573, 681, 944, 532, 472]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 6, 10, 10, 15, 15, 16, 19, 16]
	Time taken saving stuff: 0.15s

=== episode:27 Env-steps-taken:64320
action_counts: {0: 660, 1: 1826, 2: 429, 3: 4411, 4: 1353, 5: 5082, 6: 2387, 7: 792, 8: 3998}
picked:  56

==================================================
eval-episode: 270 -> reward: 87.70833333333337, steps: 20938.0, wall-time: 26.85s
-> berries picked: 56 of 800 | patches-visited: [1, 2, 3, 9] | juice left:-0.00
==================================================


=== episode:271 Env-steps-taken:66624
action_counts: {0: 2541, 1: 3091, 2: 2684, 3: 3817, 4: 5005, 5: 3311, 6: 3366, 7: 2321, 8: 3128}
picked:  61
episode: 271/2000 -> reward: 98.68229166666673, steps:29264, time-taken: 1.08min, time-elasped: 371.84min
-> berries picked: 61 of 800 | patches-visited: [0, 1, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4553 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [329, 314, 349, 359, 580, 687, 939, 526, 470]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 9, 10, 13, 14, 11, 20, 8, 17]
	Time taken saving stuff: 0.02s

=== episode:272 Env-steps-taken:62016
action_counts: {0: 4587, 1: 3553, 2: 4433, 3: 5951, 4: 5841, 5: 6809, 6: 5280, 7: 6501, 8: 5621}
picked:  56
episode: 272/2000 -> reward: 73.70833333333333, steps:48576, time-taken: 1.59min, time-elasped: 373.43min
-> berries picked: 56 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4547 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [331, 307, 356, 361, 584, 679, 933, 528, 468]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 8, 11, 5, 11, 16, 15, 10, 19]
	Time taken saving stuff: 0.00s

=== episode:273 Env-steps-taken:65664
action_counts: {0: 5610, 1: 5291, 2: 5103, 3: 7128, 4: 7238, 5: 8701, 6: 6248, 7: 6380, 8: 3894}
picked:  63
episode: 273/2000 -> reward: 93.67187500000003, steps:55593, time-taken: 1.76min, time-elasped: 375.20min
-> berries picked: 63 of 800 | patches-visited: [0, 5, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4540 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [329, 308, 363, 357, 584, 682, 927, 521, 469]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 9, 7, 10, 9, 22, 15, 12, 13]
	Time taken saving stuff: 0.02s

=== episode:274 Env-steps-taken:63936
action_counts: {0: 5543, 1: 3399, 2: 3762, 3: 4356, 4: 4510, 5: 4488, 6: 4235, 7: 4081, 8: 4147}
picked:  56
episode: 274/2000 -> reward: 84.70833333333339, steps:38521, time-taken: 1.32min, time-elasped: 376.52min
-> berries picked: 56 of 800 | patches-visited: [0, 4, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4558 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [331, 312, 363, 357, 587, 685, 932, 521, 470]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 11, 5, 9, 18, 17, 20, 21, 10]
	Time taken saving stuff: 0.00s

=== episode:275 Env-steps-taken:60096
action_counts: {0: 2277, 1: 2985, 2: 4147, 3: 4147, 4: 3531, 5: 4268, 6: 3454, 7: 2871, 8: 2805}
picked:  43
episode: 275/2000 -> reward: 63.77604166666663, steps:30485, time-taken: 1.14min, time-elasped: 377.67min
-> berries picked: 43 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4564 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [330, 312, 371, 357, 587, 689, 929, 518, 471]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 2, 7, 7, 16, 14, 18, 14, 9]
	Time taken saving stuff: 0.12s

=== episode:276 Env-steps-taken:71616
action_counts: {0: 5093, 1: 4136, 2: 4763, 3: 7370, 4: 5819, 5: 7480, 6: 6259, 7: 6468, 8: 4362}
picked:  83
episode: 276/2000 -> reward: 123.56770833333347, steps:51750, time-taken: 1.64min, time-elasped: 379.31min
-> berries picked: 83 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4575 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [328, 312, 371, 359, 585, 691, 932, 522, 475]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 10, 6, 7, 16, 13, 16, 15, 18]
	Time taken saving stuff: 0.00s

=== episode:277 Env-steps-taken:67488
action_counts: {0: 2937, 1: 2915, 2: 3311, 3: 4939, 4: 4268, 5: 6971, 6: 4444, 7: 5390, 8: 3531}
picked:  66
episode: 277/2000 -> reward: 103.1562500000001, steps:38706, time-taken: 1.32min, time-elasped: 380.63min
-> berries picked: 66 of 800 | patches-visited: [0, 7, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4598 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [328, 315, 369, 359, 595, 698, 936, 522, 476]
	| approx positives in sample 512: 91
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 3, 7, 7, 13, 10, 18, 13, 16]
	Time taken saving stuff: 0.11s

=== episode:278 Env-steps-taken:64704
action_counts: {0: 3128, 1: 3256, 2: 3982, 3: 5764, 4: 4466, 5: 7150, 6: 3542, 7: 3839, 8: 2904}
picked:  59
episode: 278/2000 -> reward: 88.69270833333339, steps:38031, time-taken: 1.35min, time-elasped: 381.98min
-> berries picked: 59 of 800 | patches-visited: [0, 2, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4606 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [325, 313, 373, 366, 597, 701, 933, 521, 477]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 8, 10, 7, 12, 15, 20, 9, 15]
	Time taken saving stuff: 0.00s

=== episode:279 Env-steps-taken:58176
action_counts: {0: 3014, 1: 2838, 2: 2750, 3: 3762, 4: 4741, 5: 5797, 6: 4081, 7: 3124, 8: 4346}
picked:  44
episode: 279/2000 -> reward: 53.77083333333329, steps:34453, time-taken: 1.14min, time-elasped: 383.13min
-> berries picked: 44 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4616 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [325, 316, 375, 364, 601, 699, 936, 522, 478]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 12, 11, 13, 9, 18, 14, 13, 18]
	Time taken saving stuff: 0.11s

=== episode:280 Env-steps-taken:58848
action_counts: {0: 3861, 1: 3828, 2: 4147, 3: 6388, 4: 6314, 5: 10549, 6: 4829, 7: 3795, 8: 7634}
picked:  44
episode: 280/2000 -> reward: 57.27083333333331, steps:51345, time-taken: 1.51min, time-elasped: 384.64min
-> berries picked: 44 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4596 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [321, 318, 372, 360, 602, 697, 927, 523, 476]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 8, 8, 11, 12, 17, 18, 10, 10]
	Time taken saving stuff: 0.06s

=== episode:28 Env-steps-taken:53376
action_counts: {0: 44, 1: 781, 2: 176, 3: 3355, 4: 418, 5: 5412, 6: 1089, 7: 143, 8: 7987}
picked:  22

==================================================
eval-episode: 280 -> reward: 29.885416666666675, steps: 19405.0, wall-time: 20.90s
-> berries picked: 22 of 800 | patches-visited: [1, 4, 7] | juice left:-0.00
==================================================


=== episode:281 Env-steps-taken:58656
action_counts: {0: 1947, 1: 1210, 2: 1584, 3: 2277, 4: 3278, 5: 3828, 6: 2805, 7: 1936, 8: 1560}
picked:  33
episode: 281/2000 -> reward: 56.32812499999997, steps:20425, time-taken: 0.84min, time-elasped: 385.83min
-> berries picked: 33 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4594 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [321, 319, 371, 364, 605, 695, 923, 521, 475]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 8, 13, 6, 15, 16, 9, 8, 15]
	Time taken saving stuff: 0.00s

=== episode:282 Env-steps-taken:56640
action_counts: {0: 2145, 1: 1818, 2: 1958, 3: 2519, 4: 3168, 5: 5841, 6: 4048, 7: 2057, 8: 3608}
picked:  32
episode: 282/2000 -> reward: 46.833333333333314, steps:27162, time-taken: 0.99min, time-elasped: 386.83min
-> berries picked: 32 of 800 | patches-visited: [0, 2, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4595 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [321, 315, 369, 364, 602, 702, 925, 521, 476]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 4, 12, 7, 15, 18, 18, 13, 15]
	Time taken saving stuff: 0.00s

=== episode:283 Env-steps-taken:65952
action_counts: {0: 3674, 1: 3399, 2: 3982, 3: 4026, 4: 4708, 5: 7568, 6: 6226, 7: 3465, 8: 2784}
picked:  58
episode: 283/2000 -> reward: 95.19791666666673, steps:39832, time-taken: 1.34min, time-elasped: 388.17min
-> berries picked: 58 of 800 | patches-visited: [0, 1, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4605 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [319, 309, 370, 369, 599, 704, 929, 525, 481]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 17, 10, 5, 12, 13, 22, 13, 18]
	Time taken saving stuff: 0.11s

=== episode:284 Env-steps-taken:60576
action_counts: {0: 3124, 1: 3608, 2: 2805, 3: 4917, 4: 5439, 5: 7293, 6: 4004, 7: 3729, 8: 3256}
picked:  47
episode: 284/2000 -> reward: 67.25520833333331, steps:38175, time-taken: 1.26min, time-elasped: 389.43min
-> berries picked: 47 of 800 | patches-visited: [0, 5, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4606 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [318, 308, 370, 368, 604, 703, 930, 524, 481]
	| approx positives in sample 512: 93
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 4, 9, 8, 11, 13, 13, 9, 19]
	Time taken saving stuff: 0.00s

=== episode:285 Env-steps-taken:62304
action_counts: {0: 2233, 1: 2992, 2: 2453, 3: 4070, 4: 3828, 5: 4355, 6: 4466, 7: 2695, 8: 3498}
picked:  50
episode: 285/2000 -> reward: 75.23958333333333, steps:30590, time-taken: 1.10min, time-elasped: 390.53min
-> berries picked: 50 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4615 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [320, 306, 374, 365, 606, 702, 930, 530, 482]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 11, 7, 12, 13, 11, 22, 13, 12]
	Time taken saving stuff: 0.02s

=== episode:286 Env-steps-taken:58848
action_counts: {0: 5278, 1: 3344, 2: 3498, 3: 3762, 4: 7755, 5: 6886, 6: 3784, 7: 3630, 8: 3289}
picked:  45
episode: 286/2000 -> reward: 58.26562499999996, steps:41226, time-taken: 1.36min, time-elasped: 391.90min
-> berries picked: 45 of 800 | patches-visited: [0, 3, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4623 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [327, 304, 376, 357, 605, 706, 932, 531, 485]
	| approx positives in sample 512: 125
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 15, 10, 8, 19, 10, 22, 13, 18]
	Time taken saving stuff: 0.00s

=== episode:287 Env-steps-taken:60864
action_counts: {0: 4411, 1: 3828, 2: 3242, 3: 5489, 4: 10021, 5: 5599, 6: 3894, 7: 3949, 8: 4213}
picked:  42
episode: 287/2000 -> reward: 67.78124999999999, steps:44646, time-taken: 1.44min, time-elasped: 393.33min
-> berries picked: 42 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4607 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [324, 306, 374, 353, 603, 700, 931, 531, 485]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 17, 10, 8, 12, 20, 20, 10, 12]
	Time taken saving stuff: 0.01s

=== episode:288 Env-steps-taken:53856
action_counts: {0: 1628, 1: 1771, 2: 1243, 3: 1298, 4: 4213, 5: 2651, 6: 2365, 7: 1529, 8: 2531}
picked:  19
episode: 288/2000 -> reward: 31.40104166666667, steps:19229, time-taken: 0.77min, time-elasped: 394.11min
-> berries picked: 19 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4607 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [323, 306, 377, 349, 602, 707, 930, 527, 486]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 9, 13, 10, 10, 12, 18, 9, 22]
	Time taken saving stuff: 0.00s

=== episode:289 Env-steps-taken:56544
action_counts: {0: 2046, 1: 2002, 2: 2187, 3: 2288, 4: 8052, 5: 2497, 6: 2970, 7: 2673, 8: 3883}
picked:  27
episode: 289/2000 -> reward: 45.359374999999986, steps:28598, time-taken: 1.03min, time-elasped: 395.14min
-> berries picked: 27 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4604 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [323, 305, 377, 346, 602, 706, 931, 529, 485]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 9, 6, 11, 6, 10, 31, 9, 15]
	Time taken saving stuff: 0.10s

=== episode:290 Env-steps-taken:49152
action_counts: {0: 858, 1: 737, 2: 693, 3: 2002, 4: 4202, 5: 1243, 6: 1012, 7: 759, 8: 1024}
picked:  6
episode: 290/2000 -> reward: 6.968750000000001, steps:12530, time-taken: 0.55min, time-elasped: 395.69min
-> berries picked: 6 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4598 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [323, 304, 376, 348, 602, 702, 929, 528, 486]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 6, 10, 12, 14, 16, 15, 14, 15]
	Time taken saving stuff: 0.05s

=== episode:29 Env-steps-taken:55200
action_counts: {0: 253, 1: 264, 2: 913, 3: 209, 4: 10494, 5: 2673, 6: 1397, 7: 374, 8: 3576}
picked:  23

==================================================
eval-episode: 290 -> reward: 38.38020833333333, steps: 20153.0, wall-time: 22.98s
-> berries picked: 23 of 800 | patches-visited: [1, 8] | juice left:-0.00
==================================================


=== episode:291 Env-steps-taken:51744
action_counts: {0: 869, 1: 836, 2: 814, 3: 1265, 4: 3894, 5: 1584, 6: 605, 7: 759, 8: 804}
picked:  12
episode: 291/2000 -> reward: 20.4375, steps:11430, time-taken: 0.58min, time-elasped: 396.66min
-> berries picked: 12 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4604 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [323, 305, 378, 347, 602, 704, 929, 528, 488]
	| approx positives in sample 512: 129
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 5, 14, 15, 20, 17, 21, 11, 23]
	Time taken saving stuff: 0.02s

=== episode:292 Env-steps-taken:57504
action_counts: {0: 2013, 1: 1441, 2: 2123, 3: 3487, 4: 4521, 5: 3905, 6: 3801, 7: 1628, 8: 1683}
picked:  34
episode: 292/2000 -> reward: 51.32291666666664, steps:24602, time-taken: 0.97min, time-elasped: 397.63min
-> berries picked: 34 of 800 | patches-visited: [0, 5, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4613 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [319, 303, 383, 348, 601, 707, 931, 529, 492]
	| approx positives in sample 512: 133
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 13, 14, 13, 16, 16, 17, 25]
	Time taken saving stuff: 0.01s

=== episode:293 Env-steps-taken:52992
action_counts: {0: 924, 1: 748, 2: 1551, 3: 1859, 4: 3905, 5: 1628, 6: 1683, 7: 968, 8: 2036}
picked:  18
episode: 293/2000 -> reward: 26.906250000000004, steps:15302, time-taken: 0.69min, time-elasped: 398.32min
-> berries picked: 18 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4615 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [317, 302, 386, 348, 602, 711, 927, 529, 493]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 9, 8, 9, 20, 14, 17, 15, 18]
	Time taken saving stuff: 0.10s

=== episode:294 Env-steps-taken:65568
action_counts: {0: 3663, 1: 3212, 2: 2915, 3: 3278, 4: 9053, 5: 5456, 6: 5830, 7: 3949, 8: 2494}
picked:  59
episode: 294/2000 -> reward: 93.19270833333339, steps:39850, time-taken: 1.39min, time-elasped: 399.71min
-> berries picked: 59 of 800 | patches-visited: [0, 5, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4631 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [319, 300, 389, 345, 604, 717, 933, 530, 494]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 8, 10, 13, 14, 18, 17, 10, 16]
	Time taken saving stuff: 0.10s

=== episode:295 Env-steps-taken:54336
action_counts: {0: 1430, 1: 1661, 2: 1320, 3: 2299, 4: 3036, 5: 2475, 6: 1947, 7: 1430, 8: 1068}
picked:  22
episode: 295/2000 -> reward: 34.88541666666667, steps:16666, time-taken: 0.72min, time-elasped: 400.43min
-> berries picked: 22 of 800 | patches-visited: [0, 2, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4639 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [318, 302, 388, 348, 606, 716, 933, 532, 496]
	| approx positives in sample 512: 135
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 8, 15, 11, 13, 19, 24, 15, 20]
	Time taken saving stuff: 0.01s

=== episode:296 Env-steps-taken:48768
action_counts: {0: 429, 1: 561, 2: 506, 3: 1342, 4: 1793, 5: 660, 6: 638, 7: 473, 8: 1818}
picked:  3
episode: 296/2000 -> reward: 3.984375, steps:8220, time-taken: 0.47min, time-elasped: 400.90min
-> berries picked: 3 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4631 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [315, 302, 386, 347, 605, 714, 932, 534, 496]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 7, 7, 7, 16, 14, 20, 9, 24]
	Time taken saving stuff: 0.02s

=== episode:297 Env-steps-taken:52896
action_counts: {0: 1749, 1: 1595, 2: 2068, 3: 5324, 4: 3773, 5: 2310, 6: 2090, 7: 2453, 8: 2234}
picked:  18
episode: 297/2000 -> reward: 25.406250000000007, steps:23596, time-taken: 0.85min, time-elasped: 401.75min
-> berries picked: 18 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4605 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [312, 300, 385, 343, 598, 707, 933, 532, 495]
	| approx positives in sample 512: 131
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 4, 7, 7, 16, 21, 17, 25, 24]
	Time taken saving stuff: 0.10s

=== episode:298 Env-steps-taken:67776
action_counts: {0: 4180, 1: 4202, 2: 3124, 3: 4807, 4: 8488, 5: 6677, 6: 6490, 7: 5269, 8: 3058}
picked:  64
episode: 298/2000 -> reward: 104.66666666666674, steps:46295, time-taken: 1.53min, time-elasped: 403.28min
-> berries picked: 64 of 800 | patches-visited: [0, 6, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4610 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [313, 299, 386, 340, 605, 707, 934, 528, 498]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 10, 9, 10, 14, 13, 19, 14, 18]
	Time taken saving stuff: 0.00s

=== episode:299 Env-steps-taken:57312
action_counts: {0: 2178, 1: 1716, 2: 2365, 3: 2574, 4: 4202, 5: 2431, 6: 2057, 7: 2277, 8: 1288}
picked:  30
episode: 299/2000 -> reward: 48.343749999999986, steps:21088, time-taken: 0.84min, time-elasped: 404.12min
-> berries picked: 30 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4611 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [314, 298, 389, 339, 610, 706, 927, 528, 500]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 5, 8, 2, 14, 8, 19, 11, 17]
	Time taken saving stuff: 0.02s

=== episode:300 Env-steps-taken:63840
action_counts: {0: 2915, 1: 2321, 2: 2717, 3: 2706, 4: 3993, 5: 4895, 6: 3520, 7: 3465, 8: 1644}
picked:  54
episode: 300/2000 -> reward: 83.21875000000003, steps:28176, time-taken: 1.04min, time-elasped: 405.17min
-> berries picked: 54 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4631 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [310, 298, 390, 342, 617, 711, 933, 528, 502]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 7, 10, 4, 17, 9, 19, 15, 18]
	Time taken saving stuff: 0.11s

=== episode:30 Env-steps-taken:60768
action_counts: {0: 770, 1: 352, 2: 660, 3: 1661, 4: 8822, 5: 7997, 6: 5633, 7: 671, 8: 1716}
picked:  43

==================================================
eval-episode: 300 -> reward: 68.27604166666664, steps: 28282.0, wall-time: 26.40s
-> berries picked: 43 of 800 | patches-visited: [0, 1, 3] | juice left:-0.00
==================================================


=== episode:301 Env-steps-taken:54144
action_counts: {0: 2288, 1: 3597, 2: 2882, 3: 2288, 4: 5808, 5: 3795, 6: 2618, 7: 1958, 8: 1563}
picked:  21
episode: 301/2000 -> reward: 31.89062500000001, steps:26797, time-taken: 0.98min, time-elasped: 406.59min
-> berries picked: 21 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4621 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [306, 297, 393, 338, 619, 705, 936, 526, 501]
	| approx positives in sample 512: 137
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 9, 15, 11, 16, 19, 19, 14, 22]
	Time taken saving stuff: 0.00s

=== episode:302 Env-steps-taken:58656
action_counts: {0: 2563, 1: 2585, 2: 3212, 3: 2266, 4: 4950, 5: 4202, 6: 2101, 7: 3135, 8: 1596}
picked:  34
episode: 302/2000 -> reward: 56.32291666666664, steps:26610, time-taken: 1.01min, time-elasped: 407.60min
-> berries picked: 34 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4612 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [303, 300, 393, 339, 622, 699, 929, 524, 503]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 12, 11, 6, 16, 15, 18, 5, 13]
	Time taken saving stuff: 0.10s

=== episode:303 Env-steps-taken:61056
action_counts: {0: 3564, 1: 3597, 2: 3498, 3: 5500, 4: 8602, 5: 6006, 6: 4532, 7: 4939, 8: 3940}
picked:  48
episode: 303/2000 -> reward: 68.74999999999997, steps:44178, time-taken: 1.40min, time-elasped: 409.01min
-> berries picked: 48 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4597 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [310, 301, 392, 339, 619, 699, 915, 520, 502]
	| approx positives in sample 512: 125
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 6, 15, 7, 14, 22, 27, 10, 17]
	Time taken saving stuff: 0.00s

=== episode:304 Env-steps-taken:56832
action_counts: {0: 3014, 1: 2646, 2: 3883, 3: 5951, 4: 8459, 5: 6490, 6: 3751, 7: 2805, 8: 2574}
picked:  32
episode: 304/2000 -> reward: 46.83333333333332, steps:39573, time-taken: 1.29min, time-elasped: 410.30min
-> berries picked: 32 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4575 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [309, 300, 394, 332, 621, 691, 908, 517, 503]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 12, 7, 2, 17, 12, 21, 11, 13]
	Time taken saving stuff: 0.10s

=== episode:305 Env-steps-taken:58560
action_counts: {0: 2739, 1: 2299, 2: 3289, 3: 4334, 4: 7062, 5: 4873, 6: 2948, 7: 2937, 8: 2003}
picked:  37
episode: 305/2000 -> reward: 56.807291666666636, steps:32484, time-taken: 1.11min, time-elasped: 411.42min
-> berries picked: 37 of 800 | patches-visited: [0, 1, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4574 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [308, 296, 397, 335, 620, 693, 899, 520, 506]
	| approx positives in sample 512: 120
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 10, 7, 16, 13, 18, 16, 21]
	Time taken saving stuff: 0.00s

=== episode:306 Env-steps-taken:63744
action_counts: {0: 4114, 1: 3091, 2: 3575, 3: 5225, 4: 6006, 5: 8481, 6: 5369, 7: 5324, 8: 3872}
picked:  56
episode: 306/2000 -> reward: 83.70833333333333, steps:45057, time-taken: 1.38min, time-elasped: 412.80min
-> berries picked: 56 of 800 | patches-visited: [0, 1, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4503 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [305, 292, 380, 331, 609, 680, 880, 519, 507]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 10, 8, 5, 9, 17, 15, 10, 19]
	Time taken saving stuff: 0.10s

=== episode:307 Env-steps-taken:60192
action_counts: {0: 1914, 1: 2222, 2: 2772, 3: 3366, 4: 4037, 5: 4950, 6: 3432, 7: 2915, 8: 1827}
picked:  44
episode: 307/2000 -> reward: 65.2708333333333, steps:27435, time-taken: 1.00min, time-elasped: 413.80min
-> berries picked: 44 of 800 | patches-visited: [0, 2, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4497 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [303, 292, 379, 332, 611, 679, 881, 513, 507]
	| approx positives in sample 512: 95
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 4, 9, 2, 14, 15, 19, 10, 17]
	Time taken saving stuff: 0.00s

=== episode:308 Env-steps-taken:54144
action_counts: {0: 1034, 1: 1265, 2: 1463, 3: 1650, 4: 4290, 5: 2574, 6: 1639, 7: 1320, 8: 1332}
picked:  20
episode: 308/2000 -> reward: 32.89583333333334, steps:16567, time-taken: 0.71min, time-elasped: 414.52min
-> berries picked: 20 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4493 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [302, 291, 380, 332, 608, 681, 880, 512, 507]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 5, 8, 2, 15, 15, 15, 9, 18]
	Time taken saving stuff: 0.11s

=== episode:309 Env-steps-taken:53664
action_counts: {0: 693, 1: 935, 2: 792, 3: 1628, 4: 2552, 5: 1672, 6: 1199, 7: 748, 8: 617}
picked:  19
episode: 309/2000 -> reward: 30.401041666666675, steps:10836, time-taken: 0.57min, time-elasped: 415.09min
-> berries picked: 19 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4499 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [302, 295, 380, 332, 610, 678, 879, 515, 508]
	| approx positives in sample 512: 97
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 8, 4, 9, 10, 22, 15, 12, 12]
	Time taken saving stuff: 0.00s

=== episode:310 Env-steps-taken:58368
action_counts: {0: 1936, 1: 2101, 2: 2475, 3: 4202, 4: 5002, 5: 5291, 6: 2706, 7: 2992, 8: 1573}
picked:  40
episode: 310/2000 -> reward: 54.791666666666636, steps:28278, time-taken: 1.04min, time-elasped: 416.13min
-> berries picked: 40 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4502 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [299, 294, 381, 334, 613, 684, 874, 515, 508]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 3, 10, 5, 12, 14, 16, 16, 18]
	Time taken saving stuff: 0.06s

=== episode:31 Env-steps-taken:58656
action_counts: {0: 1683, 1: 253, 2: 932, 3: 1287, 4: 15015, 5: 4620, 6: 2200, 7: 858, 8: 4103}
picked:  37

==================================================
eval-episode: 310 -> reward: 56.30729166666664, steps: 30951.0, wall-time: 29.63s
-> berries picked: 37 of 800 | patches-visited: [1, 2] | juice left:-0.00
==================================================


=== episode:311 Env-steps-taken:59040
action_counts: {0: 4169, 1: 3399, 2: 2684, 3: 4191, 4: 4510, 5: 6743, 6: 4092, 7: 1980, 8: 3169}
picked:  37
episode: 311/2000 -> reward: 59.30729166666664, steps:34937, time-taken: 1.21min, time-elasped: 417.84min
-> berries picked: 37 of 800 | patches-visited: [0, 4, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4497 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [300, 295, 383, 335, 613, 686, 871, 504, 510]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 7, 6, 8, 14, 17, 23, 11, 12]
	Time taken saving stuff: 0.11s

=== episode:312 Env-steps-taken:61152
action_counts: {0: 4114, 1: 4411, 2: 4334, 3: 4877, 4: 4939, 5: 5687, 6: 4444, 7: 3465, 8: 3025}
picked:  47
episode: 312/2000 -> reward: 70.2552083333333, steps:39296, time-taken: 1.32min, time-elasped: 419.16min
-> berries picked: 47 of 800 | patches-visited: [0, 1, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4500 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [302, 297, 386, 328, 620, 683, 870, 500, 514]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 11, 9, 11, 15, 12, 15, 14, 11]
	Time taken saving stuff: 0.00s

=== episode:313 Env-steps-taken:66720
action_counts: {0: 5698, 1: 4180, 2: 4136, 3: 4312, 4: 9504, 5: 5742, 6: 5225, 7: 4785, 8: 3796}
picked:  63
episode: 313/2000 -> reward: 99.17187500000007, steps:47378, time-taken: 1.53min, time-elasped: 420.69min
-> berries picked: 63 of 800 | patches-visited: [0, 1, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4505 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [305, 297, 381, 332, 619, 682, 871, 499, 519]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 10, 11, 9, 10, 15, 20, 8, 19]
	Time taken saving stuff: 0.00s

=== episode:314 Env-steps-taken:57600
action_counts: {0: 2739, 1: 1991, 2: 1859, 3: 2321, 4: 4158, 5: 2750, 6: 3256, 7: 2123, 8: 1673}
picked:  34
episode: 314/2000 -> reward: 50.82291666666664, steps:22870, time-taken: 0.87min, time-elasped: 421.56min
-> berries picked: 34 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4505 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [314, 298, 380, 332, 618, 680, 863, 500, 520]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 11, 6, 4, 13, 17, 14, 9, 22]
	Time taken saving stuff: 0.09s

=== episode:315 Env-steps-taken:60672
action_counts: {0: 4191, 1: 2959, 2: 3421, 3: 4906, 4: 5841, 5: 8788, 6: 5456, 7: 4488, 8: 3157}
picked:  49
episode: 315/2000 -> reward: 66.74479166666664, steps:43207, time-taken: 1.40min, time-elasped: 422.96min
-> berries picked: 49 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4496 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [316, 299, 375, 329, 616, 681, 863, 498, 519]
	| approx positives in sample 512: 97
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 9, 2, 13, 12, 11, 20, 8, 16]
	Time taken saving stuff: 0.02s

=== episode:316 Env-steps-taken:62016
action_counts: {0: 4609, 1: 3718, 2: 4125, 3: 4257, 4: 4235, 5: 5495, 6: 4631, 7: 3960, 8: 5258}
picked:  48
episode: 316/2000 -> reward: 74.74999999999999, steps:40288, time-taken: 1.38min, time-elasped: 424.34min
-> berries picked: 48 of 800 | patches-visited: [0, 2, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4488 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [308, 300, 376, 332, 611, 677, 860, 501, 523]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 13, 6, 14, 12, 17, 11, 20, 12]
	Time taken saving stuff: 0.08s

=== episode:317 Env-steps-taken:53664
action_counts: {0: 1100, 1: 1078, 2: 704, 3: 968, 4: 1111, 5: 2618, 6: 2450, 7: 1496, 8: 1210}
picked:  17
episode: 317/2000 -> reward: 30.411458333333343, steps:12735, time-taken: 0.62min, time-elasped: 424.97min
-> berries picked: 17 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4493 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [308, 301, 373, 333, 613, 675, 863, 502, 525]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 12, 8, 11, 9, 8, 16, 10, 18]
	Time taken saving stuff: 0.11s

=== episode:318 Env-steps-taken:54240
action_counts: {0: 3388, 1: 3960, 2: 3036, 3: 4631, 4: 4345, 5: 4809, 6: 3080, 7: 3839, 8: 3135}
picked:  23
episode: 318/2000 -> reward: 32.38020833333334, steps:34223, time-taken: 1.14min, time-elasped: 426.11min
-> berries picked: 23 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4467 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [306, 296, 372, 325, 612, 664, 865, 502, 525]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 6, 8, 6, 11, 19, 25, 13, 21]
	Time taken saving stuff: 0.10s

=== episode:319 Env-steps-taken:55968
action_counts: {0: 3124, 1: 3344, 2: 2750, 3: 3113, 4: 5412, 5: 7337, 6: 3344, 7: 2838, 8: 4313}
picked:  33
episode: 319/2000 -> reward: 41.32812499999999, steps:35575, time-taken: 1.17min, time-elasped: 427.29min
-> berries picked: 33 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4463 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [306, 294, 373, 327, 611, 661, 861, 501, 529]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 5, 10, 15, 15, 23, 9, 21]
	Time taken saving stuff: 0.10s

=== episode:320 Env-steps-taken:56160
action_counts: {0: 2354, 1: 4092, 2: 2706, 3: 2838, 4: 3971, 5: 8382, 6: 3410, 7: 3806, 8: 2498}
picked:  29
episode: 320/2000 -> reward: 44.34895833333333, steps:34057, time-taken: 1.15min, time-elasped: 428.44min
-> berries picked: 29 of 800 | patches-visited: [0, 2, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4448 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [300, 293, 373, 324, 604, 664, 860, 499, 531]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 6, 10, 10, 12, 13, 17, 12, 18]
	Time taken saving stuff: 0.05s

=== episode:32 Env-steps-taken:58752
action_counts: {0: 1518, 1: 11000, 2: 3663, 3: 1705, 4: 3212, 5: 12338, 6: 5489, 7: 704, 8: 2739}
picked:  37

==================================================
eval-episode: 320 -> reward: 56.807291666666636, steps: 42368.0, wall-time: 27.45s
-> berries picked: 37 of 800 | patches-visited: [1, 8] | juice left:-0.00
==================================================


=== episode:321 Env-steps-taken:58752
action_counts: {0: 2486, 1: 3311, 2: 4356, 3: 3289, 4: 5940, 5: 7139, 6: 3388, 7: 3377, 8: 2388}
picked:  36
episode: 321/2000 -> reward: 56.81249999999997, steps:35674, time-taken: 1.21min, time-elasped: 430.11min
-> berries picked: 36 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4447 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [297, 293, 375, 328, 605, 665, 859, 494, 531]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 10, 14, 9, 12, 15, 13, 14, 17]
	Time taken saving stuff: 0.00s

=== episode:322 Env-steps-taken:60096
action_counts: {0: 2607, 1: 2728, 2: 2552, 3: 2772, 4: 5236, 5: 4169, 6: 3091, 7: 2739, 8: 1925}
picked:  42
episode: 322/2000 -> reward: 63.78124999999996, steps:27819, time-taken: 1.03min, time-elasped: 431.14min
-> berries picked: 42 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4459 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [296, 293, 378, 333, 607, 668, 857, 494, 533]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 10, 11, 10, 10, 17, 25, 10, 17]
	Time taken saving stuff: 0.00s

=== episode:323 Env-steps-taken:64224
action_counts: {0: 3190, 1: 2959, 2: 4070, 3: 4400, 4: 6721, 5: 7227, 6: 5335, 7: 5523, 8: 4037}
picked:  58
episode: 323/2000 -> reward: 87.19791666666669, steps:43462, time-taken: 1.39min, time-elasped: 432.53min
-> berries picked: 58 of 800 | patches-visited: [0, 3, 5, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4465 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [295, 294, 381, 337, 608, 674, 849, 490, 537]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 3, 12, 8, 9, 20, 10, 9, 20]
	Time taken saving stuff: 0.10s

=== episode:324 Env-steps-taken:58368
action_counts: {0: 1848, 1: 2277, 2: 2277, 3: 2189, 4: 5357, 5: 4665, 6: 2332, 7: 3003, 8: 3124}
picked:  33
episode: 324/2000 -> reward: 54.82812499999998, steps:27072, time-taken: 1.02min, time-elasped: 433.55min
-> berries picked: 33 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4465 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [293, 295, 383, 335, 604, 677, 851, 488, 539]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 9, 11, 8, 9, 13, 19, 13, 21]
	Time taken saving stuff: 0.12s

=== episode:325 Env-steps-taken:56256
action_counts: {0: 2893, 1: 3223, 2: 2585, 3: 2475, 4: 3014, 5: 5819, 6: 2871, 7: 2563, 8: 2256}
picked:  27
episode: 325/2000 -> reward: 43.85937499999999, steps:27699, time-taken: 1.06min, time-elasped: 434.62min
-> berries picked: 27 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4465 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [294, 294, 385, 331, 606, 672, 852, 490, 541]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 7, 6, 13, 14, 12, 12, 10, 19]
	Time taken saving stuff: 0.02s

=== episode:326 Env-steps-taken:51936
action_counts: {0: 737, 1: 902, 2: 1100, 3: 1496, 4: 1804, 5: 2563, 6: 858, 7: 748, 8: 969}
picked:  13
episode: 326/2000 -> reward: 21.432291666666668, steps:11177, time-taken: 0.59min, time-elasped: 435.21min
-> berries picked: 13 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4464 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [295, 296, 388, 331, 605, 672, 849, 486, 542]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 6, 9, 7, 12, 17, 19, 11, 20]
	Time taken saving stuff: 0.01s

=== episode:327 Env-steps-taken:49824
action_counts: {0: 1177, 1: 1001, 2: 624, 3: 561, 4: 1034, 5: 1232, 6: 902, 7: 825, 8: 1837}
picked:  6
episode: 327/2000 -> reward: 10.46875, steps:9193, time-taken: 0.52min, time-elasped: 435.73min
-> berries picked: 6 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4455 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [294, 297, 385, 330, 604, 671, 846, 485, 543]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 7, 5, 8, 20, 18, 20, 11, 13]
	Time taken saving stuff: 0.10s

=== episode:328 Env-steps-taken:53664
action_counts: {0: 2090, 1: 2101, 2: 1903, 3: 1892, 4: 4466, 5: 5313, 6: 1331, 7: 1419, 8: 2883}
picked:  19
episode: 328/2000 -> reward: 30.401041666666675, steps:23398, time-taken: 0.89min, time-elasped: 436.62min
-> berries picked: 19 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4458 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [292, 301, 388, 327, 608, 669, 846, 483, 544]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 11, 6, 5, 11, 10, 22, 7, 25]
	Time taken saving stuff: 0.10s

=== episode:329 Env-steps-taken:57504
action_counts: {0: 1595, 1: 1199, 2: 1397, 3: 1221, 4: 1870, 5: 2937, 6: 1980, 7: 1837, 8: 1905}
picked:  30
episode: 329/2000 -> reward: 50.34375, steps:15941, time-taken: 0.74min, time-elasped: 437.37min
-> berries picked: 30 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4475 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [293, 302, 387, 327, 609, 675, 847, 487, 548]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 7, 8, 9, 18, 12, 12, 7, 24]
	Time taken saving stuff: 0.01s

=== episode:330 Env-steps-taken:65952
action_counts: {0: 3608, 1: 4499, 2: 3905, 3: 3971, 4: 5819, 5: 8063, 6: 4000, 7: 3729, 8: 3498}
picked:  58
episode: 330/2000 -> reward: 95.19791666666671, steps:41092, time-taken: 1.41min, time-elasped: 438.78min
-> berries picked: 58 of 800 | patches-visited: [0, 5, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4476 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [289, 296, 390, 330, 613, 681, 846, 482, 549]
	| approx positives in sample 512: 136
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 10, 17, 10, 15, 16, 17, 12, 26]
	Time taken saving stuff: 0.06s

=== episode:33 Env-steps-taken:57024
action_counts: {0: 319, 1: 649, 2: 15246, 3: 473, 4: 2926, 5: 3575, 6: 19327, 7: 759, 8: 6788}
picked:  33

==================================================
eval-episode: 330 -> reward: 46.828124999999986, steps: 50062.0, wall-time: 32.02s
-> berries picked: 33 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:331 Env-steps-taken:48768
action_counts: {0: 154, 1: 143, 2: 176, 3: 66, 4: 121, 5: 143, 6: 275, 7: 242, 8: 397}
picked:  3
episode: 331/2000 -> reward: 4.984375, steps:1717, time-taken: 0.29min, time-elasped: 439.60min
-> berries picked: 3 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4476 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [290, 296, 390, 330, 612, 680, 846, 482, 550]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 5, 11, 8, 9, 16, 10, 15, 19]
	Time taken saving stuff: 0.00s

=== episode:332 Env-steps-taken:56160
action_counts: {0: 4092, 1: 4631, 2: 3740, 3: 4851, 4: 4785, 5: 13062, 6: 4829, 7: 5764, 8: 5819}
picked:  31
episode: 332/2000 -> reward: 42.338541666666664, steps:51573, time-taken: 1.53min, time-elasped: 441.14min
-> berries picked: 31 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4430 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [288, 288, 383, 324, 604, 681, 838, 476, 548]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 13, 7, 7, 15, 12, 19, 12, 19]
	Time taken saving stuff: 0.01s

=== episode:333 Env-steps-taken:63936
action_counts: {0: 2343, 1: 2156, 2: 3102, 3: 4983, 4: 3850, 5: 4345, 6: 3300, 7: 2882, 8: 2395}
picked:  51
episode: 333/2000 -> reward: 84.73437500000003, steps:29356, time-taken: 1.09min, time-elasped: 442.23min
-> berries picked: 51 of 800 | patches-visited: [0, 2, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4435 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [286, 285, 383, 335, 606, 683, 836, 472, 549]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 6, 12, 9, 9, 18, 7, 15, 19]
	Time taken saving stuff: 0.00s

=== episode:334 Env-steps-taken:62880
action_counts: {0: 3091, 1: 3905, 2: 2420, 3: 3520, 4: 3652, 5: 6578, 6: 4928, 7: 3487, 8: 2839}
picked:  50
episode: 334/2000 -> reward: 78.23958333333333, steps:34420, time-taken: 1.23min, time-elasped: 443.47min
-> berries picked: 50 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4435 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [285, 286, 381, 338, 611, 684, 831, 471, 548]
	| approx positives in sample 512: 99
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 13, 5, 7, 9, 11, 16, 10, 20]
	Time taken saving stuff: 0.09s

=== episode:335 Env-steps-taken:58368
action_counts: {0: 1672, 1: 1826, 2: 1452, 3: 2508, 4: 3212, 5: 4037, 6: 3322, 7: 2750, 8: 2410}
picked:  42
episode: 335/2000 -> reward: 54.781249999999964, steps:23189, time-taken: 0.94min, time-elasped: 444.42min
-> berries picked: 42 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4449 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [283, 287, 382, 338, 613, 687, 835, 472, 552]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 6, 7, 10, 19, 10, 17, 14, 23]
	Time taken saving stuff: 0.00s

=== episode:336 Env-steps-taken:55296
action_counts: {0: 1298, 1: 1100, 2: 1749, 3: 1650, 4: 3377, 5: 2926, 6: 1287, 7: 1331, 8: 1200}
picked:  23
episode: 336/2000 -> reward: 39.88020833333333, steps:15918, time-taken: 0.69min, time-elasped: 445.10min
-> berries picked: 23 of 800 | patches-visited: [0, 1, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4458 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [280, 288, 384, 342, 615, 690, 834, 471, 554]
	| approx positives in sample 512: 129
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 10, 17, 7, 17, 12, 24, 13, 23]
	Time taken saving stuff: 0.10s

=== episode:337 Env-steps-taken:57312
action_counts: {0: 1958, 1: 1496, 2: 1595, 3: 2112, 4: 2068, 5: 3542, 6: 3487, 7: 2706, 8: 1714}
picked:  34
episode: 337/2000 -> reward: 50.32291666666664, steps:20678, time-taken: 0.84min, time-elasped: 445.95min
-> berries picked: 34 of 800 | patches-visited: [0, 5, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4480 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [282, 290, 389, 341, 616, 692, 837, 477, 556]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 13, 9, 5, 15, 16, 20, 11, 16]
	Time taken saving stuff: 0.00s

=== episode:338 Env-steps-taken:48960
action_counts: {0: 143, 1: 143, 2: 242, 3: 517, 4: 231, 5: 132, 6: 121, 7: 143, 8: 320}
picked:  3
episode: 338/2000 -> reward: 5.984375, steps:1992, time-taken: 0.32min, time-elasped: 446.27min
-> berries picked: 3 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4483 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [282, 291, 390, 340, 617, 692, 837, 477, 557]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 4, 6, 8, 21, 11, 22, 10, 19]
	Time taken saving stuff: 0.00s

=== episode:339 Env-steps-taken:62496
action_counts: {0: 2641, 1: 2695, 2: 2233, 3: 3234, 4: 5049, 5: 8063, 6: 5148, 7: 3619, 8: 4004}
picked:  45
episode: 339/2000 -> reward: 77.265625, steps:36686, time-taken: 1.20min, time-elasped: 447.47min
-> berries picked: 45 of 800 | patches-visited: [0, 1, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4491 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [283, 289, 387, 337, 620, 698, 842, 477, 558]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 5, 10, 3, 14, 14, 23, 13, 26]
	Time taken saving stuff: 0.06s

=== episode:340 Env-steps-taken:60000
action_counts: {0: 1584, 1: 2486, 2: 2101, 3: 3366, 4: 4499, 5: 4202, 6: 3003, 7: 2838, 8: 1838}
picked:  41
episode: 340/2000 -> reward: 65.2864583333333, steps:25917, time-taken: 1.00min, time-elasped: 448.47min
-> berries picked: 41 of 800 | patches-visited: [0, 1, 3, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4513 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [284, 291, 387, 340, 630, 702, 844, 475, 560]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 6, 5, 6, 16, 16, 16, 10, 23]
	Time taken saving stuff: 0.05s

=== episode:34 Env-steps-taken:61440
action_counts: {0: 429, 1: 11, 2: 121, 3: 517, 4: 4565, 5: 12727, 6: 5390, 7: 2599, 8: 1804}
picked:  47

==================================================
eval-episode: 340 -> reward: 71.75520833333331, steps: 28163.0, wall-time: 24.55s
-> berries picked: 47 of 800 | patches-visited: [1, 5, 9] | juice left:-0.00
==================================================


=== episode:341 Env-steps-taken:62304
action_counts: {0: 2926, 1: 2420, 2: 3036, 3: 4411, 4: 3025, 5: 6325, 6: 4433, 7: 2794, 8: 2982}
picked:  51
episode: 341/2000 -> reward: 75.234375, steps:32352, time-taken: 1.16min, time-elasped: 450.04min
-> berries picked: 51 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4524 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [280, 290, 388, 347, 634, 703, 847, 473, 562]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 5, 16, 10, 14, 15, 16, 7, 17]
	Time taken saving stuff: 0.02s

=== episode:342 Env-steps-taken:55296
action_counts: {0: 1397, 1: 1716, 2: 847, 3: 2497, 4: 2420, 5: 2112, 6: 1936, 7: 1716, 8: 1354}
picked:  27
episode: 342/2000 -> reward: 39.85937499999999, steps:15995, time-taken: 0.71min, time-elasped: 450.75min
-> berries picked: 27 of 800 | patches-visited: [0, 5, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4533 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [279, 291, 388, 352, 635, 704, 848, 471, 565]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 2, 11, 7, 16, 22, 21, 6, 15]
	Time taken saving stuff: 0.10s

=== episode:343 Env-steps-taken:53856
action_counts: {0: 2376, 1: 2145, 2: 2189, 3: 2948, 4: 5522, 5: 8580, 6: 4873, 7: 2464, 8: 5457}
picked:  23
episode: 343/2000 -> reward: 31.38020833333334, steps:36554, time-taken: 1.20min, time-elasped: 451.96min
-> berries picked: 23 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4512 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [277, 289, 386, 348, 633, 705, 842, 468, 564]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 3, 6, 7, 15, 19, 20, 19, 17]
	Time taken saving stuff: 0.02s

=== episode:344 Env-steps-taken:65664
action_counts: {0: 4048, 1: 3773, 2: 3685, 3: 5203, 4: 4466, 5: 5412, 6: 5186, 7: 4015, 8: 3421}
picked:  63
episode: 344/2000 -> reward: 92.67187500000006, steps:39209, time-taken: 1.34min, time-elasped: 453.30min
-> berries picked: 63 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4535 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [277, 296, 388, 352, 638, 703, 844, 471, 566]
	| approx positives in sample 512: 106
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 3, 11, 9, 12, 18, 14, 7, 23]
	Time taken saving stuff: 0.02s

=== episode:345 Env-steps-taken:57504
action_counts: {0: 2508, 1: 2211, 2: 2409, 3: 3905, 4: 5038, 5: 8349, 6: 4257, 7: 2871, 8: 4434}
picked:  35
episode: 345/2000 -> reward: 50.31770833333331, steps:35982, time-taken: 1.25min, time-elasped: 454.55min
-> berries picked: 35 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4540 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [274, 296, 388, 356, 643, 703, 843, 470, 567]
	| approx positives in sample 512: 124
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 7, 12, 6, 18, 17, 22, 12, 23]
	Time taken saving stuff: 0.11s

=== episode:346 Env-steps-taken:55200
action_counts: {0: 2387, 1: 1727, 2: 1837, 3: 1771, 4: 3135, 5: 4114, 6: 2552, 7: 2871, 8: 2817}
picked:  26
episode: 346/2000 -> reward: 38.36458333333333, steps:23211, time-taken: 0.87min, time-elasped: 455.42min
-> berries picked: 26 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4541 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [272, 295, 391, 356, 645, 705, 843, 466, 568]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 3, 7, 13, 16, 16, 19, 13, 20]
	Time taken saving stuff: 0.12s

=== episode:347 Env-steps-taken:55104
action_counts: {0: 2838, 1: 2442, 2: 3090, 3: 2970, 4: 2585, 5: 4620, 6: 2035, 7: 2717, 8: 2508}
picked:  27
episode: 347/2000 -> reward: 37.859375, steps:25805, time-taken: 0.95min, time-elasped: 456.37min
-> berries picked: 27 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4540 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [270, 295, 390, 362, 644, 708, 842, 461, 568]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 11, 17, 4, 15, 22, 15, 12, 21]
	Time taken saving stuff: 0.10s

=== episode:348 Env-steps-taken:60864
action_counts: {0: 2123, 1: 1936, 2: 2266, 3: 3245, 4: 3872, 5: 6479, 6: 2321, 7: 4488, 8: 1849}
picked:  44
episode: 348/2000 -> reward: 67.7708333333333, steps:28579, time-taken: 1.02min, time-elasped: 457.39min
-> berries picked: 44 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4539 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [270, 297, 390, 356, 648, 715, 838, 458, 567]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 3, 7, 6, 16, 18, 17, 8, 23]
	Time taken saving stuff: 0.11s

=== episode:349 Env-steps-taken:59808
action_counts: {0: 2431, 1: 4444, 2: 2805, 3: 4697, 4: 5676, 5: 6226, 6: 3715, 7: 2662, 8: 3520}
picked:  42
episode: 349/2000 -> reward: 62.28124999999996, steps:36176, time-taken: 1.22min, time-elasped: 458.61min
-> berries picked: 42 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4536 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [270, 296, 388, 357, 652, 715, 835, 454, 569]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 8, 9, 12, 11, 15, 26, 10, 18]
	Time taken saving stuff: 0.11s

=== episode:350 Env-steps-taken:59808
action_counts: {0: 1826, 1: 1936, 2: 2409, 3: 3091, 4: 3553, 5: 4125, 6: 3289, 7: 2057, 8: 2537}
picked:  40
episode: 350/2000 -> reward: 62.29166666666663, steps:24823, time-taken: 0.92min, time-elasped: 459.54min
-> berries picked: 40 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4554 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [271, 298, 392, 357, 659, 721, 835, 451, 570]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 7, 6, 4, 8, 12, 19, 12, 27]
	Time taken saving stuff: 0.06s

=== episode:35 Env-steps-taken:57312
action_counts: {0: 528, 1: 22330, 2: 836, 3: 1441, 4: 2046, 5: 22582, 6: 1463, 7: 231, 8: 363}
picked:  29

==================================================
eval-episode: 350 -> reward: 49.348958333333314, steps: 51820.0, wall-time: 26.95s
-> berries picked: 29 of 800 | patches-visited: [1, 3] | juice left:-0.00
==================================================


=== episode:351 Env-steps-taken:56928
action_counts: {0: 2475, 1: 1650, 2: 1782, 3: 2167, 4: 3916, 5: 5434, 6: 5335, 7: 2035, 8: 2660}
picked:  30
episode: 351/2000 -> reward: 47.343749999999986, steps:27454, time-taken: 1.02min, time-elasped: 461.01min
-> berries picked: 30 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4553 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [267, 294, 383, 358, 657, 726, 842, 454, 572]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 6, 5, 5, 13, 17, 21, 9, 20]
	Time taken saving stuff: 0.01s

=== episode:352 Env-steps-taken:52416
action_counts: {0: 726, 1: 561, 2: 1056, 3: 1298, 4: 1903, 5: 1969, 6: 1617, 7: 627, 8: 1035}
picked:  17
episode: 352/2000 -> reward: 23.91145833333334, steps:10792, time-taken: 0.55min, time-elasped: 461.57min
-> berries picked: 17 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4561 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [266, 291, 383, 362, 659, 731, 842, 454, 573]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 7, 8, 4, 11, 19, 19, 9, 23]
	Time taken saving stuff: 0.02s

=== episode:353 Env-steps-taken:54624
action_counts: {0: 1430, 1: 1694, 2: 1331, 3: 2057, 4: 2629, 5: 6149, 6: 3300, 7: 2769, 8: 2585}
picked:  22
episode: 353/2000 -> reward: 36.38541666666667, steps:23944, time-taken: 0.94min, time-elasped: 462.51min
-> berries picked: 22 of 800 | patches-visited: [0, 3, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4551 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [265, 287, 379, 364, 656, 730, 846, 450, 574]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 9, 12, 9, 6, 9, 13, 10, 21]
	Time taken saving stuff: 0.11s

=== episode:354 Env-steps-taken:59904
action_counts: {0: 1386, 1: 1254, 2: 1815, 3: 2266, 4: 1958, 5: 5214, 6: 2464, 7: 1859, 8: 1810}
picked:  40
episode: 354/2000 -> reward: 64.79166666666663, steps:20026, time-taken: 0.85min, time-elasped: 463.36min
-> berries picked: 40 of 800 | patches-visited: [0, 1, 3, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4566 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [261, 282, 381, 367, 661, 736, 848, 452, 578]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 3, 10, 11, 10, 18, 16, 12, 22]
	Time taken saving stuff: 0.00s

=== episode:355 Env-steps-taken:58944
action_counts: {0: 1837, 1: 3058, 2: 2673, 3: 1947, 4: 3102, 5: 2233, 6: 2937, 7: 2222, 8: 1773}
picked:  35
episode: 355/2000 -> reward: 57.81770833333331, steps:21782, time-taken: 0.88min, time-elasped: 464.24min
-> berries picked: 35 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4575 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [260, 286, 384, 365, 660, 738, 849, 453, 580]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 8, 9, 9, 12, 13, 13, 10, 18]
	Time taken saving stuff: 0.09s

=== episode:356 Env-steps-taken:57888
action_counts: {0: 2728, 1: 2959, 2: 1925, 3: 2959, 4: 2978, 5: 4807, 6: 3267, 7: 2695, 8: 3036}
picked:  37
episode: 356/2000 -> reward: 53.30729166666664, steps:27354, time-taken: 0.96min, time-elasped: 465.21min
-> berries picked: 37 of 800 | patches-visited: [0, 1, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4585 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [262, 285, 388, 364, 661, 745, 845, 453, 582]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 8, 11, 12, 17, 15, 16, 10, 18]
	Time taken saving stuff: 0.00s

=== episode:357 Env-steps-taken:58560
action_counts: {0: 2805, 1: 2409, 2: 2035, 3: 2442, 4: 2145, 5: 5093, 6: 4543, 7: 2354, 8: 2817}
picked:  36
episode: 357/2000 -> reward: 54.81249999999998, steps:26643, time-taken: 0.98min, time-elasped: 466.19min
-> berries picked: 36 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4599 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [258, 286, 384, 364, 666, 750, 851, 458, 582]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 7, 7, 10, 14, 15, 17, 15, 28]
	Time taken saving stuff: 0.11s

=== episode:358 Env-steps-taken:54912
action_counts: {0: 2453, 1: 2794, 2: 2530, 3: 3256, 4: 2552, 5: 5742, 6: 3597, 7: 5895, 8: 2090}
picked:  27
episode: 358/2000 -> reward: 36.859375, steps:30909, time-taken: 1.05min, time-elasped: 467.24min
-> berries picked: 27 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4586 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [256, 285, 379, 357, 670, 755, 849, 454, 581]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 8, 8, 4, 11, 15, 27, 9, 17]
	Time taken saving stuff: 0.02s

=== episode:359 Env-steps-taken:61536
action_counts: {0: 2354, 1: 2519, 2: 3718, 3: 3465, 4: 4433, 5: 4840, 6: 4301, 7: 3344, 8: 3795}
picked:  45
episode: 359/2000 -> reward: 72.26562499999999, steps:32769, time-taken: 1.19min, time-elasped: 468.44min
-> berries picked: 45 of 800 | patches-visited: [0, 5, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4602 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [256, 288, 381, 351, 675, 758, 849, 458, 586]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 9, 12, 9, 11, 16, 18, 5, 23]
	Time taken saving stuff: 0.00s

=== episode:360 Env-steps-taken:58368
action_counts: {0: 4257, 1: 6182, 2: 4378, 3: 5346, 4: 5214, 5: 12485, 6: 4840, 7: 7007, 8: 5610}
picked:  39
episode: 360/2000 -> reward: 53.79687499999997, steps:55319, time-taken: 1.75min, time-elasped: 470.19min
-> berries picked: 39 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4593 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [257, 279, 379, 350, 681, 763, 844, 453, 587]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 7, 4, 9, 13, 22, 22, 10, 13]
	Time taken saving stuff: 0.09s

=== episode:36 Env-steps-taken:52800
action_counts: {0: 396, 1: 330, 2: 550, 3: 198, 4: 418, 5: 3575, 6: 308, 7: 6479, 8: 3477}
picked:  19

==================================================
eval-episode: 360 -> reward: 25.901041666666668, steps: 15731.0, wall-time: 21.26s
-> berries picked: 19 of 800 | patches-visited: [0, 1] | juice left:-0.00
==================================================


=== episode:361 Env-steps-taken:62400
action_counts: {0: 4323, 1: 6941, 2: 4598, 3: 3663, 4: 5357, 5: 9878, 6: 7205, 7: 4910, 8: 5984}
picked:  55
episode: 361/2000 -> reward: 74.71354166666667, steps:52859, time-taken: 1.69min, time-elasped: 472.24min
-> berries picked: 55 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4585 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [253, 277, 375, 342, 683, 769, 850, 451, 585]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 7, 13, 8, 10, 14, 16, 9, 26]
	Time taken saving stuff: 0.02s

=== episode:362 Env-steps-taken:58848
action_counts: {0: 2123, 1: 2882, 2: 2706, 3: 3751, 4: 4510, 5: 6259, 6: 3564, 7: 2772, 8: 3839}
picked:  42
episode: 362/2000 -> reward: 57.28124999999997, steps:32406, time-taken: 1.12min, time-elasped: 473.36min
-> berries picked: 42 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4594 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [250, 275, 378, 341, 686, 775, 854, 447, 588]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 9, 8, 9, 12, 14, 22, 11, 23]
	Time taken saving stuff: 0.11s

=== episode:363 Env-steps-taken:52128
action_counts: {0: 814, 1: 910, 2: 825, 3: 1045, 4: 1342, 5: 1804, 6: 1452, 7: 1298, 8: 1892}
picked:  16
episode: 363/2000 -> reward: 23.416666666666668, steps:11382, time-taken: 0.60min, time-elasped: 473.97min
-> berries picked: 16 of 800 | patches-visited: [0, 2, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4594 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [252, 273, 376, 341, 686, 775, 855, 446, 590]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 9, 4, 9, 13, 17, 15, 5, 19]
	Time taken saving stuff: 0.12s

=== episode:364 Env-steps-taken:55008
action_counts: {0: 1331, 1: 1397, 2: 1716, 3: 2453, 4: 2761, 5: 2915, 6: 2662, 7: 1913, 8: 2398}
picked:  23
episode: 364/2000 -> reward: 37.38020833333333, steps:19546, time-taken: 0.79min, time-elasped: 474.76min
-> berries picked: 23 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4588 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [249, 275, 376, 338, 689, 776, 854, 441, 590]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 6, 16, 9, 10, 9, 18, 9, 27]
	Time taken saving stuff: 0.00s

=== episode:365 Env-steps-taken:55680
action_counts: {0: 2399, 1: 3586, 2: 3069, 3: 3927, 4: 4653, 5: 8217, 6: 4202, 7: 6270, 8: 4631}
picked:  27
episode: 365/2000 -> reward: 40.85937499999999, steps:40954, time-taken: 1.46min, time-elasped: 476.22min
-> berries picked: 27 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4560 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [246, 273, 375, 336, 686, 770, 849, 437, 588]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 7, 13, 10, 14, 13, 11, 9, 26]
	Time taken saving stuff: 0.02s

=== episode:366 Env-steps-taken:52032
action_counts: {0: 3245, 1: 6996, 2: 3718, 3: 4037, 4: 7271, 5: 9491, 6: 5027, 7: 5940, 8: 6292}
picked:  17
episode: 366/2000 -> reward: 20.911458333333336, steps:52017, time-taken: 1.58min, time-elasped: 477.80min
-> berries picked: 17 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4519 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [243, 270, 367, 332, 677, 764, 846, 432, 588]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 8, 8, 8, 18, 19, 17, 10, 24]
	Time taken saving stuff: 0.02s

=== episode:367 Env-steps-taken:59424
action_counts: {0: 2596, 1: 3575, 2: 3872, 3: 3267, 4: 5170, 5: 5129, 6: 4048, 7: 4422, 8: 3993}
picked:  44
episode: 367/2000 -> reward: 60.270833333333286, steps:36072, time-taken: 1.28min, time-elasped: 479.08min
-> berries picked: 44 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4517 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [247, 271, 367, 332, 677, 763, 845, 429, 586]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 7, 6, 10, 14, 5, 13, 9, 21]
	Time taken saving stuff: 0.02s

=== episode:368 Env-steps-taken:62112
action_counts: {0: 2651, 1: 3487, 2: 2816, 3: 3839, 4: 4730, 5: 5764, 6: 5676, 7: 4763, 8: 6139}
picked:  48
episode: 368/2000 -> reward: 75.24999999999999, steps:39865, time-taken: 1.34min, time-elasped: 480.43min
-> berries picked: 48 of 800 | patches-visited: [0, 4, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4525 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [248, 271, 365, 331, 682, 765, 845, 430, 588]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 9, 10, 10, 17, 8, 26, 8, 23]
	Time taken saving stuff: 0.10s

=== episode:369 Env-steps-taken:63072
action_counts: {0: 4367, 1: 5313, 2: 4081, 3: 6083, 4: 8514, 5: 8687, 6: 6248, 7: 7425, 8: 6468}
picked:  54
episode: 369/2000 -> reward: 79.21875000000001, steps:57186, time-taken: 1.77min, time-elasped: 482.20min
-> berries picked: 54 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4528 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [252, 268, 357, 327, 690, 770, 847, 431, 586]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 7, 10, 8, 16, 19, 17, 12, 24]
	Time taken saving stuff: 0.11s

=== episode:370 Env-steps-taken:61824
action_counts: {0: 3223, 1: 3982, 2: 3289, 3: 4136, 4: 5511, 5: 7381, 6: 5423, 7: 4510, 8: 4719}
picked:  45
episode: 370/2000 -> reward: 72.765625, steps:42174, time-taken: 1.32min, time-elasped: 483.52min
-> berries picked: 45 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4541 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [249, 264, 357, 335, 696, 773, 850, 431, 586]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 7, 9, 7, 14, 17, 12, 9, 20]
	Time taken saving stuff: 0.15s

=== episode:37 Env-steps-taken:56832
action_counts: {0: 473, 1: 198, 2: 297, 3: 770, 4: 5280, 5: 2002, 6: 2387, 7: 418, 8: 4674}
picked:  31

==================================================
eval-episode: 370 -> reward: 47.83854166666665, steps: 16499.0, wall-time: 26.14s
-> berries picked: 31 of 800 | patches-visited: [0, 1, 3] | juice left:-0.00
==================================================


=== episode:371 Env-steps-taken:53760
action_counts: {0: 1199, 1: 1683, 2: 2332, 3: 2255, 4: 2404, 5: 2222, 6: 3003, 7: 2090, 8: 1848}
picked:  23
episode: 371/2000 -> reward: 30.880208333333346, steps:19036, time-taken: 0.78min, time-elasped: 484.75min
-> berries picked: 23 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4538 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [246, 265, 360, 335, 693, 769, 849, 434, 587]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 8, 6, 8, 15, 16, 17, 11, 24]
	Time taken saving stuff: 0.10s

=== episode:372 Env-steps-taken:53856
action_counts: {0: 2332, 1: 2387, 2: 2464, 3: 2090, 4: 2552, 5: 6787, 6: 4499, 7: 5950, 8: 2981}
picked:  19
episode: 372/2000 -> reward: 31.401041666666675, steps:32042, time-taken: 1.12min, time-elasped: 485.87min
-> berries picked: 19 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4510 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [244, 265, 358, 333, 682, 767, 844, 432, 585]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 4, 6, 13, 18, 12, 15, 12, 20]
	Time taken saving stuff: 0.00s

=== episode:373 Env-steps-taken:67584
action_counts: {0: 6391, 1: 4587, 2: 4499, 3: 5159, 4: 4763, 5: 5942, 6: 8052, 7: 4191, 8: 5489}
picked:  70
episode: 373/2000 -> reward: 102.63541666666677, steps:49073, time-taken: 1.64min, time-elasped: 487.51min
-> berries picked: 70 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4503 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [246, 263, 360, 335, 682, 766, 838, 430, 583]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 4, 14, 6, 12, 14, 9, 6, 24]
	Time taken saving stuff: 0.02s

=== episode:374 Env-steps-taken:66912
action_counts: {0: 6237, 1: 4576, 2: 3278, 3: 4675, 4: 4730, 5: 5181, 6: 3542, 7: 3674, 8: 2938}
picked:  65
episode: 374/2000 -> reward: 100.16145833333341, steps:38831, time-taken: 1.48min, time-elasped: 488.99min
-> berries picked: 65 of 800 | patches-visited: [0, 7, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4517 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [252, 264, 366, 336, 677, 768, 835, 431, 588]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 9, 6, 12, 17, 14, 20, 12, 15]
	Time taken saving stuff: 0.00s

=== episode:375 Env-steps-taken:59424
action_counts: {0: 2013, 1: 1661, 2: 1683, 3: 2464, 4: 3520, 5: 3442, 6: 1848, 7: 1672, 8: 1628}
picked:  37
episode: 375/2000 -> reward: 61.307291666666636, steps:19931, time-taken: 0.88min, time-elasped: 489.87min
-> berries picked: 37 of 800 | patches-visited: [0, 2, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4524 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [251, 263, 367, 336, 684, 772, 830, 432, 589]
	| approx positives in sample 512: 95
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 7, 6, 5, 13, 17, 12, 9, 19]
	Time taken saving stuff: 0.10s

=== episode:376 Env-steps-taken:58368
action_counts: {0: 6710, 1: 3949, 2: 2970, 3: 3069, 4: 3399, 5: 4224, 6: 2717, 7: 4004, 8: 3576}
picked:  37
episode: 376/2000 -> reward: 53.80729166666664, steps:34618, time-taken: 1.26min, time-elasped: 491.14min
-> berries picked: 37 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4506 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [252, 263, 364, 335, 680, 768, 824, 431, 589]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 14, 14, 5, 16, 22, 10, 6, 19]
	Time taken saving stuff: 0.00s

=== episode:377 Env-steps-taken:55392
action_counts: {0: 3696, 1: 1672, 2: 1650, 3: 2079, 4: 3135, 5: 3652, 6: 3256, 7: 2563, 8: 2109}
picked:  24
episode: 377/2000 -> reward: 40.37499999999999, steps:23812, time-taken: 0.94min, time-elasped: 492.08min
-> berries picked: 24 of 800 | patches-visited: [0, 6, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4504 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [253, 265, 364, 331, 682, 764, 824, 431, 590]
	| approx positives in sample 512: 87
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 4, 8, 10, 15, 11, 16, 4, 16]
	Time taken saving stuff: 0.00s

=== episode:378 Env-steps-taken:58752
action_counts: {0: 4268, 1: 2189, 2: 2453, 3: 2783, 4: 3124, 5: 3905, 6: 2988, 7: 2849, 8: 2145}
picked:  36
episode: 378/2000 -> reward: 57.81249999999997, steps:26704, time-taken: 1.07min, time-elasped: 493.16min
-> berries picked: 36 of 800 | patches-visited: [0, 7, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4513 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [251, 268, 364, 333, 684, 763, 825, 432, 593]
	| approx positives in sample 512: 97
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 6, 7, 8, 10, 10, 15, 11, 22]
	Time taken saving stuff: 0.10s

=== episode:379 Env-steps-taken:48864
action_counts: {0: 198, 1: 66, 2: 132, 3: 231, 4: 121, 5: 99, 6: 99, 7: 176, 8: 116}
picked:  3
episode: 379/2000 -> reward: 5.484375, steps:1238, time-taken: 0.34min, time-elasped: 493.50min
-> berries picked: 3 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4515 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [250, 269, 365, 333, 684, 763, 825, 432, 594]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 5, 5, 7, 20, 17, 14, 12, 25]
	Time taken saving stuff: 0.00s

=== episode:380 Env-steps-taken:57600
action_counts: {0: 6383, 1: 1815, 2: 2321, 3: 2035, 4: 2607, 5: 3047, 6: 3707, 7: 2651, 8: 2475}
picked:  32
episode: 380/2000 -> reward: 50.833333333333314, steps:27041, time-taken: 1.07min, time-elasped: 494.57min
-> berries picked: 32 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4522 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [252, 271, 369, 334, 684, 760, 822, 434, 596]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 9, 9, 12, 16, 12, 15, 12, 18]
	Time taken saving stuff: 0.07s

=== episode:38 Env-steps-taken:53568
action_counts: {0: 1705, 1: 396, 2: 264, 3: 352, 4: 418, 5: 594, 6: 2530, 7: 561, 8: 2447}
picked:  19

==================================================
eval-episode: 380 -> reward: 29.90104166666667, steps: 9267.0, wall-time: 21.79s
-> berries picked: 19 of 800 | patches-visited: [1, 5] | juice left:-0.00
==================================================


=== episode:381 Env-steps-taken:61632
action_counts: {0: 8756, 1: 4752, 2: 2761, 3: 4488, 4: 4400, 5: 7303, 6: 7051, 7: 4510, 8: 5401}
picked:  55
episode: 381/2000 -> reward: 72.71354166666669, steps:49422, time-taken: 1.72min, time-elasped: 496.66min
-> berries picked: 55 of 800 | patches-visited: [0, 4, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4530 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [252, 268, 367, 340, 678, 763, 829, 435, 598]
	| approx positives in sample 512: 89
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 10, 6, 5, 2, 13, 13, 13, 19]
	Time taken saving stuff: 0.01s

=== episode:382 Env-steps-taken:58368
action_counts: {0: 3388, 1: 2035, 2: 2662, 3: 3707, 4: 3069, 5: 4510, 6: 4543, 7: 3157, 8: 3502}
picked:  38
episode: 382/2000 -> reward: 53.8020833333333, steps:30573, time-taken: 1.19min, time-elasped: 497.85min
-> berries picked: 38 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4541 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [253, 267, 366, 344, 678, 768, 828, 440, 597]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 6, 8, 6, 13, 18, 15, 11, 14]
	Time taken saving stuff: 0.09s

=== episode:383 Env-steps-taken:61728
action_counts: {0: 3608, 1: 2574, 2: 2343, 3: 3443, 4: 3751, 5: 4213, 6: 3553, 7: 2068, 8: 2223}
picked:  50
episode: 383/2000 -> reward: 72.23958333333333, steps:27776, time-taken: 1.18min, time-elasped: 499.04min
-> berries picked: 50 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4563 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [254, 268, 370, 345, 687, 769, 831, 440, 599]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 9, 15, 5, 8, 10, 14, 16, 22]
	Time taken saving stuff: 0.13s

=== episode:384 Env-steps-taken:62592
action_counts: {0: 4015, 1: 4653, 2: 2948, 3: 4389, 4: 2904, 5: 3795, 6: 4048, 7: 2981, 8: 2938}
picked:  49
episode: 384/2000 -> reward: 76.74479166666666, steps:32671, time-taken: 1.63min, time-elasped: 500.68min
-> berries picked: 49 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4572 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [254, 271, 369, 345, 687, 769, 832, 445, 600]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 6, 6, 3, 12, 15, 18, 9, 24]
	Time taken saving stuff: 0.09s

=== episode:385 Env-steps-taken:57408
action_counts: {0: 7106, 1: 3498, 2: 3080, 3: 4147, 4: 4158, 5: 7557, 6: 2843, 7: 4587, 8: 4147}
picked:  35
episode: 385/2000 -> reward: 49.81770833333331, steps:41123, time-taken: 1.85min, time-elasped: 502.53min
-> berries picked: 35 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4574 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [253, 271, 369, 344, 690, 769, 831, 446, 601]
	| approx positives in sample 512: 93
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 5, 11, 12, 10, 10, 16, 13, 9]
	Time taken saving stuff: 0.10s

=== episode:386 Env-steps-taken:58080
action_counts: {0: 3905, 1: 3333, 2: 2387, 3: 2563, 4: 4620, 5: 7326, 6: 4774, 7: 3608, 8: 4104}
picked:  32
episode: 386/2000 -> reward: 53.333333333333314, steps:36620, time-taken: 1.29min, time-elasped: 503.83min
-> berries picked: 32 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4564 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [256, 267, 367, 338, 689, 768, 838, 443, 598]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 8, 10, 2, 14, 16, 17, 8, 22]
	Time taken saving stuff: 0.10s

=== episode:387 Env-steps-taken:63168
action_counts: {0: 4543, 1: 3212, 2: 2431, 3: 2145, 4: 3058, 5: 5698, 6: 5049, 7: 3410, 8: 3438}
picked:  53
episode: 387/2000 -> reward: 80.72395833333336, steps:32984, time-taken: 1.25min, time-elasped: 505.08min
-> berries picked: 53 of 800 | patches-visited: [0, 2, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4588 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [261, 270, 369, 337, 687, 771, 848, 446, 599]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 7, 10, 9, 14, 17, 10, 9, 17]
	Time taken saving stuff: 0.09s

=== episode:388 Env-steps-taken:64608
action_counts: {0: 5687, 1: 3608, 2: 3608, 3: 5511, 4: 5665, 5: 7357, 6: 4400, 7: 4818, 8: 3795}
picked:  60
episode: 388/2000 -> reward: 88.18750000000004, steps:44449, time-taken: 1.68min, time-elasped: 506.77min
-> berries picked: 60 of 800 | patches-visited: [0, 4, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4598 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [263, 272, 369, 344, 689, 775, 844, 442, 600]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 6, 9, 3, 7, 19, 10, 18, 22]
	Time taken saving stuff: 0.11s

=== episode:389 Env-steps-taken:60384
action_counts: {0: 3201, 1: 2761, 2: 2640, 3: 3190, 4: 3729, 5: 8250, 6: 4499, 7: 5423, 8: 3708}
picked:  39
episode: 389/2000 -> reward: 63.80729166666663, steps:37401, time-taken: 1.38min, time-elasped: 508.15min
-> berries picked: 39 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4609 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [266, 277, 369, 344, 689, 783, 842, 437, 602]
	| approx positives in sample 512: 114
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 5, 8, 9, 14, 20, 26, 10, 16]
	Time taken saving stuff: 0.09s

=== episode:390 Env-steps-taken:52416
action_counts: {0: 1320, 1: 968, 2: 913, 3: 715, 4: 847, 5: 1859, 6: 1892, 7: 1683, 8: 903}
picked:  14
episode: 390/2000 -> reward: 23.927083333333336, steps:11100, time-taken: 0.70min, time-elasped: 508.85min
-> berries picked: 14 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4613 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [267, 278, 367, 344, 690, 782, 844, 439, 602]
	| approx positives in sample 512: 110
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 9, 10, 11, 18, 15, 14, 12, 16]
	Time taken saving stuff: 0.05s

=== episode:39 Env-steps-taken:58176
action_counts: {0: 14531, 1: 341, 2: 583, 3: 1331, 4: 13750, 5: 3674, 6: 1980, 7: 3674, 8: 6458}
picked:  38

==================================================
eval-episode: 390 -> reward: 53.8020833333333, steps: 46322.0, wall-time: 32.93s
-> berries picked: 38 of 800 | patches-visited: [1, 2] | juice left:-0.00
==================================================


=== episode:391 Env-steps-taken:54816
action_counts: {0: 7656, 1: 2882, 2: 2794, 3: 2838, 4: 3410, 5: 6501, 6: 4202, 7: 4587, 8: 4280}
picked:  28
episode: 391/2000 -> reward: 36.354166666666664, steps:39150, time-taken: 1.34min, time-elasped: 510.75min
-> berries picked: 28 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 4598 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [264, 274, 366, 340, 688, 781, 845, 440, 600]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 7, 3, 7, 16, 20, 21, 8, 23]
	Time taken saving stuff: 0.00s
