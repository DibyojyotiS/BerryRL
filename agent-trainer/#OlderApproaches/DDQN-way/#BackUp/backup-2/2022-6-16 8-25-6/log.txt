copied Agent.py to .temp\2022-6-16 8-25-6/pyfiles-backup
copied ensemble.py to .temp\2022-6-16 8-25-6/pyfiles-backup
copied eval.py to .temp\2022-6-16 8-25-6/pyfiles-backup
copied train.py to .temp\2022-6-16 8-25-6/pyfiles-backup
copied utils.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/copyfiles
copied __init__.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/copyfiles

copied random_env.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/env_generation
copied __init__.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/env_generation

copied exploration.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/exploration_subroutines
copied random_exploration.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/exploration_subroutines
copied __init__.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/exploration_subroutines

copied make_net.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/nn_utils
copied __init__.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/nn_utils

copied utils.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/printing
copied __init__.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/printing

copied env_picture.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/visualization
copied __init__.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils/visualization

copied __init__.py to .temp\2022-6-16 8-25-6/pyfiles-backup/utils


random_train_env
getBabyEnv :
	 logDir : .temp\2022-6-16 8-25-6
	 initial_juice : 0.5
	 end_on_boundary_hit : False
	 penalize_boundary_hit : False
	 allow_no_action : False
	 no_action_threshold : 0.7
	 add_exploration : True
	 field_size : (20000, 20000)
	 initial_pos_around_berry : True
	 nberries : 80
	 num_patches : 10
	 patch_size : (2600, 2600)
	 patch_with_agent_at_center : True
	 sampling_type : 0
	 seperation : 2400
	 show : False
	 spawn_radius : 100


Agent :
	 self : <Agent.Agent object at 0x00000207AFE91CC8>
	 berryField : <BerryFieldEnv instance>
	 mode : train
	 angle : 45
	 persistence : 0.8
	 worth_offset : 0.0
	 noise : 0.01
	 nstep_transition : [1]
	 positive_emphasis : 0
	 reward_patch_discovery : True
	 add_exploration : True
	 time_memory_delta : 0.01
	 time_memory_exp : 1.0
	 render : False
	 renderstep : 10
	 debug : False
	 debugDir : .temp


with living cost, rewards scaled by 2/(berryField.REWARD_RATE*MAXSIZE)
rewards are clipped between 0 and 2
Exploration subroutine as an action
initial p_action: 1
The state-transitions being appended 
            every action will be as [[state, action, sum-reward, nextState, done]] where:
            state is the one the model has taken action on,
            sum-reward is the sum of the rewards in the skip-trajectory,
            nextState is the new state after the action was repeated at most skip-steps times,
            done is wether the terminal state was reached.
Rewarding the agent for discovering new patches
Exploration subroutine added
agent now aware of total-juice
total-params:  2034
with living cost, rewards scaled by 2/(berryField.REWARD_RATE*MAXSIZE)
rewards are clipped between 0 and 2
Exploration subroutine as an action
initial p_action: 4
net(
  (feedforward): ModuleList(
    (0): Linear(in_features=39, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.1)
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): LeakyReLU(negative_slope=0.1)
  )
  (final_stage): ModuleList(
    (0): Linear(in_features=16, out_features=8, bias=True)
    (1): LeakyReLU(negative_slope=0.1)
  )
  (valueL): Linear(in_features=8, out_features=1, bias=True)
  (actadvs): Linear(in_features=8, out_features=9, bias=True)
)
PrioritizedBuffer of type replace-min
optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
), num_gradient_steps= 25
optimizing the online-model after every 100 actions
batch size=512, gamma=0.9, alpha=0.95
polyak_tau=0.1, update_freq=5

=== episode:0 Env-steps-taken:49536
action_counts: {0: 1925, 1: 5324, 2: 1705, 3: 3883, 4: 3817, 5: 1562, 6: 6138, 7: 1760, 8: 2080}
picked:  6
episode: 0/2000 -> reward: 9.96875, steps:28194, time-taken: 0.79min, time-elasped: 0.80min
-> berries picked: 6 of 800 | patches-visited: [0, 2, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 8 | amount-filled: 4.27%
	| action-stats:  [1, 3, 4, 7] [2, 4, 1, 1]
	| approx positives in sample 512: 9
	| approx action-dist in sample 512: [1, 3, 4] [3, 4, 2]
	Time taken saving stuff: 0.04s

=== episode:0 Env-steps-taken:49056
action_counts: {0: 2629, 1: 2519, 2: 19437, 3: 363, 4: 1954, 5: 0, 6: 22154, 7: 0, 8: 0}
picked:  5

==================================================
eval-episode: 0 -> reward: 5.473958333333334, steps: 49056.0, wall-time: 27.99s
-> berries picked: 5 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:1 Env-steps-taken:50688
action_counts: {0: 2299, 1: 9427, 2: 2640, 3: 5665, 4: 7073, 5: 2387, 6: 7931, 7: 2508, 8: 2353}
picked:  10
episode: 1/2000 -> reward: 14.947916666666664, steps:42283, time-taken: 1.40min, time-elasped: 2.67min
-> berries picked: 10 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 19 | amount-filled: 10.68%
	| action-stats:  [1, 3, 4, 6, 7] [2, 7, 4, 5, 1]
	| approx positives in sample 512: 27
	| approx action-dist in sample 512: [1, 3, 4, 6] [4, 12, 5, 6]
	Time taken saving stuff: 0.01s

=== episode:2 Env-steps-taken:49152
action_counts: {0: 946, 1: 5621, 2: 1199, 3: 1947, 4: 4320, 5: 1100, 6: 2728, 7: 1430, 8: 1089}
picked:  4
episode: 2/2000 -> reward: 5.979166666666667, steps:20380, time-taken: 0.91min, time-elasped: 3.58min
-> berries picked: 4 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 23 | amount-filled: 13.77%
	| action-stats:  [1, 3, 4, 6, 7] [2, 9, 6, 5, 1]
	| approx positives in sample 512: 20
	| approx action-dist in sample 512: [1, 3, 4, 6] [1, 6, 8, 5]
	Time taken saving stuff: 0.01s

=== episode:3 Env-steps-taken:50016
action_counts: {0: 2706, 1: 12715, 2: 2673, 3: 6083, 4: 8019, 5: 2618, 6: 8063, 7: 2695, 8: 4444}
picked:  7
episode: 3/2000 -> reward: 10.463541666666666, steps:50016, time-taken: 1.76min, time-elasped: 5.34min
-> berries picked: 7 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 30 | amount-filled: 21.35%
	| action-stats:  [1, 3, 4, 6, 7, 8] [3, 10, 6, 9, 1, 1]
	| approx positives in sample 512: 27
	| approx action-dist in sample 512: [1, 3, 4, 6, 8] [1, 11, 6, 8, 1]
	Time taken saving stuff: 0.00s

=== episode:4 Env-steps-taken:48960
action_counts: {0: 6237, 1: 6304, 2: 2464, 3: 3806, 4: 3685, 5: 8976, 6: 6193, 7: 2178, 8: 2343}
picked:  3
episode: 4/2000 -> reward: 4.984375, steps:42186, time-taken: 1.39min, time-elasped: 6.74min
-> berries picked: 3 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 33 | amount-filled: 27.74%
	| action-stats:  [1, 3, 4, 5, 6, 7, 8] [3, 10, 7, 1, 10, 1, 1]
	| approx positives in sample 512: 13
	| approx action-dist in sample 512: [1, 3, 4, 6, 8] [2, 6, 2, 2, 1]
	Time taken saving stuff: 0.00s

=== episode:5 Env-steps-taken:50496
action_counts: {0: 2079, 1: 8789, 2: 2123, 3: 5709, 4: 3498, 5: 6611, 6: 5973, 7: 2552, 8: 2418}
picked:  13
episode: 5/2000 -> reward: 13.932291666666663, steps:39752, time-taken: 1.63min, time-elasped: 8.37min
-> berries picked: 13 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 47 | amount-filled: 33.76%
	| action-stats:  [1, 3, 4, 5, 6, 7, 8] [11, 14, 8, 1, 11, 1, 1]
	| approx positives in sample 512: 14
	| approx action-dist in sample 512: [1, 3, 4, 5, 6] [2, 4, 3, 1, 4]
	Time taken saving stuff: 0.00s

=== episode:6 Env-steps-taken:54048
action_counts: {0: 2431, 1: 10835, 2: 2607, 3: 4521, 4: 4972, 5: 5423, 6: 8514, 7: 2552, 8: 2860}
picked:  20
episode: 6/2000 -> reward: 32.39583333333334, steps:44715, time-taken: 1.84min, time-elasped: 10.21min
-> berries picked: 20 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 68 | amount-filled: 40.54%
	| action-stats:  [1, 3, 4, 5, 6, 7, 8] [13, 16, 14, 2, 20, 1, 2]
	| approx positives in sample 512: 23
	| approx action-dist in sample 512: [1, 3, 4, 6, 8] [8, 3, 4, 7, 1]
	Time taken saving stuff: 0.01s

=== episode:7 Env-steps-taken:50592
action_counts: {0: 2552, 1: 8591, 2: 2585, 3: 3872, 4: 7973, 5: 4070, 6: 5533, 7: 2750, 8: 6743}
picked:  10
episode: 7/2000 -> reward: 14.447916666666664, steps:44669, time-taken: 1.81min, time-elasped: 12.03min
-> berries picked: 10 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 79 | amount-filled: 47.31%
	| action-stats:  [1, 3, 4, 5, 6, 7, 8] [13, 17, 18, 2, 24, 2, 3]
	| approx positives in sample 512: 12
	| approx action-dist in sample 512: [1, 3, 4, 6] [4, 1, 3, 4]
	Time taken saving stuff: 0.00s

=== episode:8 Env-steps-taken:50880
action_counts: {0: 2442, 1: 2937, 2: 2420, 3: 4169, 4: 6270, 5: 2761, 6: 6941, 7: 2068, 8: 8093}
picked:  11
episode: 8/2000 -> reward: 14.94270833333333, steps:38101, time-taken: 1.60min, time-elasped: 13.63min
-> berries picked: 11 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 90 | amount-filled: 53.08%
	| action-stats:  [1, 3, 4, 5, 6, 7, 8] [13, 20, 18, 2, 28, 2, 7]
	| approx positives in sample 512: 24
	| approx action-dist in sample 512: [1, 3, 4, 6, 8] [7, 3, 6, 5, 3]
	Time taken saving stuff: 0.01s

=== episode:9 Env-steps-taken:50304
action_counts: {0: 2750, 1: 2563, 2: 1925, 3: 3575, 4: 3894, 5: 2849, 6: 7821, 7: 2849, 8: 7118}
picked:  8
episode: 9/2000 -> reward: 13.95833333333333, steps:35344, time-taken: 1.59min, time-elasped: 15.22min
-> berries picked: 8 of 800 | patches-visited: [0, 3, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 100 | amount-filled: 58.44%
	| action-stats:  [1, 3, 4, 5, 6, 7, 8] [13, 22, 20, 5, 28, 3, 9]
	| approx positives in sample 512: 17
	| approx action-dist in sample 512: [1, 3, 4, 6, 8] [3, 4, 5, 4, 1]
	Time taken saving stuff: 0.01s

=== episode:10 Env-steps-taken:50016
action_counts: {0: 2827, 1: 3638, 2: 6589, 3: 2530, 4: 5093, 5: 7425, 6: 6017, 7: 3817, 8: 7150}
picked:  7
episode: 10/2000 -> reward: 11.463541666666666, steps:45086, time-taken: 1.78min, time-elasped: 17.00min
-> berries picked: 7 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 108 | amount-filled: 65.27%
	| action-stats:  [1, 2, 3, 4, 5, 6, 7, 8] [13, 1, 23, 20, 5, 31, 4, 11]
	| approx positives in sample 512: 24
	| approx action-dist in sample 512: [1, 3, 4, 5, 6, 7, 8] [5, 3, 4, 1, 9, 1, 1]
	Time taken saving stuff: 0.05s

=== episode:1 Env-steps-taken:49152
action_counts: {0: 1694, 1: 9768, 2: 0, 3: 7590, 4: 2948, 5: 3036, 6: 6215, 7: 5972, 8: 6347}
picked:  3

==================================================
eval-episode: 10 -> reward: 6.984375000000001, steps: 43570.0, wall-time: 37.96s
-> berries picked: 3 of 800 | patches-visited: [1, 2] | juice left:-0.00
==================================================


=== episode:11 Env-steps-taken:53184
action_counts: {0: 3212, 1: 4741, 2: 3476, 3: 5500, 4: 3311, 5: 2950, 6: 4675, 7: 8998, 8: 5005}
picked:  19
episode: 11/2000 -> reward: 28.901041666666675, steps:41868, time-taken: 1.71min, time-elasped: 19.35min
-> berries picked: 19 of 800 | patches-visited: [0, 2, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 129 | amount-filled: 71.61%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 17, 3, 27, 21, 6, 35, 7, 11]
	| approx positives in sample 512: 26
	| approx action-dist in sample 512: [1, 2, 3, 4, 6, 7, 8] [7, 2, 4, 7, 2, 1, 3]
	Time taken saving stuff: 0.01s

=== episode:12 Env-steps-taken:56256
action_counts: {0: 3344, 1: 13818, 2: 3751, 3: 4631, 4: 3388, 5: 3190, 6: 9097, 7: 7084, 8: 7953}
picked:  30
episode: 12/2000 -> reward: 42.843749999999986, steps:56256, time-taken: 2.35min, time-elasped: 21.70min
-> berries picked: 30 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 159 | amount-filled: 80.14%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [3, 25, 5, 29, 23, 6, 39, 11, 18]
	| approx positives in sample 512: 35
	| approx action-dist in sample 512: [1, 2, 3, 4, 5, 6, 7, 8] [6, 1, 7, 2, 5, 4, 5, 5]
	Time taken saving stuff: 0.00s

=== episode:13 Env-steps-taken:58272
action_counts: {0: 3267, 1: 9174, 2: 8794, 3: 4081, 4: 3201, 5: 2684, 6: 9317, 7: 10153, 8: 7601}
picked:  33
episode: 13/2000 -> reward: 53.32812499999997, steps:58272, time-taken: 2.25min, time-elasped: 23.95min
-> berries picked: 33 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 192 | amount-filled: 88.97%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 33, 6, 30, 23, 6, 43, 21, 26]
	| approx positives in sample 512: 22
	| approx action-dist in sample 512: [0, 1, 2, 4, 5, 6, 7] [2, 9, 2, 1, 1, 6, 1]
	Time taken saving stuff: 0.00s

=== episode:14 Env-steps-taken:55776
action_counts: {0: 3333, 1: 8223, 2: 8140, 3: 5500, 4: 4290, 5: 3575, 6: 10175, 7: 7282, 8: 5258}
picked:  28
episode: 14/2000 -> reward: 40.354166666666664, steps:55776, time-taken: 2.22min, time-elasped: 26.18min
-> berries picked: 28 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 220 | amount-filled: 97.42%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 38, 7, 32, 23, 6, 50, 29, 31]
	| approx positives in sample 512: 33
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 5, 2, 5, 2, 3, 11, 1, 3]
	Time taken saving stuff: 0.00s

=== episode:15 Env-steps-taken:62112
action_counts: {0: 4444, 1: 9372, 2: 6446, 3: 9031, 4: 4840, 5: 3982, 6: 7799, 7: 11121, 8: 5077}
picked:  49
episode: 15/2000 -> reward: 73.24479166666666, steps:62112, time-taken: 2.36min, time-elasped: 28.55min
-> berries picked: 49 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 268 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 49, 10, 39, 26, 7, 55, 43, 35]
	| approx positives in sample 512: 33
	| approx action-dist in sample 512: [0, 1, 4, 5, 6, 7, 8] [2, 6, 5, 1, 6, 4, 9]
	Time taken saving stuff: 0.01s

=== episode:16 Env-steps-taken:65568
action_counts: {0: 6039, 1: 7810, 2: 7843, 3: 7359, 4: 6556, 5: 3520, 6: 6622, 7: 10351, 8: 5281}
picked:  63
episode: 16/2000 -> reward: 91.17187500000006, steps:61381, time-taken: 2.31min, time-elasped: 30.86min
-> berries picked: 63 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 327 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 58, 10, 41, 33, 11, 60, 55, 49]
	| approx positives in sample 512: 35
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 6, 7, 8] [2, 6, 3, 1, 6, 7, 6, 4]
	Time taken saving stuff: 0.01s

=== episode:17 Env-steps-taken:61056
action_counts: {0: 4290, 1: 6303, 2: 6083, 3: 7250, 4: 4752, 5: 3157, 6: 8470, 7: 8085, 8: 5126}
picked:  49
episode: 17/2000 -> reward: 68.74479166666666, steps:53516, time-taken: 1.93min, time-elasped: 32.79min
-> berries picked: 49 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 373 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 65, 13, 44, 37, 13, 68, 68, 52]
	| approx positives in sample 512: 46
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 15, 1, 2, 3, 1, 8, 8, 7]
	Time taken saving stuff: 0.00s

=== episode:18 Env-steps-taken:60576
action_counts: {0: 7061, 1: 5951, 2: 8173, 3: 5687, 4: 6314, 5: 4642, 6: 8382, 7: 9438, 8: 4928}
picked:  43
episode: 18/2000 -> reward: 65.27604166666664, steps:60576, time-taken: 2.08min, time-elasped: 34.87min
-> berries picked: 43 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 413 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 74, 18, 46, 38, 14, 78, 77, 53]
	| approx positives in sample 512: 37
	| approx action-dist in sample 512: [0, 1, 2, 4, 6, 7, 8] [2, 8, 2, 2, 5, 7, 11]
	Time taken saving stuff: 0.00s

=== episode:19 Env-steps-taken:51840
action_counts: {0: 7601, 1: 4081, 2: 5709, 3: 5214, 4: 7942, 5: 3234, 6: 5841, 7: 9391, 8: 2827}
picked:  13
episode: 19/2000 -> reward: 19.932291666666668, steps:51840, time-taken: 1.81min, time-elasped: 36.69min
-> berries picked: 13 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 423 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 75, 18, 44, 40, 15, 80, 83, 53]
	| approx positives in sample 512: 39
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 8, 4, 2, 5, 1, 5, 7, 5]
	Time taken saving stuff: 0.00s

=== episode:20 Env-steps-taken:61344
action_counts: {0: 6204, 1: 5676, 2: 9262, 3: 6567, 4: 6138, 5: 3528, 6: 10714, 7: 8481, 8: 4774}
picked:  45
episode: 20/2000 -> reward: 69.26562499999999, steps:61344, time-taken: 2.06min, time-elasped: 38.75min
-> berries picked: 45 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 466 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 80, 20, 47, 42, 16, 91, 95, 59]
	| approx positives in sample 512: 42
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 8, 3, 2, 3, 3, 7, 9, 5]
	Time taken saving stuff: 0.05s

=== episode:2 Env-steps-taken:59904
action_counts: {0: 3091, 1: 2618, 2: 3366, 3: 19589, 4: 3993, 5: 3069, 6: 2376, 7: 4103, 8: 17699}
picked:  46

==================================================
eval-episode: 20 -> reward: 61.760416666666615, steps: 59904.0, wall-time: 37.35s
-> berries picked: 46 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:21 Env-steps-taken:62400
action_counts: {0: 3553, 1: 5247, 2: 7084, 3: 4037, 4: 6248, 5: 4444, 6: 6292, 7: 7128, 8: 4159}
picked:  54
episode: 21/2000 -> reward: 74.71874999999999, steps:48192, time-taken: 1.62min, time-elasped: 41.00min
-> berries picked: 54 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 516 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 94, 23, 49, 45, 22, 98, 110, 59]
	| approx positives in sample 512: 51
	| approx action-dist in sample 512: [0, 1, 3, 4, 5, 6, 7, 8] [4, 19, 3, 4, 1, 6, 8, 6]
	Time taken saving stuff: 0.00s

=== episode:22 Env-steps-taken:55008
action_counts: {0: 3366, 1: 4070, 2: 3704, 3: 7700, 4: 9669, 5: 5874, 6: 5643, 7: 4224, 8: 10758}
picked:  25
episode: 22/2000 -> reward: 36.36979166666667, steps:55008, time-taken: 1.99min, time-elasped: 42.99min
-> berries picked: 25 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 541 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 98, 24, 53, 48, 27, 100, 115, 60]
	| approx positives in sample 512: 43
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 6, 7, 8] [1, 10, 4, 5, 6, 6, 8, 3]
	Time taken saving stuff: 0.00s

=== episode:23 Env-steps-taken:66624
action_counts: {0: 5797, 1: 6105, 2: 7447, 3: 8008, 4: 9042, 5: 10043, 6: 6347, 7: 7557, 8: 5808}
picked:  70
episode: 23/2000 -> reward: 96.63541666666674, steps:66154, time-taken: 2.21min, time-elasped: 45.20min
-> berries picked: 70 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 606 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 106, 32, 60, 56, 39, 107, 126, 62]
	| approx positives in sample 512: 47
	| approx action-dist in sample 512: [1, 2, 3, 4, 6, 7, 8] [13, 5, 4, 3, 7, 10, 5]
	Time taken saving stuff: 0.00s

=== episode:24 Env-steps-taken:66144
action_counts: {0: 4169, 1: 7117, 2: 9548, 3: 6501, 4: 6182, 5: 7062, 6: 5324, 7: 8042, 8: 3839}
picked:  70
episode: 24/2000 -> reward: 94.13541666666674, steps:57784, time-taken: 1.96min, time-elasped: 47.16min
-> berries picked: 70 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 671 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [19, 122, 38, 68, 59, 51, 111, 140, 63]
	| approx positives in sample 512: 44
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 4, 5, 2, 1, 5, 9, 11, 6]
	Time taken saving stuff: 0.00s

=== episode:25 Env-steps-taken:63264
action_counts: {0: 5093, 1: 7601, 2: 7029, 3: 5553, 4: 5225, 5: 10175, 6: 5467, 7: 6754, 8: 3630}
picked:  54
episode: 25/2000 -> reward: 80.21875000000001, steps:56527, time-taken: 1.90min, time-elasped: 49.06min
-> berries picked: 54 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 722 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [22, 134, 46, 73, 64, 57, 113, 148, 65]
	| approx positives in sample 512: 53
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [2, 8, 4, 6, 2, 6, 6, 14, 5]
	Time taken saving stuff: 0.00s

=== episode:26 Env-steps-taken:73440
action_counts: {0: 5091, 1: 6787, 2: 8602, 3: 6831, 4: 5170, 5: 9955, 6: 8096, 7: 9922, 8: 3949}
picked:  88
episode: 26/2000 -> reward: 133.04166666666683, steps:64403, time-taken: 2.25min, time-elasped: 51.31min
-> berries picked: 88 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 800 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [27, 146, 56, 77, 69, 73, 114, 171, 67]
	| approx positives in sample 512: 50
	| approx action-dist in sample 512: [0, 1, 2, 4, 5, 6, 7, 8] [1, 10, 3, 5, 9, 8, 11, 3]
	Time taken saving stuff: 0.00s

=== episode:27 Env-steps-taken:64032
action_counts: {0: 8624, 1: 5676, 2: 11716, 3: 7238, 4: 4521, 5: 8976, 6: 6182, 7: 7546, 8: 3553}
picked:  54
episode: 27/2000 -> reward: 83.21875000000001, steps:64032, time-taken: 2.09min, time-elasped: 53.41min
-> berries picked: 54 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 849 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [30, 152, 67, 80, 71, 82, 118, 183, 66]
	| approx positives in sample 512: 54
	| approx action-dist in sample 512: [0, 1, 2, 4, 5, 6, 7, 8] [2, 15, 4, 3, 8, 7, 7, 8]
	Time taken saving stuff: 0.00s

=== episode:28 Env-steps-taken:66624
action_counts: {0: 9438, 1: 5918, 2: 10873, 3: 9207, 4: 4191, 5: 10395, 6: 5368, 7: 7425, 8: 3795}
picked:  73
episode: 28/2000 -> reward: 96.61979166666676, steps:66610, time-taken: 2.13min, time-elasped: 55.54min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 913 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [34, 159, 81, 82, 74, 95, 122, 195, 71]
	| approx positives in sample 512: 71
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [1, 12, 10, 9, 8, 5, 9, 15, 2]
	Time taken saving stuff: 0.01s

=== episode:29 Env-steps-taken:66240
action_counts: {0: 8041, 1: 6380, 2: 13310, 3: 5632, 4: 5214, 5: 8811, 6: 7623, 7: 6974, 8: 3960}
picked:  75
episode: 29/2000 -> reward: 94.60937500000009, steps:65945, time-taken: 2.23min, time-elasped: 57.77min
-> berries picked: 75 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 983 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [42, 171, 95, 85, 75, 109, 126, 206, 74]
	| approx positives in sample 512: 78
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 17, 8, 2, 4, 13, 6, 18, 6]
	Time taken saving stuff: 0.00s

=== episode:30 Env-steps-taken:66240
action_counts: {0: 9174, 1: 5654, 2: 12430, 3: 7194, 4: 5643, 5: 7645, 6: 7094, 7: 7128, 8: 4180}
picked:  78
episode: 30/2000 -> reward: 94.59375000000007, steps:66142, time-taken: 2.17min, time-elasped: 59.94min
-> berries picked: 78 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1052 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [52, 179, 105, 93, 84, 117, 129, 218, 75]
	| approx positives in sample 512: 62
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 10, 5, 1, 2, 8, 12, 14, 6]
	Time taken saving stuff: 0.06s

=== episode:3 Env-steps-taken:57504
action_counts: {0: 24233, 1: 1903, 2: 2442, 3: 330, 4: 23635, 5: 1474, 6: 1353, 7: 2090, 8: 44}
picked:  33

==================================================
eval-episode: 30 -> reward: 49.328124999999986, steps: 57504.0, wall-time: 37.12s
-> berries picked: 33 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:31 Env-steps-taken:68160
action_counts: {0: 8228, 1: 5126, 2: 10648, 3: 5819, 4: 4972, 5: 5384, 6: 5148, 7: 4763, 8: 3234}
picked:  75
episode: 31/2000 -> reward: 105.60937500000009, steps:53322, time-taken: 1.88min, time-elasped: 62.44min
-> berries picked: 75 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1118 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [63, 186, 119, 97, 89, 128, 133, 226, 77]
	| approx positives in sample 512: 70
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 15, 10, 4, 10, 4, 6, 13, 4]
	Time taken saving stuff: 0.00s

=== episode:32 Env-steps-taken:71424
action_counts: {0: 8470, 1: 5467, 2: 10043, 3: 6611, 4: 5313, 5: 8991, 6: 7029, 7: 8404, 8: 3762}
picked:  82
episode: 32/2000 -> reward: 122.36979166666683, steps:64090, time-taken: 2.15min, time-elasped: 64.60min
-> berries picked: 82 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1190 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [75, 191, 131, 99, 92, 143, 141, 240, 78]
	| approx positives in sample 512: 60
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 13, 8, 8, 5, 5, 6, 7, 2]
	Time taken saving stuff: 0.00s

=== episode:33 Env-steps-taken:72288
action_counts: {0: 9064, 1: 5533, 2: 11517, 3: 5687, 4: 6666, 5: 8613, 6: 7711, 7: 7741, 8: 3828}
picked:  97
episode: 33/2000 -> reward: 126.99479166666687, steps:66360, time-taken: 2.25min, time-elasped: 66.85min
-> berries picked: 97 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1276 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [84, 196, 147, 100, 105, 153, 155, 255, 81]
	| approx positives in sample 512: 63
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 8, 9, 4, 8, 6, 5, 10, 6]
	Time taken saving stuff: 0.01s

=== episode:34 Env-steps-taken:67200
action_counts: {0: 6149, 1: 4279, 2: 7678, 3: 3740, 4: 4378, 5: 6380, 6: 5181, 7: 5126, 8: 3487}
picked:  69
episode: 34/2000 -> reward: 99.64062500000007, steps:46398, time-taken: 1.62min, time-elasped: 68.47min
-> berries picked: 69 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1339 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [93, 203, 154, 102, 110, 162, 166, 266, 83]
	| approx positives in sample 512: 63
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 4, 7, 4, 8, 9, 4, 13, 9]
	Time taken saving stuff: 0.00s

=== episode:35 Env-steps-taken:70656
action_counts: {0: 8569, 1: 4972, 2: 11550, 3: 5775, 4: 9537, 5: 7489, 6: 8943, 7: 6413, 8: 4378}
picked:  87
episode: 35/2000 -> reward: 118.54687500000013, steps:67626, time-taken: 2.36min, time-elasped: 70.83min
-> berries picked: 87 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1419 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [108, 209, 161, 112, 128, 165, 176, 277, 83]
	| approx positives in sample 512: 58
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 7, 11, 3, 5, 5, 7, 13, 3]
	Time taken saving stuff: 0.00s

=== episode:36 Env-steps-taken:60768
action_counts: {0: 7821, 1: 4180, 2: 8448, 3: 11950, 4: 5082, 5: 4906, 6: 6875, 7: 8525, 8: 2981}
picked:  49
episode: 36/2000 -> reward: 66.24479166666664, steps:60768, time-taken: 2.03min, time-elasped: 72.86min
-> berries picked: 49 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1457 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [117, 211, 163, 115, 136, 169, 180, 281, 85]
	| approx positives in sample 512: 71
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 11, 9, 3, 2, 6, 9, 17, 7]
	Time taken saving stuff: 0.00s

=== episode:37 Env-steps-taken:65568
action_counts: {0: 9185, 1: 5940, 2: 10549, 3: 8555, 4: 6655, 5: 5962, 6: 8734, 7: 6050, 8: 3938}
picked:  73
episode: 37/2000 -> reward: 91.11979166666671, steps:65568, time-taken: 2.21min, time-elasped: 75.07min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1521 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [129, 216, 177, 123, 145, 174, 184, 288, 85]
	| approx positives in sample 512: 84
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 15, 8, 7, 5, 11, 10, 19, 4]
	Time taken saving stuff: 0.00s

=== episode:38 Env-steps-taken:67776
action_counts: {0: 7117, 1: 6908, 2: 10593, 3: 10681, 4: 6006, 5: 5522, 6: 7953, 7: 9014, 8: 3982}
picked:  73
episode: 38/2000 -> reward: 102.61979166666679, steps:67776, time-taken: 2.20min, time-elasped: 77.28min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1582 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [136, 220, 193, 132, 154, 180, 185, 296, 86]
	| approx positives in sample 512: 84
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 19, 9, 6, 6, 7, 14, 11, 6]
	Time taken saving stuff: 0.00s

=== episode:39 Env-steps-taken:57600
action_counts: {0: 7832, 1: 6017, 2: 5885, 3: 10989, 4: 4631, 5: 4961, 6: 7121, 7: 5401, 8: 4763}
picked:  38
episode: 39/2000 -> reward: 49.8020833333333, steps:57600, time-taken: 1.89min, time-elasped: 79.17min
-> berries picked: 38 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1614 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [139, 222, 198, 136, 160, 184, 190, 298, 87]
	| approx positives in sample 512: 78
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 10, 14, 5, 5, 5, 13, 15, 4]
	Time taken saving stuff: 0.00s

=== episode:40 Env-steps-taken:67200
action_counts: {0: 7964, 1: 6600, 2: 9900, 3: 8591, 4: 6633, 5: 5423, 6: 7568, 7: 6930, 8: 4324}
picked:  79
episode: 40/2000 -> reward: 99.58854166666676, steps:63933, time-taken: 2.13min, time-elasped: 81.30min
-> berries picked: 79 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1675 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [150, 229, 214, 140, 168, 193, 190, 304, 87]
	| approx positives in sample 512: 78
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 13, 8, 13, 5, 5, 12, 3]
	Time taken saving stuff: 0.05s

=== episode:4 Env-steps-taken:70080
action_counts: {0: 2794, 1: 22484, 2: 5764, 3: 6765, 4: 5676, 5: 12518, 6: 7721, 7: 2123, 8: 4235}
picked:  81

==================================================
eval-episode: 40 -> reward: 115.57812500000014, steps: 70080.0, wall-time: 41.88s
-> berries picked: 81 of 800 | patches-visited: [1, 4] | juice left:-0.00
==================================================


=== episode:41 Env-steps-taken:75552
action_counts: {0: 8272, 1: 8415, 2: 8481, 3: 7210, 4: 9064, 5: 8888, 6: 8690, 7: 5291, 8: 4895}
picked:  96
episode: 41/2000 -> reward: 144.00000000000006, steps:69206, time-taken: 2.38min, time-elasped: 84.38min
-> berries picked: 96 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1752 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [162, 235, 220, 146, 187, 207, 197, 310, 88]
	| approx positives in sample 512: 82
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 9, 16, 5, 8, 8, 7, 15, 6]
	Time taken saving stuff: 0.01s

=== episode:42 Env-steps-taken:68928
action_counts: {0: 5786, 1: 7073, 2: 7766, 3: 8052, 4: 8437, 5: 6248, 6: 8074, 7: 5478, 8: 4621}
picked:  80
episode: 42/2000 -> reward: 108.58333333333348, steps:61535, time-taken: 2.09min, time-elasped: 86.48min
-> berries picked: 80 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1818 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [167, 238, 227, 154, 198, 221, 209, 316, 88]
	| approx positives in sample 512: 84
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 6, 8, 8, 9, 11, 5, 21, 4]
	Time taken saving stuff: 0.00s

=== episode:43 Env-steps-taken:65088
action_counts: {0: 6358, 1: 4829, 2: 4620, 3: 5302, 4: 6897, 5: 5346, 6: 9152, 7: 5511, 8: 5214}
picked:  60
episode: 43/2000 -> reward: 89.68750000000004, steps:53229, time-taken: 1.80min, time-elasped: 88.28min
-> berries picked: 60 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1860 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [180, 240, 227, 153, 203, 231, 218, 321, 87]
	| approx positives in sample 512: 70
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 15, 12, 1, 7, 7, 5, 12, 4]
	Time taken saving stuff: 0.00s

=== episode:44 Env-steps-taken:63648
action_counts: {0: 5412, 1: 5500, 2: 5027, 3: 7931, 4: 6358, 5: 5676, 6: 5995, 7: 4554, 8: 3747}
picked:  58
episode: 44/2000 -> reward: 81.1979166666667, steps:50200, time-taken: 1.70min, time-elasped: 89.99min
-> berries picked: 58 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1908 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [191, 243, 231, 157, 210, 243, 218, 327, 88]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 22, 12, 10, 9, 11, 6, 10, 3]
	Time taken saving stuff: 0.00s

=== episode:45 Env-steps-taken:67104
action_counts: {0: 9229, 1: 7232, 2: 6490, 3: 5984, 4: 5764, 5: 5599, 6: 7568, 7: 5170, 8: 4620}
picked:  73
episode: 45/2000 -> reward: 100.11979166666674, steps:57656, time-taken: 1.97min, time-elasped: 91.96min
-> berries picked: 73 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 1969 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [202, 246, 241, 164, 219, 248, 224, 333, 92]
	| approx positives in sample 512: 97
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 12, 9, 12, 13, 18, 9, 9, 6]
	Time taken saving stuff: 0.00s

=== episode:46 Env-steps-taken:60864
action_counts: {0: 6545, 1: 7447, 2: 6105, 3: 6501, 4: 5137, 5: 6039, 6: 7953, 7: 4180, 8: 6178}
picked:  47
episode: 46/2000 -> reward: 67.7552083333333, steps:56085, time-taken: 1.88min, time-elasped: 93.84min
-> berries picked: 47 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2002 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [212, 250, 249, 164, 223, 251, 227, 333, 93]
	| approx positives in sample 512: 80
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 17, 13, 3, 11, 4, 10, 15, 1]
	Time taken saving stuff: 0.00s

=== episode:47 Env-steps-taken:66912
action_counts: {0: 8602, 1: 6721, 2: 6578, 3: 6105, 4: 5764, 5: 6688, 6: 10626, 7: 6325, 8: 4222}
picked:  73
episode: 47/2000 -> reward: 98.11979166666673, steps:61631, time-taken: 2.10min, time-elasped: 95.95min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2055 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [226, 251, 259, 169, 222, 259, 234, 340, 95]
	| approx positives in sample 512: 86
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 11, 15, 7, 9, 11, 10, 10, 2]
	Time taken saving stuff: 0.01s

=== episode:48 Env-steps-taken:58176
action_counts: {0: 6369, 1: 8360, 2: 8096, 3: 7334, 4: 4873, 5: 6116, 6: 7766, 7: 4070, 8: 5192}
picked:  43
episode: 48/2000 -> reward: 52.77604166666664, steps:58176, time-taken: 2.05min, time-elasped: 98.00min
-> berries picked: 43 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2083 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [231, 257, 265, 170, 223, 266, 237, 339, 95]
	| approx positives in sample 512: 80
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 16, 10, 6, 13, 8, 9, 9, 4]
	Time taken saving stuff: 0.01s

=== episode:49 Env-steps-taken:62400
action_counts: {0: 6424, 1: 5511, 2: 6974, 3: 8118, 4: 5423, 5: 4873, 6: 6688, 7: 4554, 8: 3620}
picked:  60
episode: 49/2000 -> reward: 74.68750000000001, steps:52185, time-taken: 1.82min, time-elasped: 99.83min
-> berries picked: 60 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2129 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [245, 260, 270, 179, 228, 271, 236, 344, 96]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 9, 18, 6, 8, 13, 8, 13, 7]
	Time taken saving stuff: 0.00s

=== episode:50 Env-steps-taken:68736
action_counts: {0: 6149, 1: 7458, 2: 6303, 3: 5643, 4: 5621, 5: 6281, 6: 8492, 7: 4169, 8: 6630}
picked:  69
episode: 50/2000 -> reward: 108.64062500000009, steps:56746, time-taken: 1.98min, time-elasped: 101.81min
-> berries picked: 69 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2185 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [255, 265, 274, 182, 238, 277, 246, 349, 99]
	| approx positives in sample 512: 95
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 11, 13, 12, 13, 10, 10, 11, 4]
	Time taken saving stuff: 0.05s

=== episode:5 Env-steps-taken:51264
action_counts: {0: 1199, 1: 121, 2: 484, 3: 23848, 4: 198, 5: 616, 6: 1177, 7: 44, 8: 23577}
picked:  13

==================================================
eval-episode: 50 -> reward: 16.932291666666664, steps: 51264.0, wall-time: 36.50s
-> berries picked: 13 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:51 Env-steps-taken:55968
action_counts: {0: 8569, 1: 6039, 2: 4939, 3: 7315, 4: 3927, 5: 4587, 6: 8624, 7: 5720, 8: 6248}
picked:  31
episode: 51/2000 -> reward: 41.33854166666666, steps:55968, time-taken: 1.84min, time-elasped: 104.26min
-> berries picked: 31 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2200 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [256, 268, 277, 183, 238, 275, 254, 350, 99]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 19, 16, 4, 9, 11, 11, 11, 11]
	Time taken saving stuff: 0.00s

=== episode:52 Env-steps-taken:66432
action_counts: {0: 8316, 1: 5489, 2: 7557, 3: 7172, 4: 5555, 5: 5874, 6: 9189, 7: 5302, 8: 4510}
picked:  66
episode: 52/2000 -> reward: 96.65625000000004, steps:58964, time-taken: 1.94min, time-elasped: 106.21min
-> berries picked: 66 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2252 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [267, 276, 283, 187, 244, 277, 262, 356, 100]
	| approx positives in sample 512: 86
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 11, 14, 9, 5, 8, 10, 10, 3]
	Time taken saving stuff: 0.00s

=== episode:53 Env-steps-taken:63648
action_counts: {0: 8382, 1: 6611, 2: 8838, 3: 5214, 4: 4928, 5: 7095, 6: 10956, 7: 6215, 8: 4884}
picked:  63
episode: 53/2000 -> reward: 81.17187500000001, steps:63123, time-taken: 2.10min, time-elasped: 108.31min
-> berries picked: 63 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2287 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [271, 281, 289, 189, 247, 280, 265, 364, 101]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [19, 9, 16, 9, 10, 16, 6, 16, 2]
	Time taken saving stuff: 0.00s

=== episode:54 Env-steps-taken:56064
action_counts: {0: 5423, 1: 5753, 2: 5479, 3: 5258, 4: 6908, 5: 6116, 6: 6633, 7: 5368, 8: 6974}
picked:  35
episode: 54/2000 -> reward: 41.817708333333314, steps:53912, time-taken: 1.88min, time-elasped: 110.20min
-> berries picked: 35 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2309 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [276, 282, 290, 191, 249, 284, 271, 365, 101]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 14, 11, 10, 9, 8, 8, 16, 5]
	Time taken saving stuff: 0.00s

=== episode:55 Env-steps-taken:64320
action_counts: {0: 5390, 1: 6061, 2: 8371, 3: 8822, 4: 7755, 5: 8527, 6: 7216, 7: 6028, 8: 6039}
picked:  73
episode: 55/2000 -> reward: 84.61979166666674, steps:64209, time-taken: 2.15min, time-elasped: 112.35min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2353 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [281, 288, 293, 193, 260, 288, 277, 366, 107]
	| approx positives in sample 512: 101
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 16, 15, 10, 7, 10, 11, 18, 3]
	Time taken saving stuff: 0.01s

=== episode:56 Env-steps-taken:63648
action_counts: {0: 5852, 1: 5049, 2: 7865, 3: 8085, 4: 5995, 5: 7271, 6: 7645, 7: 4103, 8: 5479}
picked:  54
episode: 56/2000 -> reward: 81.21875, steps:57344, time-taken: 2.00min, time-elasped: 114.36min
-> berries picked: 54 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2384 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [284, 295, 294, 193, 266, 294, 283, 368, 107]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 14, 13, 11, 11, 19, 15, 10, 8]
	Time taken saving stuff: 0.00s

=== episode:57 Env-steps-taken:63168
action_counts: {0: 6149, 1: 5478, 2: 8118, 3: 6138, 4: 7436, 5: 11149, 6: 7194, 7: 5995, 8: 5511}
picked:  60
episode: 57/2000 -> reward: 78.68750000000003, steps:63168, time-taken: 2.21min, time-elasped: 116.57min
-> berries picked: 60 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2428 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [290, 299, 300, 193, 270, 303, 294, 370, 109]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 19, 14, 9, 14, 5, 14, 10, 6]
	Time taken saving stuff: 0.00s

=== episode:58 Env-steps-taken:62496
action_counts: {0: 6325, 1: 4158, 2: 6600, 3: 8833, 4: 6655, 5: 8712, 6: 7106, 7: 9350, 8: 3919}
picked:  61
episode: 58/2000 -> reward: 75.18229166666666, steps:61658, time-taken: 2.05min, time-elasped: 118.62min
-> berries picked: 61 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2460 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [294, 298, 300, 196, 276, 313, 304, 372, 107]
	| approx positives in sample 512: 127
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 21, 24, 4, 10, 18, 13, 19, 5]
	Time taken saving stuff: 0.01s

=== episode:59 Env-steps-taken:63264
action_counts: {0: 7623, 1: 5929, 2: 7557, 3: 7117, 4: 5885, 5: 9067, 6: 6952, 7: 8734, 8: 4400}
picked:  59
episode: 59/2000 -> reward: 79.19270833333334, steps:63264, time-taken: 2.17min, time-elasped: 120.79min
-> berries picked: 59 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2494 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [298, 301, 300, 197, 276, 318, 315, 382, 107]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 14, 17, 14, 19, 9, 12, 14, 6]
	Time taken saving stuff: 0.00s

=== episode:60 Env-steps-taken:65568
action_counts: {0: 6732, 1: 5401, 2: 6479, 3: 7150, 4: 8426, 5: 9017, 6: 7425, 7: 9416, 8: 5357}
picked:  75
episode: 60/2000 -> reward: 91.10937500000009, steps:65403, time-taken: 2.10min, time-elasped: 122.89min
-> berries picked: 75 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2541 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [306, 305, 299, 198, 285, 333, 322, 387, 106]
	| approx positives in sample 512: 102
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 17, 10, 16, 13, 12, 13, 6, 7]
	Time taken saving stuff: 0.05s

=== episode:6 Env-steps-taken:48864
action_counts: {0: 0, 1: 44, 2: 0, 3: 24244, 4: 77, 5: 143, 6: 99, 7: 22, 8: 24235}
picked:  3

==================================================
eval-episode: 60 -> reward: 4.484375, steps: 48864.0, wall-time: 38.61s
-> berries picked: 3 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:61 Env-steps-taken:64416
action_counts: {0: 5599, 1: 5269, 2: 6721, 3: 6798, 4: 5610, 5: 8855, 6: 7568, 7: 6457, 8: 5292}
picked:  66
episode: 61/2000 -> reward: 85.15625000000003, steps:58169, time-taken: 1.95min, time-elasped: 125.49min
-> berries picked: 66 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2575 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [305, 305, 305, 196, 292, 336, 339, 390, 107]
	| approx positives in sample 512: 118
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [17, 15, 21, 6, 12, 10, 17, 16, 4]
	Time taken saving stuff: 0.00s

=== episode:62 Env-steps-taken:66048
action_counts: {0: 6149, 1: 4917, 2: 6776, 3: 6930, 4: 7051, 5: 7260, 6: 8382, 7: 5819, 8: 3763}
picked:  74
episode: 62/2000 -> reward: 93.6145833333334, steps:57047, time-taken: 1.95min, time-elasped: 127.44min
-> berries picked: 74 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2615 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [310, 307, 310, 201, 299, 342, 343, 397, 106]
	| approx positives in sample 512: 113
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 15, 13, 8, 12, 15, 14, 13, 9]
	Time taken saving stuff: 0.00s

=== episode:63 Env-steps-taken:64896
action_counts: {0: 7183, 1: 8558, 2: 8382, 3: 7744, 4: 5423, 5: 5500, 6: 8191, 7: 8657, 8: 5258}
picked:  68
episode: 63/2000 -> reward: 87.64583333333337, steps:64896, time-taken: 2.30min, time-elasped: 129.74min
-> berries picked: 68 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2650 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [312, 314, 315, 205, 307, 343, 339, 406, 109]
	| approx positives in sample 512: 115
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 26, 11, 11, 19, 13, 11, 12, 2]
	Time taken saving stuff: 0.00s

=== episode:64 Env-steps-taken:66528
action_counts: {0: 6622, 1: 5876, 2: 5962, 3: 6578, 4: 5027, 5: 5467, 6: 8789, 7: 8327, 8: 3542}
picked:  76
episode: 64/2000 -> reward: 97.10416666666676, steps:56190, time-taken: 1.91min, time-elasped: 131.65min
-> berries picked: 76 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2697 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [319, 316, 318, 210, 312, 347, 350, 416, 109]
	| approx positives in sample 512: 117
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 17, 17, 8, 17, 6, 19, 12, 6]
	Time taken saving stuff: 0.00s

=== episode:65 Env-steps-taken:66912
action_counts: {0: 5643, 1: 6226, 2: 8305, 3: 7271, 4: 6930, 5: 5929, 6: 6369, 7: 5566, 8: 3334}
picked:  78
episode: 65/2000 -> reward: 98.0937500000001, steps:55573, time-taken: 1.91min, time-elasped: 133.56min
-> berries picked: 78 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2748 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [316, 323, 327, 214, 320, 357, 359, 421, 111]
	| approx positives in sample 512: 107
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 12, 16, 10, 7, 16, 23, 12, 4]
	Time taken saving stuff: 0.00s

=== episode:66 Env-steps-taken:66336
action_counts: {0: 4851, 1: 5907, 2: 7359, 3: 8019, 4: 6314, 5: 4719, 6: 5918, 7: 5907, 8: 3217}
picked:  71
episode: 66/2000 -> reward: 96.13020833333341, steps:52211, time-taken: 1.80min, time-elasped: 135.36min
-> berries picked: 71 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2790 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [322, 331, 331, 217, 321, 356, 371, 428, 113]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [5, 17, 12, 11, 13, 14, 11, 16, 5]
	Time taken saving stuff: 0.00s

=== episode:67 Env-steps-taken:66240
action_counts: {0: 5599, 1: 6369, 2: 7238, 3: 8492, 4: 7040, 5: 5566, 6: 8910, 7: 6534, 8: 3466}
picked:  74
episode: 67/2000 -> reward: 94.61458333333341, steps:59214, time-taken: 1.99min, time-elasped: 137.36min
-> berries picked: 74 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2825 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [327, 335, 339, 224, 321, 359, 378, 430, 112]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 14, 22, 10, 11, 11, 27, 15, 1]
	Time taken saving stuff: 0.00s

=== episode:68 Env-steps-taken:56448
action_counts: {0: 4752, 1: 9449, 2: 5632, 3: 5412, 4: 3586, 5: 4939, 6: 4015, 7: 4444, 8: 6106}
picked:  35
episode: 68/2000 -> reward: 43.817708333333314, steps:48335, time-taken: 1.60min, time-elasped: 138.96min
-> berries picked: 35 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2816 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [327, 335, 344, 219, 316, 355, 376, 432, 112]
	| approx positives in sample 512: 124
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 16, 13, 14, 18, 8, 21, 15, 5]
	Time taken saving stuff: 0.00s

=== episode:69 Env-steps-taken:59520
action_counts: {0: 5467, 1: 4180, 2: 6501, 3: 6471, 4: 4048, 5: 4840, 6: 5137, 7: 6248, 8: 3355}
picked:  42
episode: 69/2000 -> reward: 60.78124999999997, steps:46247, time-taken: 1.70min, time-elasped: 140.66min
-> berries picked: 42 of 800 | patches-visited: [0, 9] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2826 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [326, 336, 349, 221, 318, 357, 372, 433, 114]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 14, 21, 9, 15, 8, 14, 11, 4]
	Time taken saving stuff: 0.00s

=== episode:70 Env-steps-taken:64992
action_counts: {0: 6039, 1: 5368, 2: 6897, 3: 10131, 4: 8019, 5: 5896, 6: 6666, 7: 7304, 8: 4182}
picked:  73
episode: 70/2000 -> reward: 88.11979166666673, steps:60502, time-taken: 2.08min, time-elasped: 142.75min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2852 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [335, 336, 356, 224, 317, 360, 379, 433, 112]
	| approx positives in sample 512: 121
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 12, 15, 16, 12, 17, 14, 22, 5]
	Time taken saving stuff: 0.05s

=== episode:7 Env-steps-taken:52128
action_counts: {0: 3256, 1: 110, 2: 16686, 3: 44, 4: 7392, 5: 8349, 6: 913, 7: 187, 8: 15191}
picked:  17

==================================================
eval-episode: 70 -> reward: 21.411458333333336, steps: 52128.0, wall-time: 34.85s
-> berries picked: 17 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:71 Env-steps-taken:64320
action_counts: {0: 5731, 1: 5423, 2: 7106, 3: 9416, 4: 7605, 5: 9889, 6: 7293, 7: 5621, 8: 5555}
picked:  60
episode: 71/2000 -> reward: 84.68750000000003, steps:63639, time-taken: 2.12min, time-elasped: 145.45min
-> berries picked: 60 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2872 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [340, 336, 358, 232, 310, 361, 389, 434, 112]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 14, 19, 9, 13, 15, 17, 12, 8]
	Time taken saving stuff: 0.01s

=== episode:72 Env-steps-taken:64320
action_counts: {0: 6182, 1: 5159, 2: 7205, 3: 6468, 4: 7106, 5: 11429, 6: 7186, 7: 5247, 8: 8338}
picked:  60
episode: 72/2000 -> reward: 84.68750000000001, steps:64320, time-taken: 2.21min, time-elasped: 147.67min
-> berries picked: 60 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2876 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [349, 335, 366, 236, 308, 355, 385, 430, 112]
	| approx positives in sample 512: 116
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [21, 14, 15, 12, 17, 8, 15, 13, 1]
	Time taken saving stuff: 0.00s

=== episode:73 Env-steps-taken:71136
action_counts: {0: 7777, 1: 5841, 2: 8213, 3: 7876, 4: 7337, 5: 7249, 6: 9306, 7: 7271, 8: 4587}
picked:  81
episode: 73/2000 -> reward: 121.07812500000013, steps:65457, time-taken: 2.18min, time-elasped: 149.84min
-> berries picked: 81 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2893 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [351, 336, 372, 237, 306, 353, 391, 435, 112]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 16, 18, 12, 9, 15, 14, 24, 3]
	Time taken saving stuff: 0.00s

=== episode:74 Env-steps-taken:67488
action_counts: {0: 5819, 1: 5665, 2: 5830, 3: 7062, 4: 7161, 5: 8041, 6: 8784, 7: 6490, 8: 5346}
picked:  73
episode: 74/2000 -> reward: 101.11979166666679, steps:60198, time-taken: 2.07min, time-elasped: 151.91min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2911 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [350, 335, 381, 240, 302, 356, 398, 434, 115]
	| approx positives in sample 512: 99
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 10, 17, 11, 13, 13, 17, 5, 2]
	Time taken saving stuff: 0.00s

=== episode:75 Env-steps-taken:64032
action_counts: {0: 7403, 1: 6182, 2: 7876, 3: 10467, 4: 6501, 5: 6391, 6: 8503, 7: 6314, 8: 4037}
picked:  69
episode: 75/2000 -> reward: 83.14062500000004, steps:63674, time-taken: 2.14min, time-elasped: 154.05min
-> berries picked: 69 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2922 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [354, 333, 385, 241, 303, 357, 403, 431, 115]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 14, 25, 5, 13, 10, 12, 10, 5]
	Time taken saving stuff: 0.01s

=== episode:76 Env-steps-taken:63360
action_counts: {0: 7073, 1: 5819, 2: 7524, 3: 7766, 4: 7645, 5: 6391, 6: 7007, 7: 4939, 8: 3719}
picked:  63
episode: 76/2000 -> reward: 79.67187500000003, steps:57883, time-taken: 1.93min, time-elasped: 155.98min
-> berries picked: 63 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2947 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [359, 336, 389, 247, 304, 359, 410, 426, 117]
	| approx positives in sample 512: 122
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 15, 16, 11, 11, 15, 19, 17, 7]
	Time taken saving stuff: 0.01s

=== episode:77 Env-steps-taken:59520
action_counts: {0: 9625, 1: 4477, 2: 4840, 3: 7106, 4: 11616, 5: 4807, 6: 7458, 7: 6368, 8: 3223}
picked:  46
episode: 77/2000 -> reward: 59.76041666666662, steps:59520, time-taken: 2.05min, time-elasped: 158.03min
-> berries picked: 46 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2940 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [355, 337, 394, 245, 301, 358, 408, 425, 117]
	| approx positives in sample 512: 112
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 12, 18, 13, 12, 8, 10, 20, 8]
	Time taken saving stuff: 0.00s

=== episode:78 Env-steps-taken:64704
action_counts: {0: 7045, 1: 5852, 2: 6479, 3: 7656, 4: 10054, 5: 6776, 6: 8052, 7: 7447, 8: 4356}
picked:  71
episode: 78/2000 -> reward: 86.6302083333334, steps:63717, time-taken: 2.12min, time-elasped: 160.15min
-> berries picked: 71 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2954 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [357, 336, 398, 253, 307, 354, 406, 425, 118]
	| approx positives in sample 512: 119
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 10, 21, 12, 12, 12, 13, 17, 9]
	Time taken saving stuff: 0.01s

=== episode:79 Env-steps-taken:64992
action_counts: {0: 6567, 1: 5005, 2: 7403, 3: 7766, 4: 6688, 5: 7106, 6: 7348, 7: 7315, 8: 3619}
picked:  64
episode: 79/2000 -> reward: 89.1666666666667, steps:58817, time-taken: 2.00min, time-elasped: 162.15min
-> berries picked: 64 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 2972 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [361, 342, 401, 257, 299, 357, 404, 429, 122]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 7, 18, 5, 13, 8, 20, 12, 3]
	Time taken saving stuff: 0.00s

=== episode:80 Env-steps-taken:68928
action_counts: {0: 7458, 1: 6501, 2: 7238, 3: 6886, 4: 7852, 5: 7524, 6: 8789, 7: 7821, 8: 4202}
picked:  89
episode: 80/2000 -> reward: 109.53645833333344, steps:64271, time-taken: 2.21min, time-elasped: 164.36min
-> berries picked: 89 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3008 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [371, 341, 406, 261, 303, 363, 409, 430, 124]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 15, 12, 9, 16, 12, 8, 9, 4]
	Time taken saving stuff: 0.06s

=== episode:8 Env-steps-taken:59232
action_counts: {0: 25036, 1: 231, 2: 3091, 3: 1408, 4: 23405, 5: 1276, 6: 1848, 7: 1375, 8: 1562}
picked:  44

==================================================
eval-episode: 80 -> reward: 58.2708333333333, steps: 59232.0, wall-time: 36.39s
-> berries picked: 44 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:81 Env-steps-taken:66144
action_counts: {0: 6677, 1: 7766, 2: 6061, 3: 7678, 4: 8910, 5: 8987, 6: 8254, 7: 5049, 8: 5797}
picked:  74
episode: 81/2000 -> reward: 94.11458333333341, steps:65179, time-taken: 2.17min, time-elasped: 167.14min
-> berries picked: 74 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3019 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [371, 345, 413, 261, 304, 366, 409, 426, 124]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 16, 11, 6, 13, 8, 9, 14, 3]
	Time taken saving stuff: 0.00s

=== episode:82 Env-steps-taken:64992
action_counts: {0: 6490, 1: 6116, 2: 7843, 3: 8723, 4: 7975, 5: 8283, 6: 7361, 7: 7370, 8: 4037}
picked:  66
episode: 82/2000 -> reward: 88.15625000000006, steps:64198, time-taken: 2.11min, time-elasped: 169.25min
-> berries picked: 66 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3010 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [368, 347, 411, 263, 300, 356, 412, 430, 123]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 5, 13, 12, 12, 12, 11, 16, 8]
	Time taken saving stuff: 0.00s

=== episode:83 Env-steps-taken:68736
action_counts: {0: 5764, 1: 7755, 2: 7117, 3: 9963, 4: 6776, 5: 6809, 6: 8151, 7: 7590, 8: 3674}
picked:  79
episode: 83/2000 -> reward: 108.58854166666676, steps:63599, time-taken: 2.16min, time-elasped: 171.41min
-> berries picked: 79 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3032 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [367, 352, 412, 274, 299, 356, 413, 436, 123]
	| approx positives in sample 512: 84
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 15, 12, 8, 11, 7, 8, 11, 3]
	Time taken saving stuff: 0.00s

=== episode:84 Env-steps-taken:72960
action_counts: {0: 7692, 1: 6765, 2: 7062, 3: 8426, 4: 8899, 5: 7689, 6: 6732, 7: 7634, 8: 4884}
picked:  93
episode: 84/2000 -> reward: 130.51562500000014, steps:65783, time-taken: 2.22min, time-elasped: 173.63min
-> berries picked: 93 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3043 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [371, 359, 417, 275, 295, 355, 412, 432, 127]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 17, 12, 9, 12, 13, 8, 13, 5]
	Time taken saving stuff: 0.00s

=== episode:85 Env-steps-taken:65184
action_counts: {0: 7299, 1: 7502, 2: 6545, 3: 7040, 4: 7095, 5: 7777, 6: 9174, 7: 6336, 8: 3883}
picked:  64
episode: 85/2000 -> reward: 90.1666666666667, steps:62651, time-taken: 2.25min, time-elasped: 175.88min
-> berries picked: 64 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3039 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [373, 358, 417, 275, 292, 350, 419, 426, 129]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 12, 16, 5, 10, 11, 13, 12, 4]
	Time taken saving stuff: 0.00s

=== episode:86 Env-steps-taken:64992
action_counts: {0: 6259, 1: 9966, 2: 7700, 3: 8657, 4: 6072, 5: 5511, 6: 7029, 7: 8162, 8: 5071}
picked:  65
episode: 86/2000 -> reward: 88.1614583333334, steps:64427, time-taken: 2.09min, time-elasped: 177.97min
-> berries picked: 65 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3041 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [379, 365, 417, 275, 285, 346, 416, 430, 128]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 13, 10, 8, 9, 6, 18, 17, 5]
	Time taken saving stuff: 0.00s

=== episode:87 Env-steps-taken:69312
action_counts: {0: 7447, 1: 7444, 2: 8338, 3: 8294, 4: 8327, 5: 6325, 6: 7711, 7: 6039, 8: 3773}
picked:  88
episode: 87/2000 -> reward: 111.54166666666681, steps:63698, time-taken: 2.24min, time-elasped: 180.21min
-> berries picked: 88 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3049 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [395, 374, 422, 272, 279, 342, 414, 423, 128]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 16, 17, 7, 9, 13, 7, 8, 5]
	Time taken saving stuff: 0.00s

=== episode:88 Env-steps-taken:64704
action_counts: {0: 7535, 1: 8778, 2: 8459, 3: 8602, 4: 5687, 5: 6149, 6: 6864, 7: 6050, 8: 3686}
picked:  73
episode: 88/2000 -> reward: 86.61979166666669, steps:61810, time-taken: 2.33min, time-elasped: 182.55min
-> berries picked: 73 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3034 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [394, 382, 421, 265, 274, 338, 408, 425, 127]
	| approx positives in sample 512: 104
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 12, 17, 16, 8, 5, 10, 19, 3]
	Time taken saving stuff: 0.01s

=== episode:89 Env-steps-taken:75648
action_counts: {0: 8800, 1: 8151, 2: 8701, 3: 9977, 4: 7832, 5: 7216, 6: 8462, 7: 8305, 8: 4202}
picked:  108
episode: 89/2000 -> reward: 143.4427083333334, steps:71646, time-taken: 2.39min, time-elasped: 184.94min
-> berries picked: 108 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3053 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [395, 395, 419, 272, 270, 337, 413, 427, 125]
	| approx positives in sample 512: 109
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 15, 14, 5, 11, 5, 24, 12, 5]
	Time taken saving stuff: 0.00s

=== episode:90 Env-steps-taken:62688
action_counts: {0: 7380, 1: 6853, 2: 7062, 3: 10307, 4: 8228, 5: 5929, 6: 7260, 7: 5698, 8: 3971}
picked:  64
episode: 90/2000 -> reward: 76.16666666666667, steps:62688, time-taken: 2.03min, time-elasped: 186.97min
-> berries picked: 64 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3048 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [397, 406, 421, 271, 267, 330, 413, 415, 128]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 15, 13, 6, 11, 11, 14, 8, 5]
	Time taken saving stuff: 0.05s

=== episode:9 Env-steps-taken:67200
action_counts: {0: 6413, 1: 4532, 2: 3190, 3: 39535, 4: 1606, 5: 5973, 6: 3080, 7: 2343, 8: 528}
picked:  80

==================================================
eval-episode: 90 -> reward: 99.58333333333343, steps: 67200.0, wall-time: 35.29s
-> berries picked: 80 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:91 Env-steps-taken:69600
action_counts: {0: 6116, 1: 7161, 2: 8151, 3: 8954, 4: 8481, 5: 6446, 6: 6864, 7: 6138, 8: 4022}
picked:  84
episode: 91/2000 -> reward: 113.06250000000014, steps:62333, time-taken: 2.06min, time-elasped: 189.62min
-> berries picked: 84 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3057 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [395, 407, 420, 285, 278, 328, 405, 409, 130]
	| approx positives in sample 512: 93
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 10, 12, 12, 8, 12, 14, 11, 7]
	Time taken saving stuff: 0.00s

=== episode:92 Env-steps-taken:62496
action_counts: {0: 5610, 1: 6732, 2: 6897, 3: 7964, 4: 6341, 5: 6721, 6: 5632, 7: 6721, 8: 3916}
picked:  56
episode: 92/2000 -> reward: 75.20833333333331, steps:56534, time-taken: 1.93min, time-elasped: 191.56min
-> berries picked: 56 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3042 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [391, 410, 418, 288, 277, 319, 400, 410, 129]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 13, 19, 9, 3, 15, 12, 12, 9]
	Time taken saving stuff: 0.00s

=== episode:93 Env-steps-taken:71808
action_counts: {0: 8602, 1: 8910, 2: 7392, 3: 6644, 4: 7579, 5: 7007, 6: 9042, 7: 6138, 8: 3961}
picked:  99
episode: 93/2000 -> reward: 124.4843750000002, steps:65275, time-taken: 2.25min, time-elasped: 193.81min
-> berries picked: 99 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3078 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [393, 425, 419, 291, 275, 321, 405, 422, 127]
	| approx positives in sample 512: 98
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 6, 17, 10, 5, 13, 13, 14, 6]
	Time taken saving stuff: 0.01s

=== episode:94 Env-steps-taken:66816
action_counts: {0: 6105, 1: 7546, 2: 5434, 3: 6347, 4: 7755, 5: 6435, 6: 7315, 7: 5621, 8: 3168}
picked:  79
episode: 94/2000 -> reward: 98.58854166666673, steps:55726, time-taken: 1.92min, time-elasped: 195.73min
-> berries picked: 79 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3074 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [400, 423, 420, 287, 273, 320, 399, 425, 127]
	| approx positives in sample 512: 93
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 15, 10, 11, 9, 6, 12, 14, 8]
	Time taken saving stuff: 0.01s

=== episode:95 Env-steps-taken:69408
action_counts: {0: 6985, 1: 9350, 2: 7766, 3: 6237, 4: 7304, 5: 7238, 6: 8327, 7: 8305, 8: 4741}
picked:  72
episode: 95/2000 -> reward: 112.1250000000001, steps:66253, time-taken: 2.22min, time-elasped: 197.96min
-> berries picked: 72 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3072 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [398, 433, 414, 292, 271, 315, 396, 428, 125]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 10, 23, 9, 7, 14, 14, 11, 7]
	Time taken saving stuff: 0.01s

=== episode:96 Env-steps-taken:67680
action_counts: {0: 8162, 1: 8701, 2: 6715, 3: 8294, 4: 5830, 5: 5335, 6: 9504, 7: 6809, 8: 3982}
picked:  79
episode: 96/2000 -> reward: 103.08854166666677, steps:63332, time-taken: 2.25min, time-elasped: 200.21min
-> berries picked: 79 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3060 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [409, 438, 406, 288, 271, 303, 396, 424, 125]
	| approx positives in sample 512: 88
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 16, 10, 8, 8, 7, 11, 15, 2]
	Time taken saving stuff: 0.00s

=== episode:97 Env-steps-taken:61536
action_counts: {0: 5357, 1: 7975, 2: 7568, 3: 7799, 4: 8679, 5: 6974, 6: 6919, 7: 5942, 8: 4323}
picked:  51
episode: 97/2000 -> reward: 70.234375, steps:61536, time-taken: 2.11min, time-elasped: 202.33min
-> berries picked: 51 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3032 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [409, 441, 403, 285, 269, 294, 386, 422, 123]
	| approx positives in sample 512: 84
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 15, 9, 7, 6, 6, 13, 15, 3]
	Time taken saving stuff: 0.00s

=== episode:98 Env-steps-taken:68544
action_counts: {0: 4521, 1: 7392, 2: 7521, 3: 8349, 4: 5203, 5: 6556, 6: 8041, 7: 5401, 8: 3278}
picked:  77
episode: 98/2000 -> reward: 107.59895833333344, steps:56262, time-taken: 2.00min, time-elasped: 204.32min
-> berries picked: 77 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3045 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [407, 442, 403, 301, 270, 293, 388, 416, 125]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 18, 11, 7, 7, 10, 14, 16, 5]
	Time taken saving stuff: 0.01s

=== episode:99 Env-steps-taken:65760
action_counts: {0: 4983, 1: 9515, 2: 5357, 3: 6347, 4: 6009, 5: 5324, 6: 6358, 7: 5841, 8: 2926}
picked:  71
episode: 99/2000 -> reward: 92.13020833333341, steps:52660, time-taken: 1.82min, time-elasped: 206.14min
-> berries picked: 71 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3050 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [411, 453, 405, 297, 265, 290, 389, 415, 125]
	| approx positives in sample 512: 95
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [19, 12, 13, 10, 6, 5, 14, 11, 5]
	Time taken saving stuff: 0.00s

=== episode:100 Env-steps-taken:75168
action_counts: {0: 6501, 1: 9042, 2: 7161, 3: 9987, 4: 8371, 5: 7260, 6: 9570, 7: 7986, 8: 3993}
picked:  103
episode: 100/2000 -> reward: 141.96354166666669, steps:69871, time-taken: 2.37min, time-elasped: 208.51min
-> berries picked: 103 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3065 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [410, 462, 404, 298, 268, 281, 397, 420, 125]
	| approx positives in sample 512: 89
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 9, 12, 10, 12, 9, 9, 13, 2]
	Time taken saving stuff: 0.07s

=== episode:10 Env-steps-taken:80160
action_counts: {0: 6512, 1: 2145, 2: 18579, 3: 3201, 4: 7216, 5: 32310, 6: 3278, 7: 6919, 8: 0}
picked:  126

==================================================
eval-episode: 100 -> reward: 167.8437499999999, steps: 80160.0, wall-time: 46.30s
-> berries picked: 126 of 800 | patches-visited: [0, 1] | juice left:-0.00
==================================================


=== episode:101 Env-steps-taken:66816
action_counts: {0: 6072, 1: 6171, 2: 5929, 3: 8690, 4: 7480, 5: 9141, 6: 8613, 7: 6875, 8: 3708}
picked:  80
episode: 101/2000 -> reward: 97.58333333333344, steps:62679, time-taken: 2.22min, time-elasped: 211.50min
-> berries picked: 80 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3080 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [409, 467, 407, 303, 271, 283, 396, 417, 127]
	| approx positives in sample 512: 90
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 20, 14, 9, 7, 9, 8, 9, 2]
	Time taken saving stuff: 0.00s

=== episode:102 Env-steps-taken:73824
action_counts: {0: 6006, 1: 9075, 2: 5951, 3: 8470, 4: 7766, 5: 6039, 6: 7950, 7: 6798, 8: 3531}
picked:  87
episode: 102/2000 -> reward: 135.0468750000001, steps:61586, time-taken: 2.28min, time-elasped: 213.78min
-> berries picked: 87 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3100 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [413, 478, 406, 303, 276, 283, 392, 419, 130]
	| approx positives in sample 512: 83
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 12, 8, 12, 8, 8, 5, 19, 4]
	Time taken saving stuff: 0.01s

=== episode:103 Env-steps-taken:66528
action_counts: {0: 6578, 1: 7909, 2: 7315, 3: 8118, 4: 6941, 5: 8162, 6: 9427, 7: 7535, 8: 3268}
picked:  77
episode: 103/2000 -> reward: 96.09895833333343, steps:65253, time-taken: 2.25min, time-elasped: 216.04min
-> berries picked: 77 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3114 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [416, 488, 406, 301, 274, 277, 394, 426, 132]
	| approx positives in sample 512: 111
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [18, 15, 14, 9, 11, 8, 11, 20, 5]
	Time taken saving stuff: 0.00s

=== episode:104 Env-steps-taken:78144
action_counts: {0: 8151, 1: 9416, 2: 6842, 3: 9394, 4: 9189, 5: 9306, 6: 9152, 7: 7810, 8: 4081}
picked:  111
episode: 104/2000 -> reward: 157.421875, steps:73341, time-taken: 2.62min, time-elasped: 218.66min
-> berries picked: 111 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3160 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [425, 502, 406, 309, 287, 281, 389, 427, 134]
	| approx positives in sample 512: 90
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 15, 9, 9, 11, 6, 17, 9, 4]
	Time taken saving stuff: 0.01s

=== episode:105 Env-steps-taken:73536
action_counts: {0: 7139, 1: 8705, 2: 7799, 3: 6655, 4: 7040, 5: 7766, 6: 9526, 7: 7513, 8: 3740}
picked:  97
episode: 105/2000 -> reward: 133.4947916666668, steps:65883, time-taken: 2.39min, time-elasped: 221.06min
-> berries picked: 97 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3183 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [428, 502, 411, 310, 284, 293, 381, 438, 136]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 18, 12, 6, 9, 10, 20, 13, 5]
	Time taken saving stuff: 0.00s

=== episode:106 Env-steps-taken:77088
action_counts: {0: 6886, 1: 10439, 2: 7691, 3: 9339, 4: 9273, 5: 7425, 6: 10296, 7: 7909, 8: 4213}
picked:  109
episode: 106/2000 -> reward: 151.93229166666666, steps:73471, time-taken: 2.58min, time-elasped: 223.64min
-> berries picked: 109 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3193 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [429, 510, 414, 307, 283, 297, 379, 437, 137]
	| approx positives in sample 512: 88
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 23, 10, 11, 5, 5, 11, 13, 1]
	Time taken saving stuff: 0.00s

=== episode:107 Env-steps-taken:68352
action_counts: {0: 5049, 1: 8866, 2: 7095, 3: 8052, 4: 7656, 5: 8316, 6: 8976, 7: 6171, 8: 4577}
picked:  80
episode: 107/2000 -> reward: 105.58333333333343, steps:64758, time-taken: 2.55min, time-elasped: 226.19min
-> berries picked: 80 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3205 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [423, 518, 411, 311, 283, 303, 383, 433, 140]
	| approx positives in sample 512: 82
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 8, 8, 9, 5, 16, 10, 9, 6]
	Time taken saving stuff: 0.00s

=== episode:108 Env-steps-taken:66432
action_counts: {0: 6215, 1: 6831, 2: 6424, 3: 6182, 4: 7711, 5: 7150, 6: 8437, 7: 6996, 8: 3850}
picked:  74
episode: 108/2000 -> reward: 95.61458333333341, steps:59796, time-taken: 2.18min, time-elasped: 228.37min
-> berries picked: 74 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3206 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [418, 517, 410, 305, 292, 307, 379, 440, 138]
	| approx positives in sample 512: 85
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 12, 7, 7, 9, 11, 10, 14, 4]
	Time taken saving stuff: 0.01s

=== episode:109 Env-steps-taken:64128
action_counts: {0: 5808, 1: 7953, 2: 5687, 3: 5533, 4: 9647, 5: 7088, 6: 9779, 7: 8459, 8: 3740}
picked:  65
episode: 109/2000 -> reward: 83.66145833333339, steps:63694, time-taken: 2.29min, time-elasped: 230.66min
-> berries picked: 65 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3209 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [420, 515, 405, 298, 292, 314, 387, 438, 140]
	| approx positives in sample 512: 88
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 12, 11, 9, 12, 12, 12, 15, 1]
	Time taken saving stuff: 0.01s

=== episode:110 Env-steps-taken:65952
action_counts: {0: 5082, 1: 7568, 2: 5291, 3: 6314, 4: 7403, 5: 6930, 6: 9028, 7: 7854, 8: 3894}
picked:  78
episode: 110/2000 -> reward: 93.0937500000001, steps:59364, time-taken: 2.07min, time-elasped: 232.74min
-> berries picked: 78 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3229 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [416, 523, 406, 302, 293, 316, 394, 438, 141]
	| approx positives in sample 512: 71
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 13, 6, 7, 7, 8, 7, 13, 2]
	Time taken saving stuff: 0.05s

=== episode:11 Env-steps-taken:51360
action_counts: {0: 11, 1: 24838, 2: 968, 3: 220, 4: 319, 5: 24531, 6: 143, 7: 330, 8: 0}
picked:  12

==================================================
eval-episode: 110 -> reward: 17.4375, steps: 51360.0, wall-time: 36.01s
-> berries picked: 12 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:111 Env-steps-taken:65568
action_counts: {0: 5742, 1: 7997, 2: 6886, 3: 7337, 4: 6369, 5: 7689, 6: 5742, 7: 5643, 8: 3257}
picked:  80
episode: 111/2000 -> reward: 91.08333333333343, steps:56662, time-taken: 1.87min, time-elasped: 235.21min
-> berries picked: 80 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3247 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [422, 530, 399, 304, 287, 322, 399, 442, 142]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 7, 11, 6, 7, 12, 12, 21, 8]
	Time taken saving stuff: 0.00s

=== episode:112 Env-steps-taken:72384
action_counts: {0: 5684, 1: 7150, 2: 5951, 3: 6160, 4: 7337, 5: 6930, 6: 6457, 7: 8415, 8: 3575}
picked:  100
episode: 112/2000 -> reward: 127.47916666666684, steps:57659, time-taken: 1.98min, time-elasped: 237.19min
-> berries picked: 100 of 800 | patches-visited: [0, 6] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3277 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [428, 526, 399, 308, 295, 323, 404, 449, 145]
	| approx positives in sample 512: 79
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [4, 15, 12, 3, 4, 9, 15, 13, 4]
	Time taken saving stuff: 0.00s

=== episode:113 Env-steps-taken:73152
action_counts: {0: 6655, 1: 9113, 2: 7337, 3: 8052, 4: 8250, 5: 8591, 6: 8195, 7: 7821, 8: 3740}
picked:  100
episode: 113/2000 -> reward: 131.47916666666677, steps:67754, time-taken: 2.44min, time-elasped: 239.64min
-> berries picked: 100 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3300 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [425, 536, 405, 311, 295, 327, 403, 452, 146]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 14, 9, 9, 5, 12, 16, 19, 8]
	Time taken saving stuff: 0.01s

=== episode:114 Env-steps-taken:82944
action_counts: {0: 9768, 1: 11154, 2: 6897, 3: 9405, 4: 8668, 5: 8802, 6: 9306, 7: 9735, 8: 4136}
picked:  126
episode: 114/2000 -> reward: 182.3437499999998, steps:77871, time-taken: 2.64min, time-elasped: 242.28min
-> berries picked: 126 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3335 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [439, 540, 403, 315, 295, 332, 407, 456, 148]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 16, 7, 7, 4, 13, 17, 18, 2]
	Time taken saving stuff: 0.00s

=== episode:115 Env-steps-taken:66912
action_counts: {0: 6919, 1: 7095, 2: 5280, 3: 6490, 4: 7095, 5: 6006, 6: 8063, 7: 8118, 8: 3422}
picked:  77
episode: 115/2000 -> reward: 98.09895833333348, steps:58488, time-taken: 2.19min, time-elasped: 244.48min
-> berries picked: 77 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3338 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [441, 545, 403, 308, 294, 334, 406, 457, 150]
	| approx positives in sample 512: 94
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 14, 7, 9, 8, 12, 12, 10, 9]
	Time taken saving stuff: 0.01s

=== episode:116 Env-steps-taken:68160
action_counts: {0: 7491, 1: 9284, 2: 5918, 3: 6843, 4: 7744, 5: 5962, 6: 7601, 7: 7227, 8: 3476}
picked:  74
episode: 116/2000 -> reward: 105.61458333333346, steps:61546, time-taken: 2.27min, time-elasped: 246.74min
-> berries picked: 74 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3337 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [447, 541, 400, 311, 287, 331, 413, 457, 150]
	| approx positives in sample 512: 72
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 8, 11, 5, 9, 9, 8, 8, 3]
	Time taken saving stuff: 0.00s

=== episode:117 Env-steps-taken:66048
action_counts: {0: 6281, 1: 8250, 2: 6446, 3: 4818, 4: 5368, 5: 4884, 6: 5456, 7: 5643, 8: 2893}
picked:  60
episode: 117/2000 -> reward: 94.68750000000006, steps:50039, time-taken: 1.80min, time-elasped: 248.55min
-> berries picked: 60 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3346 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [447, 547, 401, 313, 289, 340, 402, 458, 149]
	| approx positives in sample 512: 88
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 10, 16, 8, 4, 6, 14, 18, 3]
	Time taken saving stuff: 0.00s

=== episode:118 Env-steps-taken:71712
action_counts: {0: 6127, 1: 8184, 2: 7040, 3: 7348, 4: 8041, 5: 5148, 6: 7535, 7: 6375, 8: 3454}
picked:  91
episode: 118/2000 -> reward: 124.02604166666684, steps:59252, time-taken: 2.00min, time-elasped: 250.55min
-> berries picked: 91 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3394 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [450, 568, 401, 318, 297, 345, 405, 458, 152]
	| approx positives in sample 512: 78
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [15, 12, 10, 5, 6, 6, 6, 11, 7]
	Time taken saving stuff: 0.00s

=== episode:119 Env-steps-taken:69504
action_counts: {0: 7612, 1: 8162, 2: 5522, 3: 8789, 4: 8863, 5: 6732, 6: 6996, 7: 7590, 8: 3817}
picked:  87
episode: 119/2000 -> reward: 112.05208333333346, steps:64083, time-taken: 2.27min, time-elasped: 252.82min
-> berries picked: 87 of 800 | patches-visited: [0, 3] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3402 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [446, 574, 402, 321, 298, 344, 402, 466, 149]
	| approx positives in sample 512: 93
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 14, 8, 6, 10, 13, 10, 20, 1]
	Time taken saving stuff: 0.00s

=== episode:120 Env-steps-taken:67680
action_counts: {0: 6336, 1: 8690, 2: 5698, 3: 6404, 4: 8151, 5: 6028, 6: 7183, 7: 6875, 8: 3707}
picked:  84
episode: 120/2000 -> reward: 103.06250000000011, steps:59072, time-taken: 1.97min, time-elasped: 254.79min
-> berries picked: 84 of 800 | patches-visited: [0, 5] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3425 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [453, 586, 401, 320, 301, 344, 405, 468, 147]
	| approx positives in sample 512: 91
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [17, 12, 9, 6, 11, 13, 6, 11, 6]
	Time taken saving stuff: 0.05s

=== episode:12 Env-steps-taken:82944
action_counts: {0: 9350, 1: 16555, 2: 5500, 3: 2211, 4: 24897, 5: 3179, 6: 19492, 7: 1760, 8: 0}
picked:  134

==================================================
eval-episode: 120 -> reward: 181.8124999999997, steps: 82944.0, wall-time: 44.28s
-> berries picked: 134 of 800 | patches-visited: [1, 3] | juice left:-0.00
==================================================


=== episode:121 Env-steps-taken:69312
action_counts: {0: 8063, 1: 8789, 2: 6490, 3: 7106, 4: 6413, 5: 6182, 6: 7854, 7: 9515, 8: 4126}
picked:  80
episode: 121/2000 -> reward: 110.58333333333348, steps:64538, time-taken: 2.13min, time-elasped: 257.66min
-> berries picked: 80 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3428 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [458, 587, 404, 314, 291, 345, 405, 476, 148]
	| approx positives in sample 512: 91
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 11, 8, 9, 11, 6, 16, 13, 3]
	Time taken saving stuff: 0.00s

=== episode:122 Env-steps-taken:69024
action_counts: {0: 5643, 1: 7293, 2: 6215, 3: 8107, 4: 7502, 5: 5962, 6: 6578, 7: 5368, 8: 3334}
picked:  80
episode: 122/2000 -> reward: 109.0833333333335, steps:56002, time-taken: 1.98min, time-elasped: 259.64min
-> berries picked: 80 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3455 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [461, 590, 409, 317, 296, 346, 408, 475, 153]
	| approx positives in sample 512: 96
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 12, 5, 6, 12, 9, 16, 17, 5]
	Time taken saving stuff: 0.00s

=== episode:123 Env-steps-taken:66912
action_counts: {0: 5357, 1: 9097, 2: 8184, 3: 7656, 4: 8162, 5: 6402, 6: 5863, 7: 7842, 8: 3531}
picked:  76
episode: 123/2000 -> reward: 98.10416666666679, steps:62094, time-taken: 2.26min, time-elasped: 261.90min
-> berries picked: 76 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3457 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [460, 593, 413, 313, 302, 345, 403, 472, 156]
	| approx positives in sample 512: 80
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [17, 17, 9, 6, 6, 5, 6, 13, 1]
	Time taken saving stuff: 0.01s

=== episode:124 Env-steps-taken:69024
action_counts: {0: 5588, 1: 7854, 2: 7931, 3: 8162, 4: 8248, 5: 7161, 6: 6050, 7: 7194, 8: 4092}
picked:  83
episode: 124/2000 -> reward: 110.06770833333344, steps:62280, time-taken: 2.27min, time-elasped: 264.17min
-> berries picked: 83 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3460 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [446, 594, 416, 316, 312, 349, 397, 473, 157]
	| approx positives in sample 512: 92
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 21, 14, 9, 6, 8, 14, 10, 1]
	Time taken saving stuff: 0.00s

=== episode:125 Env-steps-taken:65856
action_counts: {0: 7304, 1: 9130, 2: 6776, 3: 6534, 4: 6303, 5: 6402, 6: 6149, 7: 6699, 8: 2883}
picked:  64
episode: 125/2000 -> reward: 92.66666666666671, steps:58180, time-taken: 2.07min, time-elasped: 266.24min
-> berries picked: 64 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3470 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [450, 598, 418, 322, 305, 351, 395, 474, 157]
	| approx positives in sample 512: 87
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 18, 8, 5, 7, 6, 8, 17, 5]
	Time taken saving stuff: 0.00s

=== episode:126 Env-steps-taken:66240
action_counts: {0: 6391, 1: 9152, 2: 7029, 3: 9691, 4: 8008, 5: 6507, 6: 8437, 7: 5709, 8: 3982}
picked:  74
episode: 126/2000 -> reward: 94.61458333333343, steps:64906, time-taken: 2.52min, time-elasped: 268.77min
-> berries picked: 74 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3480 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [454, 602, 425, 317, 306, 359, 387, 472, 158]
	| approx positives in sample 512: 105
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [13, 20, 15, 11, 12, 4, 10, 17, 3]
	Time taken saving stuff: 0.00s

=== episode:127 Env-steps-taken:68352
action_counts: {0: 8569, 1: 9152, 2: 6545, 3: 7249, 4: 7986, 5: 6798, 6: 5676, 7: 8393, 8: 3490}
picked:  78
episode: 127/2000 -> reward: 105.59375000000014, steps:63858, time-taken: 2.38min, time-elasped: 271.15min
-> berries picked: 78 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3508 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [459, 611, 427, 319, 313, 354, 388, 479, 158]
	| approx positives in sample 512: 100
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 19, 10, 7, 11, 9, 11, 18, 3]
	Time taken saving stuff: 0.00s

=== episode:128 Env-steps-taken:66144
action_counts: {0: 5874, 1: 6556, 2: 6534, 3: 5887, 4: 5390, 5: 5533, 6: 5753, 7: 6501, 8: 3146}
picked:  71
episode: 128/2000 -> reward: 95.1302083333334, steps:51174, time-taken: 1.95min, time-elasped: 273.11min
-> berries picked: 71 of 800 | patches-visited: [0, 7] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3518 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [463, 616, 431, 321, 309, 353, 391, 476, 158]
	| approx positives in sample 512: 108
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 22, 12, 8, 10, 9, 16, 11, 4]
	Time taken saving stuff: 0.00s

=== episode:129 Env-steps-taken:69696
action_counts: {0: 7744, 1: 6842, 2: 6963, 3: 5500, 4: 7007, 5: 4719, 6: 4545, 7: 5368, 8: 3102}
picked:  82
episode: 129/2000 -> reward: 113.57291666666679, steps:51790, time-taken: 2.12min, time-elasped: 275.23min
-> berries picked: 82 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3543 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [466, 625, 439, 318, 310, 357, 390, 476, 162]
	| approx positives in sample 512: 93
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 18, 6, 9, 9, 8, 13, 12, 6]
	Time taken saving stuff: 0.00s

=== episode:130 Env-steps-taken:61440
action_counts: {0: 4909, 1: 5962, 2: 5192, 3: 3806, 4: 4191, 5: 3608, 6: 3245, 7: 3432, 8: 1617}
picked:  52
episode: 130/2000 -> reward: 70.72916666666666, steps:35962, time-taken: 1.52min, time-elasped: 276.75min
-> berries picked: 52 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3562 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [469, 632, 443, 320, 311, 359, 390, 476, 162]
	| approx positives in sample 512: 77
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 15, 9, 4, 5, 10, 11, 6, 6]
	Time taken saving stuff: 0.05s

=== episode:13 Env-steps-taken:54336
action_counts: {0: 20064, 1: 1155, 2: 1881, 3: 9313, 4: 20295, 5: 506, 6: 605, 7: 517, 8: 0}
picked:  22

==================================================
eval-episode: 130 -> reward: 32.88541666666667, steps: 54336.0, wall-time: 36.79s
-> berries picked: 22 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:131 Env-steps-taken:59616
action_counts: {0: 4774, 1: 5863, 2: 5709, 3: 7326, 4: 6633, 5: 4686, 6: 3773, 7: 3573, 8: 2387}
picked:  44
episode: 131/2000 -> reward: 60.2708333333333, steps:44724, time-taken: 1.66min, time-elasped: 279.03min
-> berries picked: 44 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3560 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [466, 632, 444, 321, 312, 357, 389, 476, 163]
	| approx positives in sample 512: 84
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 8, 12, 6, 9, 10, 10, 14, 5]
	Time taken saving stuff: 0.00s

=== episode:132 Env-steps-taken:70944
action_counts: {0: 7898, 1: 8624, 2: 8635, 3: 7469, 4: 8667, 5: 6490, 6: 7194, 7: 5786, 8: 3344}
picked:  85
episode: 132/2000 -> reward: 120.05729166666683, steps:64107, time-taken: 2.20min, time-elasped: 281.23min
-> berries picked: 85 of 800 | patches-visited: [0, 4] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3576 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [468, 626, 458, 317, 322, 362, 386, 475, 162]
	| approx positives in sample 512: 85
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [7, 18, 9, 7, 10, 7, 10, 12, 5]
	Time taken saving stuff: 0.00s

=== episode:133 Env-steps-taken:72480
action_counts: {0: 7898, 1: 9790, 2: 8492, 3: 9658, 4: 8514, 5: 6523, 6: 5599, 7: 4664, 8: 3784}
picked:  97
episode: 133/2000 -> reward: 127.99479166666686, steps:64922, time-taken: 2.21min, time-elasped: 283.44min
-> berries picked: 97 of 800 | patches-visited: [0, 8] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3606 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [467, 628, 470, 325, 322, 364, 390, 477, 163]
	| approx positives in sample 512: 99
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [8, 16, 9, 12, 9, 15, 11, 14, 5]
	Time taken saving stuff: 0.00s

=== episode:134 Env-steps-taken:65280
action_counts: {0: 6545, 1: 9196, 2: 8833, 3: 7689, 4: 7689, 5: 4862, 6: 6116, 7: 5721, 8: 3399}
picked:  70
episode: 134/2000 -> reward: 89.63541666666676, steps:60050, time-taken: 1.99min, time-elasped: 285.43min
-> berries picked: 70 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3622 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [471, 631, 476, 326, 328, 357, 391, 478, 164]
	| approx positives in sample 512: 86
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 14, 6, 10, 10, 10, 10, 14, 3]
	Time taken saving stuff: 0.00s

=== episode:135 Env-steps-taken:80832
action_counts: {0: 8800, 1: 9788, 2: 7898, 3: 10560, 4: 12166, 5: 6864, 6: 8932, 7: 8525, 8: 3707}
picked:  118
episode: 135/2000 -> reward: 171.38541666666652, steps:77240, time-taken: 2.75min, time-elasped: 288.18min
-> berries picked: 118 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3663 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [483, 631, 485, 334, 334, 364, 399, 470, 163]
	| approx positives in sample 512: 99
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [11, 14, 15, 3, 9, 14, 15, 13, 5]
	Time taken saving stuff: 0.00s

=== episode:136 Env-steps-taken:62016
action_counts: {0: 6347, 1: 5236, 2: 5225, 3: 7764, 4: 3938, 5: 5687, 6: 5929, 7: 5489, 8: 2871}
picked:  61
episode: 136/2000 -> reward: 72.68229166666666, steps:48486, time-taken: 1.80min, time-elasped: 289.99min
-> berries picked: 61 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3674 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [488, 634, 488, 336, 336, 362, 392, 473, 165]
	| approx positives in sample 512: 91
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [17, 13, 10, 7, 7, 12, 12, 9, 4]
	Time taken saving stuff: 0.01s

=== episode:137 Env-steps-taken:65760
action_counts: {0: 6490, 1: 6952, 2: 6721, 3: 10791, 4: 6050, 5: 8195, 6: 7029, 7: 7095, 8: 3532}
picked:  75
episode: 137/2000 -> reward: 92.10937500000007, steps:62855, time-taken: 2.24min, time-elasped: 292.23min
-> berries picked: 75 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3692 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [497, 634, 485, 339, 336, 364, 396, 477, 164]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [14, 17, 15, 9, 7, 8, 12, 17, 4]
	Time taken saving stuff: 0.00s

=== episode:138 Env-steps-taken:66336
action_counts: {0: 5863, 1: 7216, 2: 7358, 3: 7051, 4: 6930, 5: 7315, 6: 5643, 7: 6127, 8: 3003}
picked:  71
episode: 138/2000 -> reward: 96.13020833333339, steps:56506, time-taken: 1.87min, time-elasped: 294.09min
-> berries picked: 71 of 800 | patches-visited: [0, 2] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3721 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [507, 638, 489, 342, 338, 368, 400, 474, 165]
	| approx positives in sample 512: 103
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [16, 18, 13, 12, 9, 9, 9, 13, 4]
	Time taken saving stuff: 0.00s

=== episode:139 Env-steps-taken:64704
action_counts: {0: 5302, 1: 7942, 2: 8514, 3: 9394, 4: 5434, 5: 6589, 6: 5808, 7: 5577, 8: 3246}
picked:  64
episode: 139/2000 -> reward: 86.6666666666667, steps:57806, time-taken: 1.97min, time-elasped: 296.06min
-> berries picked: 64 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3729 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [508, 644, 488, 345, 337, 368, 404, 469, 166]
	| approx positives in sample 512: 95
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [12, 13, 10, 10, 10, 14, 9, 9, 8]
	Time taken saving stuff: 0.00s

=== episode:140 Env-steps-taken:59616
action_counts: {0: 5874, 1: 11553, 2: 6622, 3: 5566, 4: 5973, 5: 7920, 6: 6017, 7: 6644, 8: 3366}
picked:  43
episode: 140/2000 -> reward: 60.27604166666662, steps:59535, time-taken: 2.07min, time-elasped: 298.14min
-> berries picked: 43 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3729 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [510, 648, 489, 346, 339, 366, 399, 467, 165]
	| approx positives in sample 512: 91
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [10, 13, 7, 8, 10, 14, 8, 12, 9]
	Time taken saving stuff: 0.05s

=== episode:14 Env-steps-taken:64416
action_counts: {0: 11891, 1: 8041, 2: 3223, 3: 11770, 4: 14102, 5: 1639, 6: 7513, 7: 6237, 8: 0}
picked:  63

==================================================
eval-episode: 140 -> reward: 85.17187500000004, steps: 64416.0, wall-time: 41.76s
-> berries picked: 63 of 800 | patches-visited: [1] | juice left:-0.00
==================================================


=== episode:141 Env-steps-taken:67872
action_counts: {0: 9273, 1: 10065, 2: 8052, 3: 7568, 4: 6534, 5: 6413, 6: 7260, 7: 5709, 8: 3488}
picked:  77
episode: 141/2000 -> reward: 103.09895833333347, steps:64362, time-taken: 2.26min, time-elasped: 301.10min
-> berries picked: 77 of 800 | patches-visited: [0] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3747 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [518, 648, 495, 348, 339, 369, 394, 472, 164]
	| approx positives in sample 512: 87
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [6, 10, 14, 7, 16, 9, 6, 14, 5]
	Time taken saving stuff: 0.00s

=== episode:142 Env-steps-taken:69792
action_counts: {0: 5555, 1: 7953, 2: 6985, 3: 6347, 4: 6149, 5: 5907, 6: 5500, 7: 5236, 8: 3081}
picked:  79
episode: 142/2000 -> reward: 114.08854166666679, steps:52713, time-taken: 1.80min, time-elasped: 302.90min
-> berries picked: 79 of 800 | patches-visited: [0, 1] | juice left:-0.00
	| epsilon: 0.5
	| skipsteps: 10
	| positive-in-buffer: 3777 | amount-filled: 100.00%
	| action-stats:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [517, 657, 500, 353, 342, 372, 390, 480, 166]
	| approx positives in sample 512: 85
	| approx action-dist in sample 512: [0, 1, 2, 3, 4, 5, 6, 7, 8] [9, 12, 8, 3, 10, 9, 15, 13, 6]
	Time taken saving stuff: 0.00s
